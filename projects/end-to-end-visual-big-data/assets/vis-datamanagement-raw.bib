
@article{li_approximate_2018,
	title = {Approximate {Query} {Processing}: {What} is {New} and {Where} to {Go}?},
	volume = {3},
	issn = {2364-1541},
	shorttitle = {Approximate {Query} {Processing}},
	url = {https://doi.org/10.1007/s41019-018-0074-4},
	doi = {10.1007/s41019-018-0074-4},
	abstract = {Online analytical processing (OLAP) is a core functionality in database systems. The performance of OLAP is crucial to make online decisions in many applications. However, it is rather costly to support OLAP on large datasets, especially big data, and the methods that compute exact answers cannot meet the high-performance requirement. To alleviate this problem, approximate query processing (AQP) has been proposed, which aims to find an approximate answer as close as to the exact answer efficiently. Existing AQP techniques can be broadly categorized into two categories. (1) Online aggregation: select samples online and use these samples to answer OLAP queries. (2) Offline synopses generation: generate synopses offline based on a-priori knowledge (e.g., data statistics or query workload) and use these synopses to answer OLAP queries. We discuss the research challenges in AQP and summarize existing techniques to address these challenges. In addition, we review how to use AQP to support other complex data types, e.g., spatial data and trajectory data, and support other applications, e.g., data visualization and data cleaning. We also introduce existing AQP systems and summarize their advantages and limitations. Lastly, we provide research challenges and opportunities of AQP. We believe that the survey can help the partitioners to understand existing AQP techniques and select appropriate methods in their applications.},
	language = {en},
	number = {4},
	urldate = {2018-12-19},
	journal = {Data Science and Engineering},
	author = {Li, Kaiyu and Li, Guoliang},

	year = {2018},
	keywords = {survey},
	pages = {379--397}
}

@inproceedings{kamat_distributed_2014,
	title = {Distributed and interactive cube exploration},
	doi = {10.1109/ICDE.2014.6816674},
	abstract = {Interactive ad-hoc analytics over large datasets has become an increasingly popular use case. We detail the challenges encountered when building a distributed system that allows the interactive exploration of a data cube. We introduce DICE, a distributed system that uses a novel session-oriented model for data cube exploration, designed to provide the user with interactive sub-second latencies for specified accuracy levels. A novel framework is provided that combines three concepts: faceted exploration of data cubes, speculative execution of queries and query execution over subsets of data. We discuss design considerations, implementation details and optimizations of our system. Experiments demonstrate that DICE provides a sub-second interactive cube exploration experience at the billion-tuple scale that is at least 33\% faster than current approaches.},
	booktitle = {2014 {IEEE} 30th {International} {Conference} on {Data} {Engineering}},
	author = {Kamat, N. and Jayachandran, P. and Tunga, K. and Nandi, A.},

	year = {2014},
	keywords = {Accuracy, billion-tuple scale, Catalogs, Context, data analysis, Data models, DICE system, distributed data cube exploration, Distributed databases, distributed system, faceted data cubes exploration, interactive ad-hoc analytics, interactive data cube exploration, Lattices, query processing, session-oriented model, speculative query execution, sub-second interactive cube exploration, cscheid-materialized-views, leibatt-progressive, cscheid-aqp, leibatt-aqp, cscheid-user-modeling, leibatt-materialized-views, leibatt-aggregation, leibatt-user-modeling, leibatt-interaction-aggregate, leibatt-interaction-filter, leibatt-interaction-navigate, leibatt-data-parallel, cscheid-interaction-aggregate, cscheid-interaction-navigate, cscheid-interaction-filter},
	pages = {472--483}
}

@inproceedings{battle_dynamic_2016,
	address = {New York, NY, USA},
	series = {{SIGMOD} '16},
	title = {Dynamic {Prefetching} of {Data} {Tiles} for {Interactive} {Visualization}},
	isbn = {978-1-4503-3531-7},
	url = {http://doi.acm.org/10.1145/2882903.2882919},
	doi = {10.1145/2882903.2882919},
	abstract = {In this paper, we present ForeCache, a general-purpose tool for exploratory browsing of large datasets. ForeCache utilizes a client-server architecture, where the user interacts with a lightweight client-side interface to browse datasets, and the data to be browsed is retrieved from a DBMS running on a back-end server. We assume a detail-on-demand browsing paradigm, and optimize the back-end support for this paradigm by inserting a separate middleware layer in front of the DBMS. To improve response times, the middleware layer fetches data ahead of the user as she explores a dataset. We consider two different mechanisms for prefetching: (a) learning what to fetch from the user's recent movements, and (b) using data characteristics (e.g., histograms) to find data similar to what the user has viewed in the past. We incorporate these mechanisms into a single prediction engine that adjusts its prediction strategies over time, based on changes in the user's behavior. We evaluated our prediction engine with a user study, and found that our dynamic prefetching strategy provides: (1) significant improvements in overall latency when compared with non-prefetching systems (430\% improvement); and (2) substantial improvements in both prediction accuracy (25\% improvement) and latency (88\% improvement) relative to existing prefetching techniques.},
	urldate = {2018-12-18},
	booktitle = {Proceedings of the 2016 {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Battle, Leilani and Chang, Remco and Stonebraker, Michael},
	year = {2016},
	keywords = {array browsing, data exploration, predictive caching, visual exploration, cscheid-materialized-views, cscheid-user-modeling, leibatt-materialized-views, leibatt-aggregation, leibatt-user-modeling, leibatt-interaction-aggregate, leibatt-interaction-filter, leibatt-interaction-navigate, cscheid-interaction-aggregate, cscheid-interaction-navigate, cscheid-interaction-filter},
	pages = {1363--1375}
}

@inproceedings{babcock_dynamic_2003,
	address = {New York, NY, USA},
	series = {{SIGMOD} '03},
	title = {Dynamic {Sample} {Selection} for {Approximate} {Query} {Processing}},
	isbn = {978-1-58113-634-0},
	url = {http://doi.acm.org/10.1145/872757.872822},
	doi = {10.1145/872757.872822},
	abstract = {In decision support applications, the ability to provide fast approximate answers to aggregation queries is desirable. One commonly-used technique for approximate query answering is sampling. For many aggregation queries, appropriately constructed biased (non-uniform) samples can provide more accurate approximations than a uniform sample. The optimal type of bias, however, varies from query to query. In this paper, we describe an approximate query processing technique that dynamically constructs an appropriately biased sample for each query by combining samples selected from a family of non-uniform samples that are constructed during a pre-processing phase. We show that dynamic selection of appropriate portions of previously constructed samples can provide more accurate approximate answers than static, non-adaptive usage of uniform or non-uniform samples.},
	urldate = {2018-12-18},
	booktitle = {Proceedings of the 2003 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Babcock, Brian and Chaudhuri, Surajit and Das, Gautam},
	year = {2003},
	keywords = {cscheid-materialized-views, cscheid-aqp, leibatt-aqp, cscheid-precomputed-samples, leibatt-materialized-views, leibatt-interaction-aggregate, leibatt-interaction-filter, cscheid-interaction-aggregate, cscheid-interaction-filter},
	pages = {539--550}
}

@inproceedings{ding_differentially_2011,
	address = {New York, NY, USA},
	series = {{SIGMOD} '11},
	title = {Differentially {Private} {Data} {Cubes}: {Optimizing} {Noise} {Sources} and {Consistency}},
	isbn = {978-1-4503-0661-4},
	shorttitle = {Differentially {Private} {Data} {Cubes}},
	url = {http://doi.acm.org/10.1145/1989323.1989347},
	doi = {10.1145/1989323.1989347},
	abstract = {Data cubes play an essential role in data analysis and decision support. In a data cube, data from a fact table is aggregated on subsets of the table's dimensions, forming a collection of smaller tables called cuboids. When the fact table includes sensitive data such as salary or diagnosis, publishing even a subset of its cuboids may compromise individuals' privacy. In this paper, we address this problem using differential privacy (DP), which provides provable privacy guarantees for individuals by adding noise to query answers. We choose an initial subset of cuboids to compute directly from the fact table, injecting DP noise as usual; and then compute the remaining cuboids from the initial set. Given a fixed privacy guarantee, we show that it is NP-hard to choose the initial set of cuboids so that the maximal noise over all published cuboids is minimized, or so that the number of cuboids with noise below a given threshold (precise cuboids) is maximized. We provide an efficient procedure with running time polynomial in the number of cuboids to select the initial set of cuboids, such that the maximal noise in all published cuboids will be within a factor (ln{\textbar}L{\textbar} + 1){\textasciicircum}2 of the optimal, where {\textbar}L{\textbar} is the number of cuboids to be published, or the number of precise cuboids will be within a factor (1 - 1/e) of the optimal. We also show how to enforce consistency in the published cuboids while simultaneously improving their utility (reducing error). In an empirical evaluation on real and synthetic data, we report the amounts of error of different publishing algorithms, and show that our approaches outperform baselines significantly.},
	urldate = {2018-12-18},
	booktitle = {Proceedings of the 2011 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Ding, Bolin and Winslett, Marianne and Han, Jiawei and Li, Zhenhui},
	year = {2011},
	keywords = {OLAP, data cube, differential privacy, private data analysis, cscheid-materialized-views, cscheid-maybe, cscheid-aggregate, leibatt-materialized-views, leibatt-aggregation, leibatt-maybe},
	pages = {217--228}
}

@inproceedings{beckmann_r*-tree:_1990,
	address = {New York, NY, USA},
	series = {{SIGMOD} '90},
	title = {The {R}*-tree: {An} {Efficient} and {Robust} {Access} {Method} for {Points} and {Rectangles}},
	isbn = {978-0-89791-365-2},
	shorttitle = {The {R}*-tree},
	url = {http://doi.acm.org/10.1145/93597.98741},
	doi = {10.1145/93597.98741},
	abstract = {The R-tree, one of the most popular access methods for rectangles, is based on the heuristic optimization of the area of the enclosing rectangle in each inner node. By running numerous experiments in a standardized testbed under highly varying data, queries and operations, we were able to design the R*-tree which incorporates a combined optimization of area, margin and overlap of each enclosing rectangle in the directory. Using our standardized testbed in an exhaustive performance comparison, it turned out that the R*-tree clearly outperforms the existing R-tree variants. Guttman's linear and quadratic R-tree and Greene's variant of the R-tree. This superiority of the R*-tree holds for different types of queries and operations, such as map overlay, for both rectangles and multidimensional points in all experiments. From a practical point of view the R*-tree is very attractive because of the following two reasons 1 it efficiently supports point and spatial data at the same time and 2 its implementation cost is only slightly higher than that of other R-trees.},
	urldate = {2018-12-18},
	booktitle = {Proceedings of the 1990 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Beckmann, Norbert and Kriegel, Hans-Peter and Schneider, Ralf and Seeger, Bernhard},
	year = {1990},
	keywords = {leibatt-spatial-index, leibatt-interaction-filter, leibatt-interaction-select, leibatt-interaction-navigate, cscheid-spatial-index, cscheid-interaction-navigate, cscheid-interaction-filter, leibatt-index, cscheid-interaction-select, cscheid-index},
	pages = {322--331}
}

@misc{noauthor_r*-tree:_nodate,
	title = {The {R}*-tree: an efficient and robust access method for points and rectangles},
	url = {https://dl.acm.org/citation.cfm?id=98741},
	urldate = {2018-12-18},
	file = {The R*-tree\: an efficient and robust access method for points and rectangles:/Users/cscheid/Zotero/storage/RKY26MDJ/citation.html:text/html}
}

@inproceedings{xin_c-cubing:_2006,
	title = {C-{Cubing}: {Efficient} {Computation} of {Closed} {Cubes} by {Aggregation}-{Based} {Checking}},
	shorttitle = {C-{Cubing}},
	doi = {10.1109/ICDE.2006.31},
	abstract = {It is well recognized that data cubing often produces huge outputs. Two popular efforts devoted to this problem are (1) iceberg cube, where only significant cells are kept, and (2) closed cube, where a group of cells which preserve roll-up/drill-down semantics are losslessly compressed to one cell. Due to its usability and importance, efficient computation of closed cubes still warrants a thorough study. In this paper, we propose a new measure, called closedness, for efficient closed data cubing. We show that closedness is an algebraic measure and can be computed efficiently and incrementally. Based on closedness measure, we develop an an aggregation-based approach, called C-Cubing (i.e., Closed-Cubing), and integrate it into two successful iceberg cubing algorithms: MM-Cubing and Star-Cubing. Our performance study shows that C-Cubing runs almost one order of magnitude faster than the previous approaches. We further study how the performance of the alternative algorithms of C-Cubing varies w.r.t the properties of the data sets.},
	booktitle = {22nd {International} {Conference} on {Data} {Engineering} ({ICDE}'06)},
	author = {Xin, Dong and Shao, Zheng and Han, Jiawei and Liu, Hongyan},

	year = {2006},
	keywords = {Data engineering, Relational databases, Usability, cscheid-maybe, leibatt-materialized-views, leibatt-aggregation, leibatt-maybe, leibatt-interaction-aggregate, leibatt-interaction-navigate},
	pages = {4--4}
}

@inproceedings{zhao_graph_2011,
	address = {New York, NY, USA},
	series = {{SIGMOD} '11},
	title = {Graph {Cube}: {On} {Warehousing} and {OLAP} {Multidimensional} {Networks}},
	isbn = {978-1-4503-0661-4},
	shorttitle = {Graph {Cube}},
	url = {http://doi.acm.org/10.1145/1989323.1989413},
	doi = {10.1145/1989323.1989413},
	abstract = {We consider extending decision support facilities toward large sophisticated networks, upon which multidimensional attributes are associated with network entities, thereby forming the so-called multidimensional networks. Data warehouses and OLAP (Online Analytical Processing) technology have proven to be effective tools for decision support on relational data. However, they are not well-equipped to handle the new yet important multidimensional networks. In this paper, we introduce Graph Cube, a new data warehousing model that supports OLAP queries effectively on large multidimensional networks. By taking account of both attribute aggregation and structure summarization of the networks, Graph Cube goes beyond the traditional data cube model involved solely with numeric value based group-by's, thus resulting in a more insightful and structure-enriched aggregate network within every possible multidimensional space. Besides traditional cuboid queries, a new class of OLAP queries, crossboid, is introduced that is uniquely useful in multidimensional networks and has not been studied before. We implement Graph Cube by combining special characteristics of multidimensional networks with the existing well-studied data cube techniques. We perform extensive experimental studies on a series of real world data sets and Graph Cube is shown to be a powerful and efficient tool for decision support on large multidimensional networks.},
	urldate = {2018-12-18},
	booktitle = {Proceedings of the 2011 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Zhao, Peixiang and Li, Xiaolei and Xin, Dong and Han, Jiawei},
	year = {2011},
	keywords = {OLAP, data cube, data warehouse, graph cube, multidimensional network, cscheid-materialized-views, cscheid-maybe, leibatt-materialized-views, leibatt-aggregation, leibatt-maybe, leibatt-interaction-aggregate, leibatt-interaction-navigate, cscheid-interaction-aggregate},
	pages = {853--864}
}

@misc{noauthor_range_nodate,
	title = {Range cube: efficient cube computation by exploiting data correlation - {IEEE} {Conference} {Publication}},
	url = {https://ieeexplore.ieee.org/abstract/document/1320035},
	urldate = {2018-12-18},
	keywords = {cscheid-materialized-views, leibatt-materialized-views, leibatt-aggregation, leibatt-compression, leibatt-interaction-aggregate, leibatt-interaction-filter, leibatt-interaction-select, leibatt-interaction-navigate, cscheid-interaction-aggregate, cscheid-interaction-filter, cscheid-interaction-select}
}

@inproceedings{li_high-dimensional_2004,
	title = {High-dimensional {OLAP}: a minimal cubing approach},
	isbn = {978-0-12-088469-8},
	shorttitle = {High-dimensional {OLAP}},
	url = {http://dl.acm.org/citation.cfm?id=1316689.1316736},
	urldate = {2018-12-18},
	publisher = {VLDB Endowment},
	author = {Li, Xiaolei and Han, Jiawei and Gonzalez, Hector},

	year = {2004},
	keywords = {cscheid-materialized-views, leibatt-materialized-views, leibatt-aggregation, leibatt-interaction-aggregate, leibatt-interaction-navigate, cscheid-interaction-aggregate},
	pages = {528--539}
}

@article{sellis_r+_1987,
	title = {The {R}+ -{Tree}: {A} {Dynamic} {Index} for {Multi}-{Dimensional} {Objects}.},
	shorttitle = {The {R}+ -{Tree}},
	url = {http://drum.lib.umd.edu/handle/1903/4541},
	abstract = {The problem of indexing multidimensional objects is considered. First, a classification of existing methods in given along with a discussion of the major issues involved in multidimensional data indexing. Second, a variation to Guttman's R-trees (R+ -trees) that avoids overlapping rectangles in intermediate nodes of the tree is introduced. Algorithms for searching, updating, initial packing and reorganization of the structure are discussed in detail. Finally, we provide analytical results indicating that R+ -tree achieve up to 50\% savings in disk accesses compared to an R-tree when searching files of thousands of rectangles.},
	language = {en\_US},
	urldate = {2018-12-18},
	author = {Sellis, T. and Roussopoulos, N. and Faloutsos, Christos},
	year = {1987},
	keywords = {leibatt-spatial-index, leibatt-interaction-filter, leibatt-interaction-select, leibatt-interaction-navigate, cscheid-spatial-index, cscheid-interaction-navigate, cscheid-interaction-filter, leibatt-index, cscheid-interaction-select, cscheid-index}
}

@inproceedings{roussopoulos_direct_1985,
	address = {New York, NY, USA},
	series = {{SIGMOD} '85},
	title = {Direct {Spatial} {Search} on {Pictorial} {Databases} {Using} {Packed} {R}-trees},
	isbn = {978-0-89791-160-3},
	url = {http://doi.acm.org/10.1145/318898.318900},
	doi = {10.1145/318898.318900},
	abstract = {An abstract is not available.},
	urldate = {2018-12-18},
	booktitle = {Proceedings of the 1985 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Roussopoulos, Nick and Leifker, Daniel},
	year = {1985},
	keywords = {cscheid-maybe, cscheid-filter, cscheid-select, leibatt-spatial-index, leibatt-maybe, cscheid-navigate, leibatt-interaction-filter, leibatt-interaction-select, leibatt-interaction-navigate, cscheid-spatial-index, leibatt-index},
	pages = {17--31}
}

@inproceedings{sismanis_dwarf:_2002,
	address = {New York, NY, USA},
	series = {{SIGMOD} '02},
	title = {Dwarf: {Shrinking} the {PetaCube}},
	isbn = {978-1-58113-497-1},
	shorttitle = {Dwarf},
	url = {http://doi.acm.org/10.1145/564691.564745},
	doi = {10.1145/564691.564745},
	abstract = {Dwarf is a highly compressed structure for computing, storing, and querying data cubes. Dwarf identifies prefix and suffix structural redundancies and factors them out by coalescing their store. Prefix redundancy is high on dense areas of cubes but suffix redundancy is significantly higher for sparse areas. Putting the two together fuses the exponential sizes of high dimensional full cubes into a dramatically condensed data structure. The elimination of suffix redundancy has an equally dramatic reduction in the computation of the cube because recomputation of the redundant suffixes is avoided. This effect is multiplied in the presence of correlation amongst attributes in the cube. A Petabyte 25-dimensional cube was shrunk this way to a 2.3GB Dwarf Cube, in less than 20 minutes, a 1:400000 storage reduction ratio. Still, Dwarf provides 100\% precision on cube queries and is a self-sufficient structure which requires no access to the fact table. What makes Dwarf practical is the automatic discovery,in a single pass over the fact table, of the prefix and suffix redundancies without user involvement or knowledge of the value distributions.This paper describes the Dwarf structure and the Dwarf cube construction algorithm. Further optimizations are then introduced for improving clustering and query performance. Experiments with the current implementation include comparisons on detailed measurements with real and synthetic datasets against previously published techniques. The comparisons show that Dwarfs by far out-perform these techniques on all counts: storage space, creation time, query response time, and updates of cubes.},
	urldate = {2018-12-18},
	booktitle = {Proceedings of the 2002 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Sismanis, Yannis and Deligiannakis, Antonios and Roussopoulos, Nick and Kotidis, Yannis},
	year = {2002},
	keywords = {cscheid-materialized-views, leibatt-materialized-views, leibatt-aggregation, leibatt-interaction-aggregate, leibatt-interaction-filter, leibatt-interaction-select, leibatt-interaction-navigate, cscheid-interaction-aggregate, cscheid-interaction-navigate, cscheid-interaction-filter},
	pages = {464--475}
}

@article{psallidas_smoke:_2018,
	title = {Smoke: {Fine}-grained {Lineage} at {Interactive} {Speed}},
	volume = {11},
	issn = {2150-8097},
	shorttitle = {Smoke},
	url = {https://dl.acm.org/doi/10.5555/3199517.3199522},
	doi = {10.14778/3184470.3184475},
	abstract = {Data lineage describes the relationship between individual input and output data items of a workflow and is an integral ingredient for both traditional (e.g., debugging or auditing) and emergent (e.g., explanations or cleaning) applications. The core, long-standing problem that lineage systems need to address---and the main focus of this paper---is to quickly capture lineage across a workflow in order to speed up future queries over lineage. Current lineage systems, however, either incur high lineage capture overheads, high lineage query processing costs, or both. In response, developers resort to manual implementations of applications that, in principal, can be expressed and optimized in lineage terms. This paper describes Smoke, an in-memory database engine that provides both fast lineage capture and lineage query processing. To do so, Smoke tightly integrates the lineage capture logic into physical database operators; stores lineage in efficient lineage representations; and employs optimizations if future lineage queries are known up-front. Our experiments on microbenchmarks and realistic workloads show that Smoke reduces the lineage capture overhead and lineage query costs by multiple orders of magnitude as compared to state-of-the-art alternatives. On real-world applications, we show that Smoke meets the latency requirements of interactive visualizations (e.g., {\textless} 150ms) and outperforms hand-written implementations of data profiling primitives.},
	number = {6},
	urldate = {2018-12-18},
	journal = {Proc. VLDB Endow.},
	author = {Psallidas, Fotis and Wu, Eugene},

	year = {2018},
	keywords = {leibatt-materialized-views, leibatt-provenance, leibatt-interaction-aggregate, leibatt-interaction-filter, leibatt-interaction-select, cscheid-interaction-aggregate, leibatt-index, cscheid-interaction-select, cscheid-index, cscheid-provenance},
	pages = {719--732}
}

@article{godfrey_interactive_2016,
	title = {Interactive {Visualization} of {Large} {Data} {Sets}},
	volume = {28},
	issn = {1041-4347},
	url = {https://ieeexplore.ieee.org/abstract/document/7457691},
	doi = {10.1109/TKDE.2016.2557324},
	abstract = {Visualization provides a powerful means for data analysis. But to be practical, visual analytics tools must support smooth and flexible use of visualizations at a fast rate. This becomes increasingly onerous with the ever-increasing size of real-world datasets. First, large databases make interaction more difficult once query response time exceeds several seconds. Second, any attempt to show all data points will overload the visualization, resulting in chaos that will only confuse the user. Over the last few years, substantial effort has been put into addressing both of these issues and many innovative solutions have been proposed. Indeed, data visualization is a topic that is too large to be addressed in a single survey paper. Thus, we restrict our attention here to interactive visualization of large data sets. Our focus then is skewed in a natural way towards query processing problem-provided by an underlying database system-rather than to the actual data visualization problem.},
	number = {8},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Godfrey, P. and Gryz, J. and Lasek, P.},

	year = {2016},
	keywords = {data analysis, big data, data aggregation, Data analysis, data indexing, data points, data visualisation, Data visualization, Database visualization, interactive visualization, large data set visualization, query processing problem, Scalability, visual analytics tools, Visual databases, Visualization, cscheid-survey, leibatt-survey},
	pages = {2142--2157}
}

@inproceedings{wesley_leveraging_2014,
	address = {New York, NY, USA},
	series = {{SIGMOD} '14},
	title = {Leveraging {Compression} in the {Tableau} {Data} {Engine}},
	isbn = {978-1-4503-2376-5},
	url = {http://doi.acm.org/10.1145/2588555.2595639},
	doi = {10.1145/2588555.2595639},
	abstract = {Data sets are growing rapidly and there is an attendant need for tools that facilitate human analysis of them in a timely manner. To help meet this need, column-oriented databases (or "column stores") have come into wide use because of their low latency on analytic workloads. Column stores use a number of techniques to produce these dramatic performance techniques, including the ability to perform operations directly on compressed data. In this paper, we describe how the Tableau Data Engine (an internally developed column store) leverages a number of compression techniques to improve query performance. The approach is simpler than existing systems for operating on compressed data and more unified, removing the necessity for custom data access mechanisms. The approach also uses some novel metadata extraction techniques to improve the choices made by the system's run-time optimizer.},
	urldate = {2018-12-18},
	booktitle = {Proceedings of the 2014 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Wesley, Richard Michael Grantham and Terlecki, Pawel},
	year = {2014},
	keywords = {column stores, compression, vizualization, leibatt-compression, leibatt-interaction-filter, leibatt-interaction-select, leibatt-data-parallel, cscheid-interaction-filter, cscheid-interaction-select, cscheid-data-parallel, cscheid-compression},
	pages = {563--573}
}

@inproceedings{terlecki_improving_2015,
	address = {New York, NY, USA},
	series = {{SIGMOD} '15},
	title = {On {Improving} {User} {Response} {Times} in {Tableau}},
	isbn = {978-1-4503-2758-9},
	url = {http://doi.acm.org/10.1145/2723372.2742799},
	doi = {10.1145/2723372.2742799},
	abstract = {The rapid increase in data volumes and complexity of applied analytical tasks poses a big challenge for visualization solutions. It is important to keep the experience highly interactive, so that users stay engaged and can perform insightful data exploration. Query processing usually dominates the cost of visualization generation. Therefore, in order to achieve acceptable response times, one needs to utilize backend capabilities to the fullest and apply techniques, such as caching or prefetching. In this paper we discuss key data processing components in Tableau: the query processor, query caches, Tableau Data Engine [1, 2] and Data Server. Furthermore, we cover recent performance improvements related to the number and quality of remote queries, broader reuse of cached data, and application of inter and intra query parallelism.},
	urldate = {2018-12-18},
	booktitle = {Proceedings of the 2015 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Terlecki, Pawel and Xu, Fei and Shaw, Marianne and Kim, Valeri and Wesley, Richard},
	year = {2015},
	keywords = {column store, concurrency, data server, data visualization, query batching, tableau data engine, cscheid-mqo, leibatt-mqo, leibatt-materialized-views, leibatt-interaction-aggregate, leibatt-interaction-filter, leibatt-interaction-select, leibatt-interaction-arrange, leibatt-interaction-derive, leibatt-interaction-navigate, leibatt-data-parallel, cscheid-interaction-aggregate, cscheid-interaction-navigate, cscheid-interaction-filter, cscheid-interaction-select, cscheid-data-parallel, cscheid-interaction-derive},
	pages = {1695--1706}
}

@inproceedings{wesley_analytic_2011,
	address = {New York, NY, USA},
	series = {{SIGMOD} '11},
	title = {An {Analytic} {Data} {Engine} for {Visualization} in {Tableau}},
	isbn = {978-1-4503-0661-4},
	url = {http://doi.acm.org/10.1145/1989323.1989449},
	doi = {10.1145/1989323.1989449},
	abstract = {Efficient data processing is critical for interactive visualization of analytic data sets. Inspired by the large amount of recent research on column-oriented stores, we have developed a new specialized analytic data engine tightly-coupled with the Tableau data visualization system. The Tableau Data Engine ships as an integral part of Tableau 6.0 and is intended for the desktop and server environments. This paper covers the main requirements of our project, system architecture and query-processing pipeline. We use real-life visualization scenarios to illustrate basic concepts and provide experimental evaluation.},
	urldate = {2018-12-18},
	booktitle = {Proceedings of the 2011 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Wesley, Richard and Eldridge, Matthew and Terlecki, Pawel T.},
	year = {2011},
	keywords = {column store, data visualization, tableau data engine, query optimization, TDE, cscheid-materialized-views, cscheid-maybe, leibatt-materialized-views, leibatt-compression, leibatt-interaction-aggregate, leibatt-interaction-filter, leibatt-interaction-select, cscheid-interaction-aggregate, cscheid-interaction-filter, cscheid-interaction-select},
	pages = {1185--1194}
}

@inproceedings{williams_steerable_2004,
	address = {Washington, DC, USA},
	series = {{INFOVIS} '04},
	title = {Steerable, {Progressive} {Multidimensional} {Scaling}},
	url = {http://dx.doi.org/10.1109/INFOVIS.2004.60},
	doi = {10.1109/INFOVIS.2004.60},
	abstract = {Current implementations of Multidimensional Scaling (MDS), an approach that attempts to best represent data point similarity in a low-dimensional representation, are not suited for many of todayýs large-scale datasets. We propose an extension to the spring model approach that allows the user to interactively explore datasets that are far beyond the scale of previous implementations of MDS. We present MDSteer, a steerable MDS computation engine and visualization tool that progressively computes an MDS layout and handles datasets of over one million points. Our technique employs hierarchical data structures and progressive layouts to allow the user to steer the computation of the algorithm to the interesting areas of the dataset. The algorithm iteratively alternates between a layout stage in which a sub-selection of points are added to the set of active points affected by the MDS iteration, and a binning stage which increases the depth of the bin hierarchy and organizes the currently unplaced points into separate spatial regions. This binning strategy allows the user to select onscreen regions of the layout to focus the MDS computation into the areas of the dataset that are assigned to the selected bins. We show both real and common synthetic benchmark datasets with dimensionalities ranging from 3 to 300 and cardinalities of over one million points.},
	urldate = {2018-11-29},
	booktitle = {Proceedings of the {IEEE} {Symposium} on {Information} {Visualization}},
	publisher = {IEEE Computer Society},
	author = {Williams, Matt and Munzner, Tamara},
	year = {2004},
	keywords = {dimensionality reduction, multidimensional scaling},
	pages = {57--64}
}

@article{rahman_ive_2017,
	title = {I'{Ve} {Seen} "{Enough}": {Incrementally} {Improving} {Visualizations} to {Support} {Rapid} {Decision} {Making}},
	volume = {10},
	issn = {2150-8097},
	shorttitle = {I'{Ve} {Seen} "{Enough}"},
	url = {https://doi.org/10.14778/3137628.3137637},
	doi = {10.14778/3137628.3137637},
	abstract = {Data visualization is an effective mechanism for identifying trends, insights, and anomalies in data. On large datasets, however, generating visualizations can take a long time, delaying the extraction of insights, hampering decision making, and reducing exploration time. One solution is to use online sampling-based schemes to generate visualizations faster while improving the displayed estimates incrementally, eventually converging to the exact visualization computed on the entire data. However, the intermediate visualizations are approximate, and often fluctuate drastically, leading to potentially incorrect decisions. We propose sampling-based incremental visualization algorithms that reveal the "salient" features of the visualization quickly---with a 46× speedup relative to baselines---while minimizing error, thus enabling rapid and error-free decision making. We demonstrate that these algorithms are optimal in terms of sample complexity, in that given the level of interactivity, they generate approximations that take as few samples as possible. We have developed the algorithms in the context of an incremental visualization tool, titled IncVisage, for trendline and heatmap visualizations. We evaluate the usability of IncVisage via user studies and demonstrate that users are able to make effective decisions with incrementally improving visualizations, especially compared to vanilla online-sampling based schemes.},
	number = {11},
	urldate = {2018-11-29},
	journal = {Proc. VLDB Endow.},
	author = {Rahman, Sajjadur and Aliakbarpour, Maryam and Kong, Ha Kyung and Blais, Eric and Karahalios, Karrie and Parameswaran, Aditya and Rubinfield, Ronitt},

	year = {2017},
	keywords = {leibatt-progressive, cscheid-aqp, leibatt-aqp, cscheid-progressive, cscheid-interaction-encode},
	pages = {1262--1273}
}

@article{vitter_approximate_1999,
	title = {Approximate computation of multidimensional aggregates of sparse data using wavelets},
	volume = {28},
	url = {https://dl.acm.org/citation.cfm?id=304199},
	doi = {https://doi.org/10.1145/304181.304199},
	number = {2},
	urldate = {2018-11-29},
	journal = {ACM SIGMOD Record},
	author = {Vitter, Jeffrey and Wang, Min},

	year = {1999},
	keywords = {cscheid-materialized-views, leibatt-progressive, cscheid-aqp, leibatt-aqp, leibatt-materialized-views, leibatt-aggregation, cscheid-progressive, leibatt-interaction-aggregate, leibatt-interaction-filter, leibatt-interaction-arrange, cscheid-interaction-aggregate, cscheid-interaction-filter}
}

@inproceedings{tirthapura_optimal_2011,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Optimal {Random} {Sampling} from {Distributed} {Streams} {Revisited}},
	isbn = {978-3-642-24100-0},
	abstract = {We give an improved algorithm for drawing a random sample from a large data stream when the input elements are distributed across multiple sites which communicate via a central coordinator. At any point in time the set of elements held by the coordinator represent a uniform random sample from the set of all the elements observed so far. When compared with prior work, our algorithms asymptotically improve the total number of messages sent in the system as well as the computation required of the coordinator. We also present a matching lower bound, showing that our protocol sends the optimal number of messages up to a constant factor with large probability. As a byproduct, we obtain an improved algorithm for finding the heavy hitters across multiple distributed sites.},
	language = {en},
	booktitle = {Distributed {Computing}},
	publisher = {Springer Berlin Heidelberg},
	author = {Tirthapura, Srikanta and Woodruff, David P.},
	editor = {Peleg, David},
	year = {2011},
	keywords = {distributed streams, reservoir sampling, sampling, leibatt-progressive, cscheid-aqp, leibatt-aqp, cscheid-maybe, cscheid-progressive},
	pages = {283--297}
}

@inproceedings{sidirourgos_sciborq:_nodate,
	title = {{SciBORQ}: {Scientiﬁc} data management with {Bounds} {On} {Runtime} and {Quality}},
	url = {https://research.vu.nl/en/publications/sciborq-scientific-data-management-with-bounds-on-runtime-and-qua},
	abstract = {Data warehouses underlying virtual observatories stress the capabilities of database management systems in many ways. They are ﬁlled, on a daily basis, with large amounts of factual information derived from intensive data scrubbing and computational feature extraction pipelines. The predominant data processing techniques focus on parallel loads and map-reduce feature extraction algorithms. Querying these huge databases require a sizable computing cluster, while ideally the initial investigation should run interactively, using as few resources as possible.},
	language = {en},
	booktitle = {Proceedings of the biennial {Conference} on {Innovative} {Data} {Systems} {Research}},
	author = {Sidirourgos, Lefteris and Kersten, Martin and Boncz, Peter},
	keywords = {cscheid-materialized-views, cscheid-aqp, leibatt-aqp, cscheid-precomputed-samples, leibatt-materialized-views, leibatt-interaction-aggregate, leibatt-interaction-arrange, cscheid-interaction-aggregate},
	pages = {6}
}

@inproceedings{pansare_online_nodate,
	title = {Online {Aggregation} for {Large} {MapReduce} {Jobs}},
	volume = {4},
	url = {https://asterix.ics.uci.edu/pub/vldb11-oa.pdf},
	abstract = {In online aggregation, a database system processes a user’s aggregation query in an online fashion. At all times during processing, the system gives the user an estimate of the final query result, with the confidence bounds that become tighter over time. In this paper, we consider how online aggregation can be built into a MapReduce system for large-scale data processing. Given the MapReduce paradigm’s close relationship with cloud computing (in that one might expect a large fraction of MapReduce jobs to be run in the cloud), online aggregation is a very attractive technology. Since large-scale cloud computations are typically pay-as-you-go, a user can monitor the accuracy obtained in an online fashion, and then save money by killing the computation early once sufficient accuracy has been obtained. 1.},
	booktitle = {Proceedings of the {VLDB} {Endowment}},
	author = {Pansare, Niketan and Borkar, Vinayak and Jermaine, Chris and Condie, Tyson},
	keywords = {leibatt-progressive, cscheid-aqp, leibatt-aqp, cscheid-progressive, leibatt-interaction-aggregate, leibatt-data-parallel, cscheid-interaction-aggregate, cscheid-data-parallel}
}

@inproceedings{garofalakis_approximate_2001,
	address = {San Francisco, CA, USA},
	series = {{VLDB} '01},
	title = {Approximate {Query} {Processing}: {Taming} the {TeraBytes}},
	isbn = {978-1-55860-804-7},
	shorttitle = {Approximate {Query} {Processing}},
	url = {http://dl.acm.org/citation.cfm?id=645927.672356},
	urldate = {2018-11-29},
	booktitle = {Proceedings of the 27th {International} {Conference} on {Very} {Large} {Data} {Bases}},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Garofalakis, Minos N. and Gibbon, Phillip B.},
	year = {2001},
	keywords = {leibatt-aqp, cscheid-maybe, leibatt-maybe},
	pages = {725--}
}

@inproceedings{hellerstein_online_1997,
	address = {New York, NY, USA},
	series = {{SIGMOD} '97},
	title = {Online {Aggregation}},
	isbn = {978-0-89791-911-1},
	url = {http://doi.acm.org/10.1145/253260.253291},
	doi = {10.1145/253260.253291},
	abstract = {Aggregation in traditional database systems is performed in batch mode: a query is submitted, the system processes a large volume of data over a long period of time, and, eventually, the final answer is returned. This archaic approach is frustrating to users and has been abandoned in most other areas of computing. In this paper we propose a new online aggregation interface that permits users to both observe the progress of their aggregation queries and control execution on the fly. After outlining usability and performance requirements for a system supporting online aggregation, we present a suite of techniques that extend a database system to meet these requirements. These include methods for returning the output in random order, for providing control over the relative rate at which different aggregates are computed, and for computing running confidence intervals. Finally, we report on an initial implementation of online aggregation in POSTGRES.},
	urldate = {2018-11-29},
	booktitle = {Proceedings of the 1997 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Hellerstein, Joseph M. and Haas, Peter J. and Wang, Helen J.},
	year = {1997},
	keywords = {leibatt-progressive, cscheid-aqp, leibatt-aqp, cscheid-progressive, leibatt-interaction-aggregate, leibatt-interaction-filter, leibatt-interaction-navigate, cscheid-interaction-aggregate, cscheid-interaction-navigate, cscheid-interaction-filter},
	pages = {171--182}
}

@article{chaudhuri_optimized_2007,
	title = {Optimized {Stratified} {Sampling} for {Approximate} {Query} {Processing}},
	volume = {32},
	issn = {0362-5915},
	url = {http://doi.acm.org/10.1145/1242524.1242526},
	doi = {10.1145/1242524.1242526},
	abstract = {The ability to approximately answer aggregation queries accurately and efficiently is of great benefit for decision support and data mining tools. In contrast to previous sampling-based studies, we treat the problem as an optimization problem where, given a workload of queries, we select a stratified random sample of the original data such that the error in answering the workload queries using the sample is minimized. A key novelty of our approach is that we can tailor the choice of samples to be robust, even for workloads that are “similar” but not necessarily identical to the given workload. Finally, our techniques recognize the importance of taking into account the variance in the data distribution in a principled manner. We show how our solution can be implemented on a database system, and present results of extensive experiments on Microsoft SQL Server that demonstrate the superior quality of our method compared to previous work.},
	number = {2},
	urldate = {2018-11-29},
	journal = {ACM Trans. Database Syst.},
	author = {Chaudhuri, Surajit and Das, Gautam and Narasayya, Vivek},

	year = {2007},
	keywords = {query processing, approximation, Random sampling, cscheid-aqp, leibatt-aqp, cscheid-precomputed-samples, leibatt-materialized-views, leibatt-interaction-aggregate, leibatt-interaction-filter, leibatt-interaction-derive, cscheid-interaction-aggregate, cscheid-interaction-filter}
}

@inproceedings{acharya_aqua_1999,
	address = {New York, NY, USA},
	series = {{SIGMOD} '99},
	title = {The {Aqua} {Approximate} {Query} {Answering} {System}},
	isbn = {978-1-58113-084-3},
	url = {http://doi.acm.org/10.1145/304182.304581},
	doi = {10.1145/304182.304581},
	abstract = {Aqua is a system for providing fast, approximate answers to aggregate queries, which are very common in OLAP applications. It has been designed to run on top of any commercial relational DBMS. Aqua precomputes synopses (special statistical summaries) of the original data and stores them in the DBMS. It provides approximate answers along with quality guarantees by rewriting the queries to run on these synopses. Finally, Aqua keeps the synopses up-to-date as the database changes, using fast incremental maintenance techniques.},
	urldate = {2018-11-29},
	booktitle = {Proceedings of the 1999 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Acharya, Swarup and Gibbons, Phillip B. and Poosala, Viswanath and Ramaswamy, Sridhar},
	year = {1999},
	keywords = {cscheid-materialized-views, cscheid-aqp, leibatt-aqp, cscheid-precomputed-samples, leibatt-materialized-views, leibatt-aggregation, leibatt-interaction-aggregate, leibatt-interaction-filter, leibatt-interaction-derive, cscheid-interaction-aggregate, cscheid-interaction-filter},
	pages = {574--576}
}

@misc{noauthor_congressional_nodate,
	title = {Congressional samples for approximate answering of group-by queries},
	url = {https://dl.acm.org/citation.cfm?id=335450},
	urldate = {2018-11-29},
	keywords = {cscheid-materialized-views, cscheid-aqp, leibatt-aqp, cscheid-precomputed-samples, leibatt-materialized-views, leibatt-interaction-aggregate, leibatt-interaction-filter, cscheid-interaction-aggregate, cscheid-interaction-filter, cscheid-interaction-aqp}
}

@article{jindal_selecting_2018,
	title = {Selecting {Subexpressions} to {Materialize} at {Datacenter} {Scale}},
	volume = {11},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/3192965.3192971},
	doi = {10.14778/3192965.3192971},
	abstract = {We observe significant overlaps in the computations performed by user jobs in modern shared analytics clusters. Naïvely computing the same subexpressions multiple times results in wasting cluster resources and longer execution times. Given that these shared cluster workloads consist of tens of thousands of jobs, identifying overlapping computations across jobs is of great interest to both cluster operators and users. Nevertheless, existing approaches support orders of magnitude smaller workloads or employ heuristics with limited effectiveness. In this paper, we focus on the problem of subexpression selection for large workloads, i.e., selecting common parts of job plans and materializing them to speed-up the evaluation of subsequent jobs. We provide an ILP-based formulation of our problem and map it to a bipartite graph labeling problem. Then, we introduce BigSubs, a vertex-centric graph algorithm to iteratively choose in parallel which subexpressions to materialize and which subexpressions to use for evaluating each job. We provide a distributed implementation of our approach using our internal SQL-like execution framework, SCOPE, and assess its effectiveness over production workloads. BigSubs supports workloads with tens of thousands of jobs, yielding savings of up to 40\% in machine-hours. We are currently integrating our techniques with the SCOPE runtime in our production clusters.},
	number = {7},
	urldate = {2018-11-27},
	journal = {Proc. VLDB Endow.},
	author = {Jindal, Alekh and Karanasos, Konstantinos and Rao, Sriram and Patel, Hiren},

	year = {2018},
	keywords = {cscheid-mqo, leibatt-mqo, leibatt-materialized-views, leibatt-interaction-aggregate, leibatt-interaction-filter, leibatt-interaction-select, leibatt-interaction-derive, leibatt-data-parallel, cscheid-interaction-aggregate, cscheid-interaction-filter, cscheid-interaction-select, cscheid-interaction-derive},
	pages = {800--812}
}

@inproceedings{hong_rule-based_2009,
	address = {New York, NY, USA},
	series = {{EDBT} '09},
	title = {Rule-based {Multi}-query {Optimization}},
	isbn = {978-1-60558-422-5},
	url = {http://doi.acm.org/10.1145/1516360.1516376},
	doi = {10.1145/1516360.1516376},
	abstract = {Data stream management systems usually have to process many long-running queries that are active at the same time. Multiple queries can be evaluated more efficiently together than independently, because it is often possible to share state and computation. Motivated by this observation, various Multi-Query Optimization (MQO) techniques have been proposed. However, these approaches suffer from two limitations. First, they focus on very specialized workloads. Second, integrating MQO techniques for CQL-style stream engines and those for event pattern detection engines is even harder, as the processing models of these two types of stream engines are radically different. In this paper, we propose a rule-based MQO framework. This framework incorporates a set of new abstractions, extending their counterparts, physical operators, transformation rules, and streams, in a traditional RDBMS or stream processing system. Within this framework, we can integrate new and existing MQO techniques through the use of transformation rules. This allows us to build an expressive and scalable stream system. Just as relational optimizers are crucial for the success of RDBMSes, a powerful multi-query optimizer is needed for data stream processing. This work lays the foundation for such a multi-query optimizer, creating opportunities for future research. We experimentally demonstrate the efficacy of our approach.},
	urldate = {2018-11-27},
	booktitle = {Proceedings of the 12th {International} {Conference} on {Extending} {Database} {Technology}: {Advances} in {Database} {Technology}},
	publisher = {ACM},
	author = {Hong, Mingsheng and Riedewald, Mirek and Koch, Christoph and Gehrke, Johannes and Demers, Alan},
	year = {2009},
	keywords = {cscheid-mqo, leibatt-mqo, leibatt-interaction-aggregate, leibatt-interaction-filter, leibatt-interaction-select, cscheid-interaction-aggregate, cscheid-interaction-filter, cscheid-interaction-select},
	pages = {120--131}
}

@article{kementsietsidis_scalable_2008,
	title = {Scalable {Multi}-query {Optimization} for {Exploratory} {Queries} over {Federated} {Scientific} {Databases}},
	volume = {1},
	issn = {2150-8097},
	url = {http://dx.doi.org/10.14778/1453856.1453864},
	doi = {10.14778/1453856.1453864},
	abstract = {The diversity and large volumes of data processed in the Natural Sciences today has led to a proliferation of highly-specialized and autonomous scientific databases with inherent and often intricate relationships. As a user-friendly method for querying this complex, ever-expanding network of sources for correlations, we propose exploratory queries. Exploratory queries are loosely-structured, hence requiring only minimal user knowledge of the source network. Evaluating an exploratory query usually involves the evaluation of many distributed queries. As the number of such distributed queries can quickly become large, we attack the optimization problem for exploratory queries by proposing several multi-query optimization algorithms that compute a global evaluation plan while minimizing the total communication cost, a key bottleneck in distributed settings. The proposed algorithms are necessarily heuristics, as computing an optimal global evaluation plan is shown to be NP-hard. Finally, we present an implementation of our algorithms, along with experiments that illustrate their potential not only for the optimization of exploratory queries, but also for the multiquery optimization of large batches of standard queries.},
	number = {1},
	urldate = {2018-11-27},
	journal = {Proc. VLDB Endow.},
	author = {Kementsietsidis, Anastasios and Neven, Frank and Van de Craen, Dieter and Vansummeren, Stijn},

	year = {2008},
	keywords = {cscheid-mqo, leibatt-mqo, leibatt-interaction-aggregate, leibatt-interaction-filter, leibatt-interaction-select, leibatt-interaction-derive, leibatt-data-parallel, cscheid-interaction-aggregate, cscheid-interaction-filter, cscheid-interaction-select, cscheid-interaction-derive},
	pages = {16--27}
}

@inproceedings{le_scalable_2012,
	title = {Scalable {Multi}-query {Optimization} for {SPARQL}},
	doi = {10.1109/ICDE.2012.37},
	abstract = {This paper revisits the classical problem of multi-query optimization in the context of RDF/SPARQL. We show that the techniques developed for relational and semi-structured data/query languages are hard, if not impossible, to be extended to account for RDF data model and graph query patterns expressed in SPARQL. In light of the NP-hardness of the multi-query optimization for SPARQL, we propose heuristic algorithms that partition the input batch of queries into groups such that each group of queries can be optimized together. An essential component of the optimization incorporates an efficient algorithm to discover the common sub-structures of multiple SPARQL queries and an effective cost model to compare candidate execution plans. Since our optimization techniques do not make any assumption about the underlying SPARQL query engine, they have the advantage of being portable across different RDF stores. The extensive experimental studies, performed on three popular RDF stores, show that the proposed techniques are effective, efficient and scalable.},
	booktitle = {2012 {IEEE} 28th {International} {Conference} on {Data} {Engineering}},
	author = {Le, W. and Kementsietsidis, A. and Duan, S. and Li, F.},

	year = {2012},
	keywords = {Context, query processing, Buildings, computational complexity, data models, graph query patterns, NP-hardness, Optimization, Partitioning algorithms, Pattern matching, query languages, RDF data model, RDF/SPARQL, relational data/query languages, relational databases, Resource description framework, scalable multiquery optimization, semistructured data/query languages, SPARQL query engine, World Wide Web, cscheid-maybe, cscheid-mqo, leibatt-mqo, leibatt-maybe, leibatt-interaction-filter, leibatt-interaction-select, cscheid-interaction-filter, cscheid-interaction-select},
	pages = {666--677}
}

@inproceedings{mistry_materialized_2001,
	address = {New York, NY, USA},
	series = {{SIGMOD} '01},
	title = {Materialized {View} {Selection} and {Maintenance} {Using} {Multi}-query {Optimization}},
	isbn = {978-1-58113-332-5},
	url = {http://doi.acm.org/10.1145/375663.375703},
	doi = {10.1145/375663.375703},
	abstract = {Materialized views have been found to be very effective at speeding up queries, and are increasingly being supported by commercial databases and data warehouse systems. However, whereas the amount of data entering a warehouse and the number of materialized views are rapidly increasing, the time window available for maintaining materialized views is shrinking. These trends necessitate efficient techniques for the maintenance of materialized views.
In this paper, we show how to find an efficient plan for the maintenance of a set of materialized views, by exploiting common subexpressions between different view maintenance expressions. In particular, we show how to efficiently select (a) expressions and indices that can be effectively shared, by transient materialization; (b) additional expressions and indices for permanent materialization; and (c) the best maintenance plan — incremental or recomputation — for each view. These three decisions are highly interdependent, and the choice of one affects the choice of the others. We develop a framework that cleanly integrates the various choices in a systematic and efficient manner. Our evaluations show that many-fold improvement in view maintenance time can be achieved using our techniques. Our algorithms can also be used to efficiently select materialized views to speed up workloads containing queries and updates.},
	urldate = {2018-11-27},
	booktitle = {Proceedings of the 2001 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Mistry, Hoshi and Roy, Prasan and Sudarshan, S. and Ramamritham, Krithi},
	year = {2001},
	keywords = {cscheid-materialized-views, cscheid-mqo, leibatt-mqo, leibatt-materialized-views, leibatt-interaction-aggregate, leibatt-interaction-filter, leibatt-interaction-select, leibatt-interaction-derive, cscheid-interaction-aggregate, cscheid-interaction-filter, cscheid-interaction-select, cscheid-interaction-derive, cscheid-interaction-change},
	pages = {307--318}
}

@inproceedings{roy_efficient_2000,
	address = {New York, NY, USA},
	series = {{SIGMOD} '00},
	title = {Efficient and {Extensible} {Algorithms} for {Multi} {Query} {Optimization}},
	isbn = {978-1-58113-217-5},
	url = {http://doi.acm.org/10.1145/342009.335419},
	doi = {10.1145/342009.335419},
	abstract = {Complex queries are becoming commonplace, with the growing use of decision support systems. These complex queries often have a lot of common sub-expressions, either within a single query, or across multiple such queries run as a batch. Multiquery optimization aims at exploiting common sub-expressions to reduce evaluation cost. Multi-query optimization has hither-to been viewed as impractical, since earlier algorithms were exhaustive, and explore a doubly exponential search space.
In this paper we demonstrate that multi-query optimization using heuristics is practical, and provides significant benefits. We propose three cost-based heuristic algorithms: Volcano-SH and Volcano-RU, which are based on simple modifications to the Volcano search strategy, and a greedy heuristic. Our greedy heuristic incorporates novel optimizations that improve efficiency greatly. Our algorithms are designed to be easily added to existing optimizers. We present a performance study comparing the algorithms, using workloads consisting of queries from the TPC-D benchmark. The study shows that our algorithms provide significant benefits over traditional optimization, at a very acceptable overhead in optimization time.},
	urldate = {2018-11-27},
	booktitle = {Proceedings of the 2000 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Roy, Prasan and Seshadri, S. and Sudarshan, S. and Bhobe, Siddhesh},
	year = {2000},
	keywords = {cscheid-mqo, leibatt-mqo, leibatt-interaction-aggregate, leibatt-interaction-filter, leibatt-interaction-select, leibatt-interaction-arrange, leibatt-interaction-derive, cscheid-interaction-aggregate, cscheid-interaction-filter, cscheid-interaction-select},
	pages = {249--260}
}

@article{sellis_multiple-query_1988,
	title = {Multiple-query {Optimization}},
	volume = {13},
	issn = {0362-5915},
	url = {http://doi.acm.org/10.1145/42201.42203},
	doi = {10.1145/42201.42203},
	abstract = {Some recently proposed extensions to relational database systems, as well as to deductive database systems, require support for multiple-query processing. For example, in a database system enhanced with inference capabilities, a simple query involving a rule with multiple definitions may expand to more than one actual query that has to be run over the database. It is an interesting problem then to come up with algorithms that process these queries together instead of one query at a time. The main motivation for performing such an interquery optimization lies in the fact that queries may share common data. We examine the problem of multiple-query optimization in this paper. The first major contribution of the paper is a systematic look at the problem, along with the presentation and analysis of algorithms that can be used for multiple-query optimization. The second contribution lies in the presentation of experimental results. Our results show that using multiple-query processing algorithms may reduce execution cost considerably.},
	number = {1},
	urldate = {2018-11-27},
	journal = {ACM Trans. Database Syst.},
	author = {Sellis, Timos K.},

	year = {1988},
	keywords = {cscheid-mqo, leibatt-mqo, leibatt-interaction-filter, leibatt-interaction-select, cscheid-interaction-filter, cscheid-interaction-select},
	pages = {23--52}
}

@misc{noauthor_pfunk-h_nodate,
	title = {{PFunk}-{H}},
	url = {https://dl.acm.org/citation.cfm?id=2939512},
	urldate = {2018-11-27},
	file = {PFunk-H:/Users/cscheid/Zotero/storage/U9YT2Y59/citation.html:text/html}
}

@inproceedings{ding_sample_2016,
	address = {New York, NY, USA},
	series = {{SIGMOD} '16},
	title = {Sample + {Seek}: {Approximating} {Aggregates} with {Distribution} {Precision} {Guarantee}},
	isbn = {978-1-4503-3531-7},
	shorttitle = {Sample + {Seek}},
	url = {http://doi.acm.org/10.1145/2882903.2915249},
	doi = {10.1145/2882903.2915249},
	abstract = {Data volumes are growing exponentially for our decision-support systems making it challenging to ensure interactive response time for ad-hoc queries without increasing cost of hardware. Aggregation queries with Group By that produce an aggregate value for every combination of values in the grouping columns are the most important class of ad-hoc queries. As small errors are usually tolerable for such queries, approximate query processing (AQP) has the potential to answer them over very large datasets much faster. In many cases analysts require the distribution of (group, aggvalue) pairs in the estimated answer to be guaranteed within a certain error threshold of the exact distribution. Existing AQP techniques are inadequate for two main reasons. First, users cannot express such guarantees. Second, sampling techniques used in traditional AQP can produce arbitrarily large errors even for summ queries. To address those limitations, we first introduce a new precision metric, called distribution precision, to express such error guarantees. We then study how to provide fast approximate answers to aggregation queries with distribution precision guaranteed within a user-specified error bound. The main challenges are to provide rigorous error guarantees and to handle arbitrary highly selective predicates without maintaining large-sized samples. We propose a novel sampling scheme called measure-biased sampling to address the former challenge. For the latter, we propose two new indexes to augment in-memory samples. Like other sampling-based AQP techniques, our solution supports any aggregate that can be estimated from random samples. In addition to deriving theoretical guarantees, we conduct experimental study to compare our system with state-of-the-art AQP techniques and a commercial column-store database system on both synthetic and real enterprise datasets. Our system provides a median speed-up of more than 100x with around 5\% distribution error compared with the commercial database.},
	urldate = {2018-11-27},
	booktitle = {Proceedings of the 2016 {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Ding, Bolin and Huang, Silu and Chaudhuri, Surajit and Chakrabarti, Kaushik and Wang, Chi},
	year = {2016},
	keywords = {sampling, approximate query processing, indexing, precision guarantee, cscheid-materialized-views, cscheid-aqp, leibatt-aqp, cscheid-precomputed-samples, leibatt-materialized-views, leibatt-interaction-aggregate, leibatt-interaction-filter, leibatt-interaction-derive, leibatt-interaction-navigate, cscheid-interaction-aggregate, cscheid-interaction-filter, leibatt-index, cscheid-index},
	pages = {679--694}
}

@inproceedings{li_wander_2016,
	address = {New York, NY, USA},
	series = {{SIGMOD} '16},
	title = {Wander {Join}: {Online} {Aggregation} via {Random} {Walks}},
	isbn = {978-1-4503-3531-7},
	shorttitle = {Wander {Join}},
	url = {http://doi.acm.org/10.1145/2882903.2915235},
	doi = {10.1145/2882903.2915235},
	abstract = {Joins are expensive, and online aggregation over joins was proposed to mitigate the cost, which offers users a nice and flexible tradeoff between query efficiency and accuracy in a continuous, online fashion. However, the state-of-the-art approach, in both internal and external memory, is based on ripple join, which is still very expensive and even needs unrealistic assumptions (e.g., tuples in a table are stored in random order). This paper proposes a new approach, the wander join algorithm, to the online aggregation problem by performing random walks over the underlying join graph. We also design an optimizer that chooses the optimal plan for conducting the random walks without having to collect any statistics a priori. Compared with ripple join, wander join is particularly efficient for equality joins involving multiple tables, but also supports θ-joins. Selection predicates and group-by clauses can be handled as well. Extensive experiments using the TPC-H benchmark have demonstrated the superior performance of wander join over ripple join. In particular, we have integrated and tested wander join in the latest version of PostgreSQL, demonstrating its practicality in a full-fledged database system.},
	urldate = {2018-11-27},
	booktitle = {Proceedings of the 2016 {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Li, Feifei and Wu, Bin and Yi, Ke and Zhao, Zhuoyue},
	year = {2016},
	keywords = {joins, online aggregation, random walks, leibatt-progressive, cscheid-aqp, leibatt-aqp, leibatt-aggregation, cscheid-progressive, leibatt-interaction-aggregate, leibatt-interaction-filter, cscheid-interaction-aggregate, cscheid-interaction-filter},
	pages = {615--629}
}

@misc{noauthor_visualization-aware_nodate,
	title = {Visualization-aware sampling for very large databases - {IEEE} {Conference} {Publication}},
	url = {https://ieeexplore.ieee.org/abstract/document/7498287},
	urldate = {2018-11-27},
	keywords = {cscheid-maybe},
	file = {Visualization-aware sampling for very large databases - IEEE Conference Publication:/Users/cscheid/Zotero/storage/G2H2N4XH/7498287.html:text/html}
}

@article{kim_rapid_2015,
	title = {Rapid {Sampling} for {Visualizations} with {Ordering} {Guarantees}},
	volume = {8},
	issn = {2150-8097},
	url = {http://dx.doi.org/10.14778/2735479.2735485},
	doi = {10.14778/2735479.2735485},
	abstract = {Visualizations are frequently used as a means to understand trends and gather insights from datasets, but often take a long time to generate. In this paper, we focus on the problem of rapidly generating approximate visualizations while preserving crucial visual properties of interest to analysts. Our primary focus will be on sampling algorithms that preserve the visual property of ordering; our techniques will also apply to some other visual properties. For instance, our algorithms can be used to generate an approximate visualization of a bar chart very rapidly, where the comparisons between any two bars are correct. We formally show that our sampling algorithms are generally applicable and provably optimal in theory, in that they do not take more samples than necessary to generate the visualizations with ordering guarantees. They also work well in practice, correctly ordering output groups while taking orders of magnitude fewer samples and much less time than conventional sampling schemes.},
	number = {5},
	urldate = {2018-11-27},
	journal = {Proc. VLDB Endow.},
	author = {Kim, Albert and Blais, Eric and Parameswaran, Aditya and Indyk, Piotr and Madden, Sam and Rubinfeld, Ronitt},

	year = {2015},
	keywords = {leibatt-progressive, cscheid-aqp, leibatt-aqp, cscheid-progressive, leibatt-interaction-aggregate, leibatt-interaction-arrange, cscheid-interaction-aggregate, cscheid-interaction-encode},
	pages = {521--532}
}

@inproceedings{agarwal_blinkdb:_2013,
	address = {New York, NY, USA},
	series = {{EuroSys} '13},
	title = {{BlinkDB}: {Queries} with {Bounded} {Errors} and {Bounded} {Response} {Times} on {Very} {Large} {Data}},
	isbn = {978-1-4503-1994-2},
	shorttitle = {{BlinkDB}},
	url = {http://doi.acm.org/10.1145/2465351.2465355},
	doi = {10.1145/2465351.2465355},
	abstract = {In this paper, we present BlinkDB, a massively parallel, approximate query engine for running interactive SQL queries on large volumes of data. BlinkDB allows users to trade-off query accuracy for response time, enabling interactive queries over massive data by running queries on data samples and presenting results annotated with meaningful error bars. To achieve this, BlinkDB uses two key ideas: (1) an adaptive optimization framework that builds and maintains a set of multi-dimensional stratified samples from original data over time, and (2) a dynamic sample selection strategy that selects an appropriately sized sample based on a query's accuracy or response time requirements. We evaluate BlinkDB against the well-known TPC-H benchmarks and a real-world analytic workload derived from Conviva Inc., a company that manages video distribution over the Internet. Our experiments on a 100 node cluster show that BlinkDB can answer queries on up to 17 TBs of data in less than 2 seconds (over 200 x faster than Hive), within an error of 2-10\%.},
	urldate = {2018-11-27},
	booktitle = {Proceedings of the 8th {ACM} {European} {Conference} on {Computer} {Systems}},
	publisher = {ACM},
	author = {Agarwal, Sameer and Mozafari, Barzan and Panda, Aurojit and Milner, Henry and Madden, Samuel and Stoica, Ion},
	year = {2013},
	keywords = {cscheid-materialized-views, cscheid-aqp, leibatt-aqp, cscheid-precomputed-samples, leibatt-materialized-views, leibatt-interaction-aggregate, leibatt-interaction-filter, cscheid-interaction-aggregate, cscheid-interaction-filter},
	pages = {29--42}
}

@article{zgraggen_how_2017,
	title = {How {Progressive} {Visualizations} {Affect} {Exploratory} {Analysis}},
	volume = {23},
	issn = {1077-2626},
	url = {doi.ieeecomputersociety.org/10.1109/TVCG.2016.2607714},
	doi = {10.1109/TVCG.2016.2607714},
	abstract = {The stated goal for visual data exploration is to operate at a rate that matches the pace of human data analysts, but the ever increasing amount of data has led to a fundamental problem: datasets are often too large to process within interactive time frames. Progressive analytics and visualizations have been proposed as potential solutions to this issue. By processing data incrementally in small chunks, progressive systems provide approximate query answers at interactive speeds that are then refined over time with increasing precision. We study how progressive visualizations affect users in exploratory settings in an experiment where we capture user behavior and knowledge discovery through interaction logs and think-aloud protocols. Our experiment includes three visualization conditions and different simulated dataset sizes. The visualization conditions are: (1) blocking, where results are displayed only after the entire dataset has been processed; (2) instantaneous, a hypothetical condition where results are shown almost immediately; and (3) progressive, where approximate results are displayed quickly and then refined over time. We analyze the data collected in our experiment and observe that users perform equally well with either instantaneous or progressive visualizations in key metrics, such as insight discovery rates and dataset coverage, while blocking visualizations have detrimental effects.},
	number = {8},
	urldate = {2018-11-27},
	journal = {IEEE Transactions on Visualization \& Computer Graphics},
	author = {Zgraggen, E. and Galakatos, A. and Crotty, A. and Fekete, J. and Kraska, T.},

	year = {2017},
	keywords = {Data analysis, Data visualization, Visualization, Histograms, Measurement, Prefetching, Time factors, leibatt-progressive, cscheid-aqp, leibatt-aqp, cscheid-progressive, leibatt-interaction-aggregate, leibatt-interaction-filter, leibatt-interaction-select, cscheid-interaction-aggregate, cscheid-interaction-filter, cscheid-interaction-select},
	pages = {1977--1987}
}

@article{galakatos_revisiting_2017,
	title = {Revisiting {Reuse} for {Approximate} {Query} {Processing}},
	volume = {10},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/3115404.3115418},
	doi = {10.14778/3115404.3115418},
	abstract = {Visual data exploration tools allow users to quickly gather insights from new datasets. As dataset sizes continue to increase, though, new techniques will be necessary to maintain the interactivity guarantees that these tools require. Approximate query processing (AQP) attempts to tackle this problem and allows systems to return query results at "human speed." However, existing AQP techniques start to break down when confronted with ad hoc queries that target the tails of the distribution. We therefore present an AQP formulation that can provide low-error approximate results at interactive speeds, even for queries over rare subpopulations. In particular, our formulation treats query results as random variables in order to leverage the ample opportunities for result reuse inherent in interactive data exploration. As part of our approach, we apply a variety of optimization techniques that are based on probability theory, including new query rewrite rules and index structures. We implemented these techniques in a prototype system and show that they can achieve interactivity where alternative approaches cannot.},
	number = {10},
	urldate = {2018-11-27},
	journal = {Proc. VLDB Endow.},
	author = {Galakatos, Alex and Crotty, Andrew and Zgraggen, Emanuel and Binnig, Carsten and Kraska, Tim},

	year = {2017},
	keywords = {cscheid-materialized-views, cscheid-aqp, leibatt-aqp, cscheid-mqo, leibatt-materialized-views, leibatt-interaction-aggregate, leibatt-interaction-filter, leibatt-interaction-select, leibatt-interaction-derive, cscheid-interaction-aggregate, cscheid-interaction-filter, leibatt-index},
	pages = {1142--1153}
}

@inproceedings{crotty_case_2016,
	address = {New York, NY, USA},
	series = {{HILDA} '16},
	title = {The {Case} for {Interactive} {Data} {Exploration} {Accelerators} ({IDEAs})},
	isbn = {978-1-4503-4207-0},
	url = {http://doi.acm.org/10.1145/2939502.2939513},
	doi = {10.1145/2939502.2939513},
	abstract = {Enabling interactive visualization over new datasets at "human speed" is key to democratizing data science and maximizing human productivity. In this work, we first argue why existing analytics infrastructures do not support interactive data exploration and then outline the challenges and opportunities of building a system specifically designed for interactive data exploration. Finally, we present an Interactive Data Exploration Accelerator (IDEA), a new type of system for interactive data exploration that is specifically designed to integrate with existing data management landscapes and allow users to explore their data instantly without expensive data preparation costs.},
	urldate = {2018-11-27},
	booktitle = {Proceedings of the {Workshop} on {Human}-{In}-the-{Loop} {Data} {Analytics}},
	publisher = {ACM},
	author = {Crotty, Andrew and Galakatos, Alex and Zgraggen, Emanuel and Binnig, Carsten and Kraska, Tim},
	year = {2016},
	keywords = {cscheid-materialized-views, leibatt-progressive, cscheid-aqp, leibatt-aqp, leibatt-materialized-views, cscheid-progressive, leibatt-interaction-aggregate, leibatt-interaction-filter, leibatt-interaction-select, cscheid-interaction-aggregate, cscheid-interaction-filter, leibatt-index, cscheid-index},
	pages = {11:1--11:6}
}

@inproceedings{moritz_trust_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Trust, but {Verify}: {Optimistic} {Visualizations} of {Approximate} {Queries} for {Exploring} {Big} {Data}},
	isbn = {978-1-4503-4655-9},
	shorttitle = {Trust, but {Verify}},
	url = {http://doi.acm.org/10.1145/3025453.3025456},
	doi = {10.1145/3025453.3025456},
	abstract = {Analysts need interactive speed for exploratory analysis, but big data systems are often slow. With sampling, data systems can produce approximate answers fast enough for exploratory visualization, at the cost of accuracy and trust. We propose optimistic visualization, which approaches these issues from a user experience perspective. This method lets analysts explore approximate results interactively, and provides a way to detect and recover from errors later. Pangloss implements these ideas. We discuss design issues raised by optimistic visualization systems. We test this concept with five expert visualizers in a laboratory study and three case studies at Microsoft. Analysts reported that they felt more confident in their results, and used optimistic visualization to check that their preliminary results were correct.},
	urldate = {2018-11-27},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Moritz, Dominik and Fisher, Danyel and Ding, Bolin and Wang, Chi},
	year = {2017},
	keywords = {data visualization, approximation, exploratory analysis, optimistic visualization, uncertainty, leibatt-progressive, cscheid-aqp, leibatt-aqp, cscheid-progressive, leibatt-interaction-aggregate, leibatt-interaction-filter, leibatt-interaction-derive, leibatt-interaction-navigate, leibatt-interaction-record, cscheid-interaction-aggregate, cscheid-interaction-filter},
	pages = {2904--2915}
}

@article{hellerstein_interactive_1999,
	title = {Interactive data analysis: the {Control} project},
	volume = {32},
	issn = {0018-9162},
	shorttitle = {Interactive data analysis},
	doi = {10.1109/2.781635},
	abstract = {Data analysis is fundamentally an iterative process in which you issue a query, receive a response, formulate the next query based on the response, and repeat. You usually don't issue a single, perfectly chosen query and get the information you want from a database; indeed, the purpose of data analysis is to extract unknown information, and in most situations, there is no one perfect query. People naturally start by asking broad, big-picture questions and then continually refine their questions based on feedback and domain knowledge. In the Control (Continuous Output and Navigation Technology with Refinement Online) project at the University of California, Berkeley, the authors are working with collaborators at IBM, Informix, and elsewhere to explore ways to improve human-computer interaction during data analysis. The Control project's goal is to develop interactive, intuitive techniques for analyzing massive data sets.},
	number = {8},
	journal = {Computer},
	author = {Hellerstein, J. M. and Avnur, R. and Chou, A. and Hidber, C. and Olston, C. and Raman, V. and Roth, T. and Haas, P. J.},

	year = {1999},
	keywords = {data analysis, Data analysis, Data visualization, Algorithm design and analysis, Association rules, Clustering algorithms, Continuous Output and Navigation Technology with Refinement Online project, Control systems, data mining, Data mining, database, Database languages, Database systems, domain knowledge, feedback, Feedback, human-computer interaction, interactive data analysis, intuitive techniques, iterative process, massive data set analysis, query, unknown information extraction, very large databases, leibatt-progressive, cscheid-aqp, leibatt-aqp, cscheid-progressive, leibatt-interaction-aggregate, leibatt-interaction-filter, leibatt-interaction-navigate, cscheid-interaction-aggregate, cscheid-interaction-filter},
	pages = {51--59}
}

@inproceedings{fisher_trust_2012,
	address = {New York, NY, USA},
	series = {{CHI} '12},
	title = {Trust {Me}, {I}'m {Partially} {Right}: {Incremental} {Visualization} {Lets} {Analysts} {Explore} {Large} {Datasets} {Faster}},
	isbn = {978-1-4503-1015-4},
	shorttitle = {Trust {Me}, {I}'m {Partially} {Right}},
	url = {http://doi.acm.org/10.1145/2207676.2208294},
	doi = {10.1145/2207676.2208294},
	abstract = {Queries over large scale (petabyte) data bases often mean waiting overnight for a result to come back. Scale costs time. Such time also means that potential avenues of exploration are ignored because the costs are perceived to be too high to run or even propose them. With sampleAction we have explored whether interaction techniques to present query results running over only incremental samples can be presented as sufficiently trustworthy for analysts both to make closer to real time decisions about their queries and to be more exploratory in their questions of the data. Our work with three teams of analysts suggests that we can indeed accelerate and open up the query process with such incremental visualizations.},
	urldate = {2018-11-27},
	booktitle = {Proceedings of the {SIGCHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Fisher, Danyel and Popov, Igor and Drucker, Steven and schraefel, m.c.},
	year = {2012},
	keywords = {online aggregation, exploratory data analysis, incremental visualizations, large data, leibatt-progressive, cscheid-aqp, leibatt-aqp, cscheid-progressive, leibatt-interaction-aggregate, leibatt-interaction-filter, leibatt-interaction-arrange, cscheid-interaction-aggregate, cscheid-interaction-navigate, cscheid-interaction-filter},
	pages = {1673--1682}
}

@article{jin_communication_2005,
	title = {Communication and memory optimal parallel data cube construction},
	volume = {16},
	issn = {1045-9219},
	url = {https://ieeexplore.ieee.org/document/1240625},
	doi = {10.1109/TPDS.2005.144},
	abstract = {Data cube construction is a commonly used operation in data warehouses. Because of the volume of data that is stored and analyzed in a data warehouse and the amount of computation involved in data cube construction, it is natural to consider parallel machines for this operation. This paper addresses a number of algorithmic issues in parallel data cube construction. First, we present an aggregation tree for sequential (and parallel) data cube construction, which has minimally bounded memory requirements. An aggregation tree is parameterized by the ordering of dimensions. We present a parallel algorithm based upon the aggregation tree. We analyze the interprocessor communication volume and construct a closed form expression for it. We prove that the same ordering of the dimensions in the aggregation tree minimizes both the computational and communication requirements. We also describe a method for partitioning the initial array and prove that it minimizes the communication volume. Finally, in the cases when memory may be a bottleneck, we describe how tiling can help scale sequential and parallel data cube construction. Experimental results from implementation of our algorithms on a cluster of workstations show the effectiveness of our algorithms and validate our theoretical results.},
	number = {12},
	journal = {IEEE Transactions on Parallel and Distributed Systems},
	author = {Jin, R. and Vaidyanathan, K. and Yang, G. and Agrawal, G.},

	year = {2005},
	keywords = {OLAP, data warehouse, Data analysis, Clustering algorithms, data mining, Aggregates, aggregation tree, array partitioning, closed form expression, communication analysis, communication analysis., Concurrent computing, data warehouses, Data warehouses, interprocessor communication, Marketing and sales, minimisation, Multidimensional systems, optimal parallel data cube construction, parallel algorithm, parallel algorithms, Parallel algorithms, parallel machines, Parallel machines, Performance analysis, sequential data cube construction, tree data structures, workstation clusters, cscheid-materialized-views, leibatt-materialized-views, leibatt-aggregation, leibatt-interaction-aggregate, leibatt-interaction-navigate, leibatt-data-parallel, cscheid-interaction-aggregate, cscheid-interaction-filter},
	pages = {1105--1119}
}

@article{mami_survey_2012,
	title = {A {Survey} of {View} {Selection} {Methods}},
	volume = {41},
	issn = {0163-5808},
	url = {http://doi.acm.org/10.1145/2206869.2206874},
	doi = {10.1145/2206869.2206874},
	abstract = {Materialized view selection is a critical problem in many applications such as query processing, data warehousing, distributed and semantic web databases, etc. We refer to the problem of selecting an appropriate set of materialized views as the view selection problem. Many different view selection methods have been proposed in the literature to address this issue. The present paper provides a survey of view selection methods. It defines a framework for highlighting the view selection problem by identifying the main dimensions that are the basis in the classification of view selection methods. Based on this classification, this study reviews most of the view selection methods by identifying respective potentials and limits.},
	number = {1},
	urldate = {2018-11-20},
	journal = {SIGMOD Rec.},
	author = {Mami, Imene and Bellahsene, Zohra},

	year = {2012},
	keywords = {cscheid-survey, cscheid-materialized-views, leibatt-materialized-views, leibatt-survey},
	pages = {20--29}
}

@inproceedings{boukraa_olap_2010,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{OLAP} {Operators} for {Complex} {Object} {Data} {Cubes}},
	isbn = {978-3-642-15576-5},
	abstract = {Nowadays, multidimensional models are recognized to best reflect the decision makers’ analytical view of data. The classical multidimensional models were meant to analyze conventional data (numerical and categorical). However, they fail to handle data complexity, which is expressed by the multiplicity of data sources, the heterogeneity of formats, the diversity of structures, etc. To this end, new multidimensional models have been proposed for OLAP purposes. Nevertheless, data complexity is partially covered in these models, which may cause a lack in decision making. In our previous work, we proposed to integrate data complexity within a complex object-based multidimensional model. In this paper, based on our proposed model, we provide adapted OLAP operators that take into account data complexity. Thus, we define operators to create complex data cubes, to visualize them and to analyze them.},
	language = {en},
	booktitle = {Advances in {Databases} and {Information} {Systems}},
	publisher = {Springer Berlin Heidelberg},
	author = {Boukraâ, Doulkifli and Boussaïd, Omar and Bentayeb, Fadila},
	editor = {Catania, Barbara and Ivanović, Mirjana and Thalheim, Bernhard},
	year = {2010},
	keywords = {complex cube, complex object, Multidimensional model, OLAP operator, cscheid-maybe, cscheid-materialized-views-aggregate, leibatt-materialized-views, leibatt-aggregation, leibatt-maybe, leibatt-interaction-aggregate, leibatt-interaction-filter, leibatt-interaction-navigate},
	pages = {103--116}
}

@inproceedings{dudas_owlap_2012,
	title = {{OWLAP} — {Using} {OLAP} approach in anomaly detection: {Award}: {Good} support for the data preparation, analysis, and presentation process},
	shorttitle = {{OWLAP} — {Using} {OLAP} approach in anomaly detection},
	doi = {10.1109/VAST.2012.6400523},
	abstract = {OWLAP (Operative Workbench for Large-scale Analytics and Presentation) is a visual analytics tool that allows the user to browse and drill down the multidimensional data on-line with the possibility to export result into a zooming presentation framework. We address the challenges of multidimensional visualization by aiding the cognitively hard task of understanding attributes, finding patterns and outliers. We successfully solved the challenge of real time Big Data OLAP reporting by a home developed multithreaded inmemory database manager. Our additional focus is the automatic management of summary preparation that we aid by scripting the presentation framework of Prezi Inc.},
	booktitle = {2012 {IEEE} {Conference} on {Visual} {Analytics} {Science} and {Technology} ({VAST})},
	author = {Dudás, L. and Fekete, Z. and Göbölös-Szabó, J. and Radnai, A. and Salánki, Á and Szabó, A. and Szűcs, G.},

	year = {2012},
	keywords = {H.2.1 [Information Systems]: Database Management — Logical Design, H.5.2 [Information Systems]: Information Interfaces and Presentation — User Interfaces, cscheid-maybe, leibatt-materialized-views, leibatt-aggregation, leibatt-maybe, leibatt-interaction-aggregate, leibatt-interaction-select, leibatt-interaction-navigate, leibatt-aggregation-filter},
	pages = {267--268}
}

@article{lafon_hierarchical_2013,
	title = {Hierarchical {Reorganization} of {Dimensions} in {OLAP} {Visualizations}},
	volume = {19},
	issn = {1077-2626},
	doi = {10.1109/TVCG.2013.93},
	abstract = {In this paper, we propose a new method for the visual reorganization of online analytical processing (OLAP) cubes that aims at improving their visualization. Our method addresses dimensions with hierarchically organized members. It uses a genetic algorithm that reorganizes k-ary trees. Genetic operators perform permutations of subtrees to optimize a visual homogeneity function. We propose several ways to reorganize an OLAP cube depending on which set of members is selected for the reorganization: all of the members, only the displayed members, or the members at a given level (level by level approach). The results that are evaluated by using optimization criteria show that our algorithm has a reliable performance even when it is limited to 1 minute runs. Our algorithm was integrated in an interactive 3D interface for OLAP. A user study was conducted to evaluate our approach with users. The results highlight the usefulness of reorganization in two OLAP tasks.},
	number = {11},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Lafon, S. and Bouali, F. and Guinot, C. and Venturini, G.},

	year = {2013},
	keywords = {data visualisation, Data visualization, Visualization, data mining, tree data structures, Dimension reorganization, genetic algorithm, genetic algorithms, Genetic algorithms, genetic operators, Genetics, graphical user interfaces, hierarchical dimension reorganization, interactive 3D interface, interactive knowledge discovery, interactive systems, k-ary tree reorganization, mathematical operators, OLAP cube, OLAP visualization, online analytical processing cubes, optimization criteria, Sociology, Statistics, subtree permutation, Three-dimensional displays, visual homogeneity function optimization, visual OLAP, visual reorganization, cscheid-materialized-views, cscheid-maybe, leibatt-materialized-views, leibatt-aggregation, leibatt-maybe, leibatt-interaction-aggregate, leibatt-interaction-arrange, leibatt-interaction-navigate, cscheid-interaction-encode, cscheid-interaction-arrange},
	pages = {1833--1845}
}

@inproceedings{cuzzocrea_multidimensional_2017,
	title = {Multidimensional database modeling: {Literature} survey and research agenda in the big data era},
	shorttitle = {Multidimensional database modeling},
	doi = {10.1109/ISNCC.2017.8072024},
	abstract = {The purpose of this paper is twofold: First, to survey the literature on Multidimensional Database Modeling fundamentals, and Second, to sketch a research agenda which challenges Big Data four V's, namely Volume, Velocity, Veracity, and Variety.},
	booktitle = {2017 {International} {Symposium} on {Networks}, {Computers} and {Communications} ({ISNCC})},
	author = {Cuzzocrea, A. and Moussa, R.},

	year = {2017},
	keywords = {Data models, very large databases, Data warehouses, Benchmark testing, Big Data, big data variety, big data velocity, big data veracity, big data volume, Databases, Decision support systems, multidimensional database modeling, Structured Query Language, cscheid-survey, leibatt-materialized-views, leibatt-aggregation, leibatt-survey},
	pages = {1--6}
}

@article{lafon_studying_2013,
	title = {On studying a {3D} user interface for {OLAP}},
	volume = {27},
	issn = {1573-756X},
	url = {https://doi.org/10.1007/s10618-012-0279-5},
	doi = {10.1007/s10618-012-0279-5},
	abstract = {In this paper, a new visual and interactive user interface for OLAP is presented, and its strengths and weaknesses examined. A survey on 3D interfaces for OLAP is detailed, which shows that only one interface that uses Virtual Reality has been proposed. Then we present our approach: it consists of a 3D representation of OLAP cubes where many OLAP operators have been integrated and where several measures can be visualized. A 3D stereoscopic screen can be used in conjunction with a 3D mouse. Finally a user study is reported that compares standard dynamic cross-tables with our interface on different tasks. We conclude that 3D with stereoscopy is not as promising as expected even with recent 3D devices.},
	language = {en},
	number = {1},
	urldate = {2018-11-20},
	journal = {Data Mining and Knowledge Discovery},
	author = {Lafon, Sébastien and Bouali, Fatma and Guinot, Christiane and Venturini, Gilles},

	year = {2013},
	keywords = {OLAP, 3D interfaces, User study, Virtual reality, Visual data mining, cscheid-maybe, leibatt-maybe, leibatt-interaction-aggregate, leibatt-interaction-filter, leibatt-interaction-navigate},
	pages = {4--21}
}

@article{cuzzocrea_olap_2009,
	title = {{OLAP} {Visualization}: {Models}, {Issues}, and {Techniques}},
	copyright = {Access limited to members},
	shorttitle = {{OLAP} {Visualization}},
	url = {https://www.igi-global.com/chapter/olap-visualization-models-issues-techniques/11010},
	doi = {10.4018/978-1-60566-010-3.ch222},
	abstract = {OLAP Visualization: Models, Issues, and Techniques: 10.4018/978-1-60566-010-3.ch222: The problem of efficiently visualizing multidimensional data sets produced by scientific and statistical tasks/ processes is becoming increasingly},
	language = {en},
	urldate = {2018-11-20},
	journal = {Encyclopedia of Data Warehousing and Mining, Second Edition},
	author = {Cuzzocrea, Alfredo and Mansmann, Svetlana},
	year = {2009},
	keywords = {cscheid-survey, leibatt-materialized-views, leibatt-aggregation, leibatt-survey},
	pages = {1439--1446}
}

@inproceedings{reach_bandlimited_2015,
	title = {Bandlimited {OLAP} cubes for interactive big data visualization},
	doi = {10.1109/LDAV.2015.7348078},
	abstract = {Visualizations backed by data cubes can scale to massive datasets while remaining interactive. However, the use of data cubes introduces artifacts, causing these visualizations to appear noisy at best and deceptive at worst. Moreover, data cubes highly constrain the space of possible visualizations. For example, a histogram backed by a data cube is constrained to have a bin width that is a multiple of the data cube bin size. Similarly, for dynamic queries backed by data cubes, query extents must be aligned with bin boundaries. We present bandlimited OLAP (online analytical processing) cubes (BLOCs), a technique that uses established tools from digital signal processing to generate interactive visualizations of very large datasets. Based on kernel density plots and Gaussian filtering, BLOCs suppress the artifacts that occur in data cubes and allow for a continuous range of zoom/pan positions and continuous dynamic queries.},
	booktitle = {2015 {IEEE} 5th {Symposium} on {Large} {Data} {Analysis} and {Visualization} ({LDAV})},
	author = {Reach, C. and North, C.},

	year = {2015},
	keywords = {data visualisation, Data visualization, Histograms, data mining, online analytical processing cubes, Big Data, bandlimited OLAP cubes, bin boundaries, BLOC, Brushes, continuous dynamic queries, data cube bin size, data cubes, digital signal processing, Frequency-domain analysis, Gaussian filtering, interactive big data visualization, interactive visualizations, Kernel, kernel density plots, query extents, Splines (mathematics), Time-domain analysis, zoom/pan positions, cscheid-materialized-views, leibatt-aqp, leibatt-materialized-views, leibatt-aggregation, cscheid-encode, leibatt-interaction-aggregate, leibatt-interaction-filter, leibatt-interaction-navigate, cscheid-interaction-navigate},
	pages = {107--114}
}

@article{kamat_session-based_2018,
	title = {A {Session}-{Based} {Approach} to {Fast}-{But}-{Approximate} {Interactive} {Data} {Cube} {Exploration}},
	volume = {12},
	issn = {1556-4681},
	url = {http://doi.acm.org/10.1145/3070648},
	doi = {10.1145/3070648},
	abstract = {With the proliferation of large datasets, sampling has become pervasive in data analysis. Sampling has numerous benefits—from reducing the computation time and cost to increasing the scope of interactive analysis. A popular task in data science, well-suited toward sampling, is the computation of fast-but-approximate aggregations over sampled data. Aggregation is a foundational block of data analysis, with data cube being its primary construct. We observe that such aggregation queries are typically issued in an ad-hoc, interactive setting. In contrast to one-off queries, a typical query session consists of a series of quick queries, interspersed with the user inspecting the results and formulating the next query. The similarity between session queries opens up opportunities for reusing computation of not just query results, but also error estimates. Error estimates need to be provided alongside sampled results for the results to be meaningful. We propose Sesame, a rewrite and caching framework that accelerates the entire interactive {\textless}underline{\textgreater}ses{\textless}/underline{\textgreater}sion of aggregation queries over {\textless}underline{\textgreater}sam{\textless}/underline{\textgreater}pl{\textless}underline{\textgreater}e{\textless}/underline{\textgreater}d data. We focus on two unique and computationally expensive aspects of this use case: query speculation in the presence of sampling, and error computation, and provide novel strategies for result and error reuse. We demonstrate that our approach outperforms conventional sampled aggregation techniques by at least an order of magnitude, without modifying the underlying database.},
	number = {1},
	urldate = {2018-11-20},
	journal = {ACM Trans. Knowl. Discov. Data},
	author = {Kamat, Niranjan and Nandi, Arnab},

	year = {2018},
	keywords = {interactive visualization, aggregation, Error Reuse, faceted exploration, session, cscheid-aqp, leibatt-aqp, cscheid-user-modeling, leibatt-materialized-views, leibatt-aggregation, leibatt-user-modeling, leibatt-interaction-aggregate, leibatt-interaction-filter, leibatt-interaction-navigate, cscheid-interaction-aggregate, cscheid-interaction-filter},
	pages = {9:1--9:26}
}

@article{miranda_topkube:_2018,
	title = {{TopKube}: {A} {Rank}-{Aware} {Data} {Cube} for {Real}-{Time} {Exploration} of {Spatiotemporal} {Data}},
	volume = {24},
	issn = {1077-2626},
	shorttitle = {{TopKube}},
	url = {https://ieeexplore.ieee.org/document/7858782},
	doi = {10.1109/TVCG.2017.2671341},
	abstract = {From economics to sports to entertainment and social media, ranking objects according to some notion of importance is a fundamental tool we humans use all the time to better understand our world. With the ever-increasing amount of user-generated content found online, “what's trending” is now a commonplace phrase that tries to capture the zeitgeist of the world by ranking the most popular microblogging hashtags in a given region and time. However, before we can understand what these rankings tell us about the world, we need to be able to more easily create and explore them, given the significant scale of today's data. In this paper, we describe the computational challenges in building a real-time visual exploratory tool for finding top-ranked objects; build on the recent work involving in-memory and rank-aware data cubes to propose TopKube: a data structure that answers top-k queries up to one order of magnitude faster than the previous state of the art; demonstrate the usefulness of our methods using a set of real-world, publicly available datasets; and provide a new set of benchmarks for other researchers to validate their methods and compare to our own.},
	number = {3},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Miranda, F. and Lins, L. and Klosowski, J. T. and Silva, C. T.},

	year = {2018},
	keywords = {query processing, data cube, Data visualization, Visualization, Benchmark testing, data structure, data structures, Data structures, Interactive visualization, Internet, Proposals, rank merging, rank-aware data cube, real-time exploration, Real-time systems, real-time visual exploratory tool, spatiotemporal data, top-k queries, top-K queries, TopKube, Urban areas, cscheid-materialized-views, leibatt-materialized-views, leibatt-aggregation, leibatt-interaction-aggregate, leibatt-interaction-filter, leibatt-interaction-select, leibatt-interaction-arrange, leibatt-interaction-navigate, cscheid-interaction-aggregate, cscheid-interaction-filter, cscheid-interaction-encode},
	pages = {1394--1407}
}

@inproceedings{kahng_visual_2016,
	address = {New York, NY, USA},
	series = {{HILDA} '16},
	title = {Visual {Exploration} of {Machine} {Learning} {Results} {Using} {Data} {Cube} {Analysis}},
	isbn = {978-1-4503-4207-0},
	url = {http://doi.acm.org/10.1145/2939502.2939503},
	doi = {10.1145/2939502.2939503},
	abstract = {As complex machine learning systems become more widely adopted, it becomes increasingly challenging for users to understand models or interpret the results generated from the models. We present our ongoing work on developing interactive and visual approaches for exploring and understanding machine learning results using data cube analysis. We propose MLCube, a data cube inspired framework that enables users to define instance subsets using feature conditions and computes aggregate statistics and evaluation metrics over the subsets. We also design MLCube Explorer, an interactive visualization tool for comparing models' performances over the subsets. Users can interactively specify operations, such as drilling down to specific instance subsets, to perform more in-depth exploration. Through a usage scenario, we demonstrate how MLCube Explorer works with a public advertisement click log data set, to help a user build new advertisement click prediction models that advance over an existing model.},
	urldate = {2018-11-20},
	booktitle = {Proceedings of the {Workshop} on {Human}-{In}-the-{Loop} {Data} {Analytics}},
	publisher = {ACM},
	author = {Kahng, Minsuk and Fang, Dezhi and Chau, Duen Horng (Polo)},
	year = {2016},
	keywords = {data cube, data visualization, interactive data analysis, machine learning, cscheid-maybe, leibatt-materialized-views, leibatt-aggregation, leibatt-maybe, leibatt-interaction-aggregate, leibatt-interaction-filter, leibatt-interaction-select, leibatt-interaction-navigate},
	pages = {1:1--1:6}
}

@article{liu_immens:_2013,
	title = {{imMens}: {Real}-time {Visual} {Querying} of {Big} {Data}},
	volume = {32},
	copyright = {© 2013 The Author(s) Computer Graphics Forum © 2013 The Eurographics Association and Blackwell Publishing Ltd.},
	issn = {1467-8659},
	shorttitle = {{imMens}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.12129},
	doi = {10.1111/cgf.12129},
	abstract = {Data analysts must make sense of increasingly large data sets, sometimes with billions or more records. We present methods for interactive visualization of big data, following the principle that perceptual and interactive scalability should be limited by the chosen resolution of the visualized data, not the number of records. We first describe a design space of scalable visual summaries that use data reduction methods (such as binned aggregation or sampling) to visualize a variety of data types. We then contribute methods for interactive querying (e.g., brushing \& linking) among binned plots through a combination of multivariate data tiles and parallel query processing. We implement our techniques in imMens, a browser-based visual analysis system that uses WebGL for data processing and rendering on the GPU. In benchmarks imMens sustains 50 frames-per-second brushing \& linking among dozens of visualizations, with invariant performance on data sizes ranging from thousands to billions of records.},
	language = {en},
	number = {3pt4},
	urldate = {2018-11-20},
	journal = {Computer Graphics Forum},
	author = {Liu, Zhicheng and Jiang, Biye and Heer, Jeffrey},

	year = {2013},
	keywords = {H.5.2, Information, Interfaces—, Interfaces:, User, cscheid-materialized-views, leibatt-materialized-views, leibatt-aggregation, leibatt-interaction-aggregate, leibatt-interaction-filter, leibatt-interaction-select, leibatt-interaction-navigate, leibatt-data-parallel, cscheid-interaction-aggregate, cscheid-interaction-navigate, cscheid-interaction-filter},
	pages = {421--430}
}

@article{wang_gaussian_2017,
	title = {Gaussian {Cubes}: {Real}-{Time} {Modeling} for {Visual} {Exploration} of {Large} {Multidimensional} {Datasets}},
	volume = {23},
	issn = {1077-2626},
	shorttitle = {Gaussian {Cubes}},
	doi = {10.1109/TVCG.2016.2598694},
	abstract = {Recently proposed techniques have finally made it possible for analysts to interactively explore very large datasets in real time. However powerful, the class of analyses these systems enable is somewhat limited: specifically, one can only quickly obtain plots such as histograms and heatmaps. In this paper, we contribute Gaussian Cubes, which significantly improves on state-of-the-art systems by providing interactive modeling capabilities, which include but are not limited to linear least squares and principal components analysis (PCA). The fundamental insight in Gaussian Cubes is that instead of precomputing counts of many data subsets (as state-of-the-art systems do), Gaussian Cubes precomputes the best multivariate Gaussian for the respective data subsets. As an example, Gaussian Cubes can fit hundreds of models over millions of data points in well under a second, enabling novel types of visual exploration of such large datasets. We present three case studies that highlight the visualization and analysis capabilities in Gaussian Cubes, using earthquake safety simulations, astronomical catalogs, and transportation statistics. The dataset sizes range around one hundred million elements and 5 to 10 dimensions. We present extensive performance results, a discussion of the limitations in Gaussian Cubes, and future research directions.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Wang, Z. and Ferreira, N. and Wei, Y. and Bhaskar, A. S. and Scheidegger, C.},

	year = {2017},
	keywords = {Data models, visual exploration, data visualisation, Data visualization, interactive visualization, Visualization, dimensionality reduction, data cubes, Analytical models, astronomical catalogs, Computational modeling, Data modeling, data subsets, earthquake safety simulations, Gaussian cubes, Gaussian processes, interactive modeling capabilities, large multidimensional datasets, least squares approximations, linear least squares, Manuals, multivariate Gaussian, PCA, principal component analysis, Principal component analysis, real-time modeling, transportation statistics, cscheid-materialized-views, leibatt-aqp, leibatt-materialized-views, leibatt-interaction-aggregate, leibatt-interaction-filter, leibatt-interaction-select, leibatt-interaction-navigate, cscheid-interaction-aggregate, cscheid-interaction-encode},
	pages = {681--690}
}

@article{pahins_hashedcubes:_2017,
	title = {Hashedcubes: {Simple}, {Low} {Memory}, {Real}-{Time} {Visual} {Exploration} of {Big} {Data}},
	volume = {23},
	issn = {1077-2626},
	shorttitle = {Hashedcubes},
	doi = {10.1109/TVCG.2016.2598624},
	abstract = {We propose Hashedcubes, a data structure that enables real-time visual exploration of large datasets that improves the state of the art by virtue of its low memory requirements, low query latencies, and implementation simplicity. In some instances, Hashedcubes notably requires two orders of magnitude less space than recent data cube visualization proposals. In this paper, we describe the algorithms to build and query Hashedcubes, and how it can drive well-known interactive visualizations such as binned scatterplots, linked histograms and heatmaps. We report memory usage, build time and query latencies for a variety of synthetic and real-world datasets, and find that although sometimes Hashedcubes offers slightly slower querying times to the state of the art, the typical query is answered fast enough to easily sustain a interaction. In datasets with hundreds of millions of elements, only about 2\% of the queries take longer than 40ms. Finally, we discuss the limitations of data structure, potential spacetime tradeoffs, and future research directions.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Pahins, C. A. L. and Stephens, S. A. and Scheidegger, C. and Comba, J. L. D.},

	year = {2017},
	keywords = {query processing, data cube, data visualisation, Data visualization, Scalability, Visualization, interactive systems, Big Data, interactive visualizations, data structure, data structures, Arrays, Big data, binned scatterplots, data cube visualization, Hashedcube query, heatmaps, interactive exploration, linked histograms, low memory real-time visual exploration, Memory management, memory requirements, multidimensional data, query latencies, querying times, real-world datasets, Sorting, synthetic datasets, cscheid-materialized-views, leibatt-materialized-views, leibatt-aggregation, leibatt-interaction-aggregate, leibatt-interaction-filter, leibatt-interaction-select, leibatt-interaction-navigate, cscheid-spatial-index, cscheid-interaction-aggregate, cscheid-interaction-filter, cscheid-index},
	pages = {671--680}
}

@article{lins_nanocubes_2013,
	title = {Nanocubes for {Real}-{Time} {Exploration} of {Spatiotemporal} {Datasets}},
	volume = {19},
	issn = {1077-2626},
	doi = {10.1109/TVCG.2013.179},
	abstract = {Consider real-time exploration of large multidimensional spatiotemporal datasets with billions of entries, each defined by a location, a time, and other attributes. Are certain attributes correlated spatially or temporally? Are there trends or outliers in the data? Answering these questions requires aggregation over arbitrary regions of the domain and attributes of the data. Many relational databases implement the well-known data cube aggregation operation, which in a sense precomputes every possible aggregate query over the database. Data cubes are sometimes assumed to take a prohibitively large amount of space, and to consequently require disk storage. In contrast, we show how to construct a data cube that fits in a modern laptop's main memory, even for billions of entries; we call this data structure a nanocube. We present algorithms to compute and query a nanocube, and show how it can be used to generate well-known visual encodings such as heatmaps, histograms, and parallel coordinate plots. When compared to exact visualizations created by scanning an entire dataset, nanocube plots have bounded screen error across a variety of scales, thanks to a hierarchical structure in space and time. We demonstrate the effectiveness of our technique on a variety of real-world datasets, and present memory, timing, and network bandwidth measurements. We find that the timings for the queries in our examples are dominated by network and user-interaction latencies.},
	number = {12},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Lins, L. and Klosowski, J. T. and Scheidegger, C.},

	year = {2013},
	keywords = {query processing, data visualisation, Data visualization, relational databases, data structures, heatmaps, interactive exploration, aggregate query, Algorithms, Androids, Computer Graphics, Computer Systems, data attributes, Data cube, data cube aggregation operation, Encoding, exact visualizations, hierarchical structure, histograms, Humanoid robots, Image Enhancement, Information Storage and Retrieval, location attribute, memory measurement, nanocube query, Nanostructured materials, network bandwidth measurement, network latency, parallel coordinate plots, realtime spatiotemporal datasets exploration, Reproducibility of Results, Sensitivity and Specificity, Spatio-Temporal Analysis, Spatiotemporal phenomena, time attribute, timing measurement, User-Computer Interface, user-interaction latency, visual encodings, cscheid-materialized-views, leibatt-materialized-views, leibatt-aggregation, leibatt-interaction-aggregate, leibatt-interaction-filter, leibatt-interaction-select, leibatt-interaction-navigate, cscheid-interaction-aggregate, cscheid-interaction-navigate, cscheid-interaction-filter},
	pages = {2456--2465}
}

@article{vassiliadis_survey_1999,
	title = {A {Survey} of {Logical} {Models} for {OLAP} {Databases}},
	volume = {28},
	issn = {0163-5808},
	url = {http://doi.acm.org/10.1145/344816.344869},
	doi = {10.1145/344816.344869},
	abstract = {In this paper, we present different proposals for multidimensional data cubes, which are the basic logical model for OLAP applications. We have grouped the work in the field in two categories: commercial tools (presented along with terminology and standards) and academic efforts. We further divide the academic efforts in two subcategories: the relational model extensions and the cube-oriented approaches. Finally, we attempt a comparative analysis of the various efforts.},
	number = {4},
	urldate = {2018-11-20},
	journal = {SIGMOD Rec.},
	author = {Vassiliadis, Panos and Sellis, Timos},

	year = {1999},
	keywords = {cscheid-survey, maybe, leibatt-materialized-views, leibatt-aggregation, leibatt-survey},
	pages = {64--69}
}

@article{gan_moment-based_2018,
	title = {Moment-based {Quantile} {Sketches} for {Efficient} {High} {Cardinality} {Aggregation} {Queries}},
	volume = {11},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/3236187.3236212},
	doi = {10.14778/3236187.3236212},
	abstract = {Interactive analytics increasingly involves querying for quantiles over sub-populations of high cardinality datasets. Data processing engines such as Druid and Spark use mergeable summaries to estimate quantiles, but summary merge times can be a bottleneck during aggregation. We show how a compact and efficiently mergeable quantile sketch can support aggregation workloads. This data structure, which we refer to as the moments sketch, operates with a small memory footprint (200 bytes) and computationally efficient (50ns) merges by tracking only a set of summary statistics, notably the sample moments. We demonstrate how we can efficiently estimate quantiles using the method of moments and the maximum entropy principle, and show how the use of a cascade further improves query time for threshold predicates. Empirical evaluation shows that the moments sketch can achieve less than 1 percent quantile error with 15× less overhead than comparable summaries, improving end query time in the MacroBase engine by up to 7× and the Druid engine by up to 60×.},
	number = {11},
	urldate = {2018-11-20},
	journal = {Proc. VLDB Endow.},
	author = {Gan, Edward and Ding, Jialin and Tai, Kai Sheng and Sharan, Vatsal and Bailis, Peter},

	year = {2018},
	keywords = {cscheid-materialized-views, cscheid-aqp, leibatt-aqp, leibatt-materialized-views, leibatt-aggregation, leibatt-interaction-aggregate, leibatt-interaction-filter, cscheid-interaction-aggregate, cscheid-interaction-filter},
	pages = {1647--1660}
}

@article{halevy_answering_2001,
	title = {Answering queries using views: {A} survey},
	volume = {10},
	issn = {0949-877X},
	shorttitle = {Answering queries using views},
	url = {https://doi.org/10.1007/s007780100054},
	doi = {10.1007/s007780100054},
	abstract = {. The problem of answering queries using views is to find efficient methods of answering a query using a set of previously defined materialized views over the database, rather than accessing the database relations. The problem has recently received significant attention because of its relevance to a wide variety of data management problems. In query optimization, finding a rewriting of a query using a set of materialized views can yield a more efficient query execution plan. To support the separation of the logical and physical views of data, a storage schema can be described using views over the logical schema. As a result, finding a query execution plan that accesses the storage amounts to solving the problem of answering queries using views. Finally, the problem arises in data integration systems, where data sources can be described as precomputed views over a mediated schema. This article surveys the state of the art on the problem of answering queries using views, and synthesizes the disparate works into a coherent framework. We describe the different applications of the problem, the algorithms proposed to solve it and the relevant theoretical results.},
	language = {en},
	number = {4},
	urldate = {2018-11-20},
	journal = {The VLDB Journal},
	author = {Halevy, Alon Y.},

	year = {2001},
	keywords = {Key words: Materialized views – Data integration – Query optimization – Survey – Date warehousing – Web-site management, cscheid-survey, leibatt-materialized-views, leibatt-survey},
	pages = {270--294}
}

@inproceedings{idreos_overview_2015,
	address = {New York, NY, USA},
	series = {{SIGMOD} '15},
	title = {Overview of {Data} {Exploration} {Techniques}},
	isbn = {978-1-4503-2758-9},
	url = {http://doi.acm.org/10.1145/2723372.2731084},
	doi = {10.1145/2723372.2731084},
	abstract = {Data exploration is about efficiently extracting knowledge from data even if we do not know exactly what we are looking for. In this tutorial, we survey recent developments in the emerging area of database systems tailored for data exploration. We discuss new ideas on how to store and access data as well as new ideas on how to interact with a data system to enable users and applications to quickly figure out which data parts are of interest. In addition, we discuss how to exploit lessons-learned from past research, the new challenges data exploration crafts, emerging applications and future research directions.},
	urldate = {2018-11-20},
	booktitle = {Proceedings of the 2015 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Idreos, Stratos and Papaemmanouil, Olga and Chaudhuri, Surajit},
	year = {2015},
	keywords = {data, exploration, cscheid-survey, leibatt-survey},
	pages = {277--281}
}

@article{chaudhuri_overview_1997,
	title = {An {Overview} of {Data} {Warehousing} and {OLAP} {Technology}},
	volume = {26},
	issn = {0163-5808},
	url = {http://doi.acm.org/10.1145/248603.248616},
	doi = {10.1145/248603.248616},
	abstract = {Data warehousing and on-line analytical processing (OLAP) are essential elements of decision support, which has increasingly become a focus of the database industry. Many commercial products and services are now available, and all of the principal database management system vendors now have offerings in these areas. Decision support places some rather different requirements on database technology compared to traditional on-line transaction processing applications. This paper provides an overview of data warehousing and OLAP technologies, with an emphasis on their new requirements. We describe back end tools for extracting, cleaning and loading data into a data warehouse; multidimensional data models typical of OLAP; front end client tools for querying and data analysis; server extensions for efficient query processing; and tools for metadata management and for managing the warehouse. In addition to surveying the state of the art, this paper also identifies some promising research issues, some of which are related to problems that the database research community has worked on for years, but others are only just beginning to be addressed. This overview is based on a tutorial that the authors presented at the VLDB Conference, 1996.},
	number = {1},
	urldate = {2018-11-20},
	journal = {SIGMOD Rec.},
	author = {Chaudhuri, Surajit and Dayal, Umeshwar},

	year = {1997},
	keywords = {cscheid-survey, leibatt-materialized-views, leibatt-aggregation, leibatt-survey},
	pages = {65--74}
}

@article{gray_data_1997,
	title = {Data {Cube}: {A} {Relational} {Aggregation} {Operator} {Generalizing} {Group}-{By}, {Cross}-{Tab}, and {Sub}-{Totals}},
	volume = {1},
	issn = {1573-756X},
	shorttitle = {Data {Cube}},
	url = {https://doi.org/10.1023/A:1009726021843},
	doi = {10.1023/A:1009726021843},
	abstract = {Data analysis applications typically aggregate data across manydimensions looking for anomalies or unusual patterns. The SQL aggregatefunctions and the GROUP BY operator produce zero-dimensional orone-dimensional aggregates. Applications need the N-dimensionalgeneralization of these operators. This paper defines that operator, calledthe data cube or simply cube. The cube operator generalizes the histogram,cross-tabulation, roll-up,drill-down, and sub-total constructs found in most report writers.The novelty is that cubes are relations. Consequently, the cubeoperator can be imbedded in more complex non-procedural dataanalysis programs. The cube operator treats each of the Naggregation attributes as a dimension of N-space. The aggregate ofa particular set of attribute values is a point in this space. Theset of points forms an N-dimensional cube. Super-aggregates arecomputed by aggregating the N-cube to lower dimensional spaces.This paper (1) explains the cube and roll-up operators, (2) showshow they fit in SQL, (3) explains how users can define new aggregatefunctions for cubes, and (4) discusses efficient techniques tocompute the cube. Many of these features are being added to the SQLStandard.},
	language = {en},
	number = {1},
	urldate = {2018-11-20},
	journal = {Data Mining and Knowledge Discovery},
	author = {Gray, Jim and Chaudhuri, Surajit and Bosworth, Adam and Layman, Andrew and Reichart, Don and Venkatrao, Murali and Pellow, Frank and Pirahesh, Hamid},

	year = {1997},
	keywords = {data cube, data mining, database, query, aggregation, analysis, summarization, cscheid-materialized-views, leibatt-materialized-views, leibatt-aggregation, leibatt-interaction-aggregate, leibatt-interaction-filter, leibatt-interaction-navigate, cscheid-interaction-aggregate, cscheid-interaction-filter},
	pages = {29--53}
}

@inproceedings{riedewald_space-efficient_2000,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Space-{Efficient} {Data} {Cubes} for {Dynamic} {Environments}},
	isbn = {978-3-540-44466-4},
	url = {https://link.springer.com/chapter/10.1007/3-540-44466-1_3},
	abstract = {Data cubes provide aggregate information to support the analysis of the contents of data warehouses and databases. An important tool to analyze data in data cubes is the range query. For range queries that summarize large regions of massive data cubes, computing the query result on-the-fly can result in non-interactive response times. To speed up range queries, values that summarize regions of the data cube are precomputed and stored. This faster response time results in more expensive updates and/or space overhead. While the emphasis is typically on low query and update costs, growing data collections increase the demand for space-efficient approaches. In this paper two techniques are presented that have the same update and query costs as earlier approaches, without introducing any space overhead.},
	language = {en},
	booktitle = {Data {Warehousing} and {Knowledge} {Discovery}},
	publisher = {Springer Berlin Heidelberg},
	author = {Riedewald, Mirek and Agrawal, Divyakant and Abbadi, Amr El and Pajarola, Renato},
	editor = {Kambayashi, Yahiko and Mohania, Mukesh and Tjoa, A. Min},
	year = {2000},
	keywords = {cscheid-materialized-views, leibatt-materialized-views, leibatt-aggregation, leibatt-interaction-aggregate, leibatt-interaction-filter, leibatt-interaction-select, leibatt-interaction-navigate, cscheid-interaction-aggregate, cscheid-interaction-filter},
	pages = {24--33}
}

@inproceedings{cuzzocrea_hierarchy-driven_2006,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {A {Hierarchy}-{Driven} {Compression} {Technique} for {Advanced} {OLAP} {Visualization} of {Multidimensional} {Data} {Cubes}},
	isbn = {978-3-540-37737-5},
	url = {https://link.springer.com/chapter/10.1007/11823728_11},
	abstract = {In this paper, we investigate the problem of visualizing multidimensional data cubes, and propose a novel technique for supporting advanced OLAP visualization of such data structures. Founding on very efficient data compression solutions for two-dimensional data domains, the proposed technique relies on the amenity of generating “semantics-aware” compressed representation of two-dimensional OLAP views extracted from multidimensional data cubes via the so-called OLAP dimension flattening process. A wide set of experimental results conducted on several kind of synthetic two-dimensional OLAP views clearly confirm the effectiveness and the efficiency of our technique, also in comparison with state-of-the-art proposals.},
	language = {en},
	booktitle = {Data {Warehousing} and {Knowledge} {Discovery}},
	publisher = {Springer Berlin Heidelberg},
	author = {Cuzzocrea, Alfredo and Saccà, Domenico and Serafino, Paolo},
	editor = {Tjoa, A. Min and Trujillo, Juan},
	year = {2006},
	keywords = {Data Cube, Multidimensional Database, OLAP Query, Range Query, Splitting Position, cscheid-materialized-views, leibatt-materialized-views, leibatt-aggregation, leibatt-compression, leibatt-interaction-aggregate, leibatt-interaction-filter, leibatt-interaction-navigate, cscheid-interaction-aggregate, cscheid-interaction-navigate, cscheid-interaction-filter, cscheid-aggregation, cscheid-interaction-select},
	pages = {106--119}
}

@inproceedings{sarawagi_i3:_2000,
	address = {New York, NY, USA},
	series = {{SIGMOD} '00},
	title = {I3: {Intelligent}, {Interactive} {Investigation} of {OLAP} {Data} {Cubes}},
	isbn = {978-1-58113-217-5},
	shorttitle = {I3},
	url = {http://doi.acm.org/10.1145/342009.336564},
	doi = {10.1145/342009.336564},
	abstract = {The goal of the i3(eye cube) project is to enhance multidimensional database products with a suite of advanced operators to automate data analysis tasks that are currently handled through manual exploration. Most OLAP products are rather simplistic and rely heavily on the user's intuition to manually drive the discovery process. Such ad hoc user-driven exploration gets tedious and error-prone as data dimensionality and size increases. We first investigated how and why analysts currently explore the data cube and then automated them using advanced operators that can be invoked interactively like existing simple operators.
Our proposed suite of extensions appear in the form of a toolkit attached with a OLAP product. At this demo we will present three such operators: DIFF, RELAX and INFORM with illustrations from real-life datasets.},
	urldate = {2018-11-20},
	booktitle = {Proceedings of the 2000 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Sarawagi, Sunita and Sathe, Gayatri},
	year = {2000},
	keywords = {cscheid-materialized-views, cscheid-user-modeling, leibatt-materialized-views, leibatt-aggregation, leibatt-user-modeling, leibatt-interaction-aggregate, leibatt-interaction-filter, leibatt-interaction-navigate, cscheid-interaction-aggregate},
	pages = {589--}
}

@article{liang_range_2000,
	title = {Range queries in dynamic {OLAP} data cubes},
	volume = {34},
	issn = {0169-023X},
	url = {http://www.sciencedirect.com/science/article/pii/S0169023X00000070},
	doi = {10.1016/S0169-023X(00)00007-0},
	abstract = {A range query applies an aggregation operation (e.g., SUM) over all selected cells of an OLAP data cube where the selection is specified by providing ranges of values for numeric dimensions. Range sum queries on data cubes are a powerful analysis tool. Many application domains require that data cubes are updated often and the information provided by analysis tools are current or “near current”. Existing techniques for range sum queries on data cubes, however, can incur update costs in the order of the size of the data cube. Since the size of a data cube is exponential in the number of its dimensions, rebuilding the entire data cube can be very costly and is not realistic. To cope with this dynamic data cube problem, a new approach has been introduced recently, which achieves constant time per range sum query while constraining each update cost within O(nd/2), where d is the number of dimensions of the data cube and n is the number of distinct values of the domain at each dimension. In this paper, we provide a new algorithm for the problem which requires O(n1/3) time for each range sum query and O(nd/3) time for each update. Our algorithm improves the update time by a factor of O(nd/6) in contrast to the current one for the problem O(nd/2). Like all existing techniques, our approach to answering range sum queries is also based on some precomputed auxiliary information (prefix sums) that is used to answer ad hoc queries at run time. Under both the product model and a new model introduced in this paper, the total cost for updates and range queries of the proposed algorithm is smallest compared with the cost by all known algorithms. Therefore our algorithm reduces the overall time complexity for range sum queries significantly.},
	number = {1},
	urldate = {2018-11-20},
	journal = {Data \& Knowledge Engineering},
	author = {Liang, Weifa and Wang, Hui and Orlowska, Maria E.},

	year = {2000},
	keywords = {OLAP, Data cube, Multidimensional database, Precomputation, Query algorithm, Range sum query, The update on-the-fly, cscheid-materialized-views, leibatt-materialized-views, leibatt-aggregation, leibatt-interaction-aggregate, leibatt-interaction-filter, leibatt-interaction-select, leibatt-interaction-navigate, cscheid-interaction-aggregate, cscheid-interaction-navigate, cscheid-interaction-filter},
	pages = {21--38}
}

@inproceedings{riedewald_flexible_2001,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Flexible {Data} {Cubes} for {Online} {Aggregation}},
	isbn = {978-3-540-44503-6},
	abstract = {Applications like Online Analytical Processing depend heavily on the ability to quickly summarize large amounts of information. Techniques were proposed recently that speed up aggregate range queries on MOLAP data cubes by storing pre-computed aggregates. These approaches try to handle data cubes of any dimensionality by dealing with all dimensions at the same time and treat the different dimensions uniformly. The algorithms are typically complex, and it is difficult to prove their correctness and to analyze their performance. We present a new technique to generate Iterative Data Cubes (IDC) that addresses these problems. The proposed approach provides a modular framework for combining one-dimensional aggregation techniques to create space-optimal high-dimensional data cubes. A large variety of cost tradeoffs for high-dimensional IDC can be generated, making it easy to find the right configuration based on the application requirements.},
	language = {en},
	booktitle = {Database {Theory} — {ICDT} 2001},
	publisher = {Springer Berlin Heidelberg},
	author = {Riedewald, Mirek and Agrawal, Divyakant and El Abbadi, Amr},
	editor = {Van den Bussche, Jan and Vianu, Victor},
	year = {2001},
	keywords = {cscheid-materialized-views, leibatt-materialized-views, leibatt-aggregation, leibatt-interaction-aggregate, leibatt-interaction-filter, leibatt-interaction-navigate, cscheid-interaction-aggregate, cscheid-interaction-filter},
	pages = {159--173}
}

@inproceedings{geffner_relative_1999,
	title = {Relative prefix sums: an efficient approach for querying dynamic {OLAP} data cubes},
	shorttitle = {Relative prefix sums},
	doi = {10.1109/ICDE.1999.754948},
	abstract = {Range sum queries on data cubes are a powerful tool for analysis. A range sum query applies an aggregation operation (e.g., SUM) over all selected cells in a data cube, where the selection is specified by providing ranges of values for numeric dimensions. Many application domains require that information provided by analysis tools be current or "near-current." Existing techniques for range sum queries on data cubes, however, can incur update costs on the order of the size of the data cube. Since the size of a data cube is exponential in the number of its dimensions, rebuilding the entire data cube can be very costly. We present an approach that achieves constant time range sum queries while constraining update costs. Our method reduces the overall complexity of the range sum problem.},
	booktitle = {Proceedings 15th {International} {Conference} on {Data} {Engineering} ({Cat}. {No}.{99CB36337})},
	author = {Geffner, S. and Agrawal, D. and Abbadi, A. El and Smith, T.},

	year = {1999},
	keywords = {query processing, Data analysis, computational complexity, data mining, Aggregates, Data warehouses, Marketing and sales, Multidimensional systems, Databases, aggregation operation, analysis tools, application domains, Computer science, constant time range sum queries, Costs, dynamic OLAP data cube querying, Information analysis, Insurance, numeric dimensions, overall complexity, range sum problem, relative prefix sums, update costs, cscheid-materialized-views, leibatt-materialized-views, leibatt-aggregation, leibatt-interaction-aggregate, leibatt-interaction-filter, leibatt-interaction-select, leibatt-interaction-navigate, cscheid-interaction-aggregate, cscheid-interaction-filter},
	pages = {328--335}
}

@article{stefanovic_object-based_2000,
	title = {Object-based selective materialization for efficient implementation of spatial data cubes},
	volume = {12},
	issn = {1041-4347},
	doi = {10.1109/69.895803},
	abstract = {With a huge amount of data stored in spatial databases and the introduction of spatial components to many relational or object-relational databases, it is important to study the methods for spatial data warehousing and OLAP of spatial data. In this paper, we study methods for spatial OLAP, by integrating nonspatial OLAP methods with spatial database implementation techniques. A spatial data warehouse model, which consists of both spatial and nonspatial dimensions and measures, is proposed. Methods for the computation of spatial data cubes and analytical processing on such spatial data cubes are studied, with several strategies being proposed, including approximation and selective materialization of the spatial objects resulting from spatial OLAP operations. The focus of our study is on a method for spatial cube construction, called object-based selective materialization, which is different from cuboid-based selective materialization (proposed in previous studies of nonspatial data cube construction). Rather than using a cuboid as an atomic structure during the selective materialization, we explore granularity on a much finer level: that of a single cell of a cuboid. Several algorithms are proposed for object-based selective materialization of spatial data cubes, and a performance study has demonstrated the effectiveness of these techniques.},
	number = {6},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Stefanovic, N. and Han, Jiawei and Koperski, K.},

	year = {2000},
	keywords = {Relational databases, Data analysis, relational databases, data mining, Data mining, data warehouses, Data warehouses, Computer Society, cuboid cell, Focusing, granularity, Image databases, Land surface temperature, object-based selective materialization, object-oriented databases, object-relational databases, online analytical processing, performance, spatial data analysis, spatial data cube construction, spatial data warehousing, spatial databases, Spatial databases, spatial object approximation, spatial OLAP, visual databases, Warehousing, cscheid-materialized-views, leibatt-materialized-views, leibatt-aggregation, leibatt-user-modeling, leibatt-interaction-aggregate, leibatt-interaction-filter, leibatt-interaction-navigate, cscheid-interaction-aggregate, cscheid-interaction-navigate, cscheid-interaction-filter},
	pages = {938--958}
}

@inproceedings{mumick_maintenance_1997,
	address = {New York, NY, USA},
	series = {{SIGMOD} '97},
	title = {Maintenance of {Data} {Cubes} and {Summary} {Tables} in a {Warehouse}},
	isbn = {978-0-89791-911-1},
	url = {http://doi.acm.org/10.1145/253260.253277},
	doi = {10.1145/253260.253277},
	abstract = {Data warehouses contain large amounts of information, often collected from a variety of independent sources. Decision-support functions in a warehouse, such as on-line analytical processing (OLAP), involve hundreds of complex aggregate queries over large volumes of data. It is not feasible to compute these queries by scanning the data sets each time. Warehouse applications therefore build a large number of summary tables, or materialized aggregate views, to help them increase the system performance.
As changes, most notably new transactional data, are collected at the data sources, all summary tables at the warehouse that depend upon this data need to be updated. Usually, source changes are loaded into the warehouse at regular intervals, usually once a day, in a batch window, and the warehouse is made unavailable for querying while it is updated. Since the number of summary tables that need to be maintained is often large, a critical issue for data warehousing is how to maintain the summary tables efficiently.
In this paper we propose a method of maintaining aggregate views (the summary-delta table method), and use it to solve two problems in maintaining summary tables in a warehouse: (1) how to efficiently maintain a summary table while minimizing the batch window needed for maintenance, and (2) how to maintain a large set of summary tables defined over the same base tables.
While several papers have addressed the issues relating to choosing and materializing a set of summary tables, this is the first paper to address maintaining summary tables efficiently.},
	urldate = {2018-11-20},
	booktitle = {Proceedings of the 1997 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Mumick, Inderpal Singh and Quass, Dallan and Mumick, Barinderpal Singh},
	year = {1997},
	keywords = {cscheid-materialized-views, leibatt-materialized-views, leibatt-aggregation, leibatt-interaction-aggregate, leibatt-interaction-navigate, cscheid-interaction-aggregate, cscheid-interaction-change},
	pages = {100--111}
}

@inproceedings{sarawagi_discovery-driven_1998,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Discovery-driven exploration of {OLAP} data cubes},
	isbn = {978-3-540-69709-1},
	url = {https://link.springer.com/chapter/10.1007/BFb0100984},
	abstract = {Analysts predominantly use OLAP data cubes to identify regions of anomalies that may represent problem areas or new opportunities. The current OLAP systems support hypothesis-driven exploration of data cubes through operations such as drill-down, roll-up, and selection. Using these operations, an analyst navigates unaided through a huge search space looking at large number of values to spot exceptions. We propose a new discovery-driven exploration paradigm that mines the data for such exceptions and summarizes the exceptions at appropriate levels in advance. It then uses these exceptions to lead the analyst to interesting regions of the cube during navigation. We present the statistical foundation underlying our approach. We then discuss the computational issue of finding exceptions in data and making the process efficient on large multidimensional data bases.},
	language = {en},
	booktitle = {Advances in {Database} {Technology} — {EDBT}'98},
	publisher = {Springer Berlin Heidelberg},
	author = {Sarawagi, Sunita and Agrawal, Rakesh and Megiddo, Nimrod},
	editor = {Schek, Hans-Jörg and Alonso, Gustavo and Saltor, Felix and Ramos, Isidro},
	year = {1998},
	keywords = {Data Cube, Aggregate Function, Cube Computation, Huge Search Space, Multiple Equation, cscheid-materialized-views, leibatt-aqp, cscheid-user-modeling, leibatt-materialized-views, leibatt-aggregation, leibatt-user-modeling, leibatt-interaction-aggregate, leibatt-interaction-filter, leibatt-interaction-navigate, cscheid-interaction-aggregate, cscheid-interaction-navigate, cscheid-interaction-filter},
	pages = {168--182}
}

@inproceedings{mendelzon_maintaining_1999,
	title = {Maintaining {Data} {Cubes} under {Dimension} {Updates}},
	isbn = {978-0-7695-0071-3},
	url = {doi.ieeecomputersociety.org/10.1109/ICDE.1999.754950},
	doi = {10.1109/ICDE.1999.754950},
	abstract = {OLAP systems support data analysis through a multidimensional data model, according to which data facts are viewed as points in a space of application-related "dimensions", organized into levels which conform a hierarchy. The usual assumption is that the data points reflect the dynamic aspect of the data warehouse, while dimensions are relatively static. However, in practice, dimension updates are often necessary to adapt the multidimensional database to changing requirements. Structural updates can also take place, like addition of categories or modification of the hierarchical structure. When these updates are performed, the materialized aggregate views that are typically stored in OLAP systems must be efficiently maintained. These updates are poorly supported (or not supported at all) in current commercial systems, and have received little attention in the research literature. We present a formal model of dimension updates in a multidimensional model, a collection of primitive operators to perform them, and a study of the effect of these updates on a class of materialized views, giving an algorithm to efficiently maintain them.},
	urldate = {2018-11-20},
	booktitle = {Proceedings 15th {International} {Conference} on {Data} {Engineering} ({Cat}. {No}.{99CB36337})({ICDE})},
	author = {Mendelzon, A. O. and Vaisman, A. A. and Hurtado, C. A.},
	year = {1999},
	keywords = {cscheid-materialized-views, leibatt-materialized-views, leibatt-aggregation, leibatt-interaction-aggregate, leibatt-interaction-navigate, cscheid-interaction-aggregate, cscheid-interaction-change},
	pages = {346}
}

@article{stolte_multiscale_2003,
	title = {Multiscale visualization using data cubes},
	volume = {9},
	issn = {1077-2626},
	doi = {10.1109/TVCG.2003.1196005},
	abstract = {Most analysts start with an overview of the data before gradually refining their view to be more focused and detailed. Multiscale pan-and-zoom systems are effective because they directly support this approach. However, generating abstract overviews of large data sets is difficult and most systems take advantage of only one type of abstraction: visual abstraction. Furthermore, these existing systems limit the analyst to a single zooming path on their data and thus to a single set of abstract views. This paper presents: 1) a formalism for describing multiscale visualizations of data cubes with both data and visual abstraction and 2) a method for independently zooming along one or more dimensions by traversing a zoom graph with nodes at different levels of detail. As an example of how to design multiscale visualizations using our system, we describe four design patterns using our formalism. These design patterns show the effectiveness of multiscale visualization of general relational databases.},
	number = {2},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Stolte, C. and Tang, D. and Hanrahan, P.},

	year = {2003},
	keywords = {Relational databases, Data analysis, data visualisation, Data visualization, Visual databases, relational databases, data cubes, data structures, visual databases, database visualization, Filtering, Filters, graphic formalism, Graphics, multiscale visualization, Polarization, Switches, visual abstraction, zoom graph, cscheid-materialized-views, leibatt-materialized-views, leibatt-aggregation, leibatt-maybe, leibatt-interaction-aggregate, leibatt-interaction-filter, leibatt-interaction-navigate, cscheid-interaction-aggregate, cscheid-interaction-navigate, cscheid-interaction-filter},
	pages = {176--187}
}

@inproceedings{harinarayan_implementing_1996,
	address = {New York, NY, USA},
	series = {{SIGMOD} '96},
	title = {Implementing {Data} {Cubes} {Efficiently}},
	isbn = {978-0-89791-794-0},
	url = {http://doi.acm.org/10.1145/233269.233333},
	doi = {10.1145/233269.233333},
	abstract = {Decision support applications involve complex queries on very large databases. Since response times should be small, query optimization is critical. Users typically view the data as multidimensional data cubes. Each cell of the data cube is a view consisting of an aggregation of interest, like total sales. The values of many of these cells are dependent on the values of other cells in the data cube. A common and powerful query optimization technique is to materialize some or all of these cells rather than compute them from raw data each time. Commercial systems differ mainly in their approach to materializing the data cube. In this paper, we investigate the issue of which cells (views) to materialize when it is too expensive to materialize all views. A lattice framework is used to express dependencies among views. We present greedy algorithms that work off this lattice and determine a good set of views to materialize. The greedy algorithm performs within a small constant factor of optimal under a variety of models. We then consider the most common case of the hypercube lattice and examine the choice of materialized views for hypercubes in detail, giving some good tradeoffs between the space used and the average time to answer a query.},
	urldate = {2018-11-20},
	booktitle = {Proceedings of the 1996 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Harinarayan, Venky and Rajaraman, Anand and Ullman, Jeffrey D.},
	year = {1996},
	keywords = {cscheid-materialized-views-aggregate, leibatt-materialized-views, leibatt-aggregation, leibatt-interaction-aggregate, leibatt-interaction-navigate, cscheid-interaction-aggregate, cscheid-interaction-filter},
	pages = {205--216}
}

@inproceedings{buccafurri_quad-tree_2003,
	title = {A quad-tree based multiresolution approach for two-dimensional summary data},
	booktitle = {15th {International} {Conference} on {Scientific} and {Statistical} {Database} {Management}, 2003.},
	publisher = {IEEE},
	author = {Buccafurri, Francesco and Furfaro, Filippo and Sacca, Domenico and Sirangelo, Cristina},
	year = {2003},
	keywords = {cscheid-materialized-views, cscheid-spatial-index, cscheid-interaction-aggregate, cscheid-interaction-navigate, cscheid-interaction-filter, cscheid-index},
	pages = {127--137}
}

@inproceedings{park_visualization-aware_2016,
	title = {Visualization-aware sampling for very large databases},
	url = {https://ieeexplore.ieee.org/document/7498287},
	doi = {10.1109/ICDE.2016.7498287},
	abstract = {Interactive visualizations are crucial in ad hoc data exploration and analysis. However, with the growing number of massive datasets, generating visualizations in interactive timescales is increasingly challenging. One approach for improving the speed of the visualization tool is via data reduction in order to reduce the computational overhead, but at a potential cost in visualization accuracy. Common data reduction techniques, such as uniform and stratified sampling, do not exploit the fact that the sampled tuples will be transformed into a visualization for human consumption. We propose a visualization-aware sampling (VAS) that guarantees high quality visualizations with a small subset of the entire dataset. We validate our method when applied to scatter and map plots for three common visualization goals: regression, density estimation, and clustering. The key to our sampling method's success is in choosing a set of tuples that minimizes a visualization-inspired loss function. While existing sampling approaches minimize the error of aggregation queries, we focus on a loss function that maximizes the visual fidelity of scatter plots. Our user study confirms that our proposed loss function correlates strongly with user success in using the resulting visualizations. Our experiments show that (i) VAS improves user's success by up to 35\% in various visualization tasks, and (ii) VAS can achieve a required visualization quality up to 400× faster.},
	booktitle = {2016 {IEEE} 32nd {International} {Conference} on {Data} {Engineering} ({ICDE})},
	author = {Park, Y. and Cafarella, M. and Mozafari, B.},

	year = {2016},
	keywords = {data analysis, data visualisation, Data visualization, Visual databases, very large databases, interactive systems, interactive visualizations, cscheid-materialized-views, ad hoc data analysis, ad hoc data exploration, aggregation queries, clustering, data reduction, density estimation, Estimation, estimation theory, Human computer interaction, pattern clustering, Query processing, regression, regression analysis, sampling methods, scatter plots visual fidelity, Software architecture, stratified sampling, uniform sampling, visualization-aware sampling, visualization-inspired loss function, cscheid-aqp, leibatt-aqp, cscheid-precomputed-samples, cscheid-interaction-navigate, cscheid-interaction-filter, cscheid-interaction-encode},
	pages = {755--766}
}

@inproceedings{williams_steerable_2004-1,
	title = {Steerable, {Progressive} {Multidimensional} {Scaling}},
	doi = {10.1109/INFVIS.2004.60},
	abstract = {Current implementations of multidimensional scaling (MDS), an approach that attempts to best represent data point similarity in a low-dimensional representation, are not suited for many of today's large-scale datasets. We propose an extension to the spring model approach that allows the user to interactively explore datasets that are far beyond the scale of previous implementations of MDS. We present MDSteer, a steerable MDS computation engine and visualization tool that progressively computes an MDS layout and handles datasets of over one million points. Our technique employs hierarchical data structures and progressive layouts to allow the user to steer the computation of the algorithm to the interesting areas of the dataset. The algorithm iteratively alternates between a layout stage in which a subselection of points are added to the set of active points affected by the MDS iteration, and a binning stage which increases the depth of the bin hierarchy and organizes the currently unplaced points into separate spatial regions. This binning strategy allows the user to select onscreen regions of the layout to focus the MDS computation into the areas of the dataset that are assigned to the selected bins. We show both real and common synthetic benchmark datasets with dimensionalities ranging from 3 to 300 and cardinalities of over one million points},
	booktitle = {{IEEE} {Symposium} on {Information} {Visualization}},
	author = {Williams, M. and Munzner, T.},

	year = {2004},
	keywords = {data visualisation, Data visualization, dimensionality reduction, multidimensional scaling, computational complexity, very large databases, Multidimensional systems, graphical user interfaces, interactive systems, data structures, Data structures, Costs, leibatt-progressive, bin hierarchy, Chromium, computational geometry, data point similarity, data visualization tool, Engines, hierarchical data structures, Iterative algorithms, iterative methods, Large-scale systems, MDSteer, progressive multidimensional scaling, Psychology, spring model approach, Springs, leibatt-aqp, cscheid-maybe, leibatt-interaction-aggregate, leibatt-interaction-select},
	pages = {57--64}
}

@inproceedings{alabi_pfunk-h:_2016,
	address = {New York, NY, USA},
	series = {{HILDA} '16},
	title = {{PFunk}-{H}: {Approximate} {Query} {Processing} {Using} {Perceptual} {Models}},
	isbn = {978-1-4503-4207-0},
	shorttitle = {{PFunk}-{H}},
	url = {http://doi.acm.org/10.1145/2939502.2939512},
	doi = {10.1145/2939502.2939512},
	abstract = {Interactive visualization tools (e.g., crossfilter) are critical to many data analysts by making the discovery and verification of hypotheses quick and seamless. Increasing data sizes has made the scalability of these tools a necessity. To bridge the gap between data sizes and interactivity, many visualization systems have turned to sampling-based approximate query processing frameworks. However, these systems are currently oblivious to human perceptual visual accuracy. This could either lead to overly aggressive sampling when the approximation accuracy is higher than needed or an incorrect visual rendering when the accuracy is too lax. Thus, for both correctness and efficiency, we propose to use empirical knowledge of human perceptual limitations to automatically bound the error of approximate answers meant for visualization. This paper explores a preliminary model of sampling-based approximate query processing that uses perceptual models (encoded as functions) to construct approximate answers intended for visualization. We present initial results that show that the approximate and non-approximate answers for a given query differ by a perceptually indiscernible amount, as defined by perceptual functions.},
	urldate = {2019-02-23},
	booktitle = {Proceedings of the {Workshop} on {Human}-{In}-the-{Loop} {Data} {Analytics}},
	publisher = {ACM},
	author = {Alabi, Daniel and Wu, Eugene},
	year = {2016},
	note = {event-place: San Francisco, California},
	keywords = {leibatt-progressive, cscheid-aqp, leibatt-aqp, cscheid-progressive, leibatt-interaction-aggregate, leibatt-interaction-filter, cscheid-interaction-aggregate, cscheid-interaction-filter, cscheid-interaction-encode},
	pages = {10:1--10:6}
}

@inproceedings{chaudhuri_robust_2001,
	address = {New York, NY, USA},
	series = {{SIGMOD} '01},
	title = {A {Robust}, {Optimization}-based {Approach} for {Approximate} {Answering} of {Aggregate} {Queries}},
	isbn = {978-1-58113-332-5},
	url = {http://doi.acm.org/10.1145/375663.375694},
	doi = {10.1145/375663.375694},
	abstract = {The ability to approximately answer aggregation queries accurately and efficiently is of great benefit for decision support and data mining tools. In contrast to previous sampling-based studies, we treat the problem as an optimization problem whose goal is to minimize the error in answering queries in the given workload. A key novelty of our approach is that we can tailor the choice of samples to be robust even for workloads that are “similar” but not necessarily identical to the given workload. Finally, our techniques recognize the importance of taking into account the variance in the data distribution in a principled manner. We show how our solution can be implemented on a database system, and present results of extensive experiments on Microsoft SQL Server 2000 that demonstrate the superior quality of our method compared to previous work.},
	urldate = {2019-02-23},
	booktitle = {Proceedings of the 2001 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Chaudhuri, Surajit and Das, Gautam and Narasayya, Vivek},
	year = {2001},
	note = {event-place: Santa Barbara, California, USA},
	keywords = {cscheid-materialized-views, cscheid-aqp, leibatt-aqp, cscheid-precomputed-samples, cscheid-select, leibatt-materialized-views, leibatt-interaction-aggregate, leibatt-interaction-filter, cscheid-interaction-aggregate, cscheid-interaction-filter},
	pages = {295--306}
}

@inproceedings{peng_aqp++:_2018,
	title = {{AQP}++: {Connecting} {Approximate} {Query} {Processing} {With} {Aggregate} {Precomputation} for {Interactive} {Analytics}},
	isbn = {978-1-4503-4703-7},
	shorttitle = {{AQP}++},
	url = {http://dl.acm.org/citation.cfm?id=3183713.3183747},
	doi = {10.1145/3183713.3183747},
	urldate = {2019-04-26},
	publisher = {ACM},
	author = {Peng, Jinglin and Zhang, Dongxiang and Wang, Jiannan and Pei, Jian},

	year = {2018},
	keywords = {cscheid-materialized-views, cscheid-aqp, leibatt-aqp, leibatt-materialized-views, leibatt-aggregation, leibatt-interaction-aggregate, leibatt-interaction-filter, leibatt-interaction-select, cscheid-interaction-filter, cscheid-interaction-select},
	pages = {1477--1492}
}

@article{kwon_sampling_2017,
	title = {Sampling for {Scalable} {Visual} {Analytics}},
	volume = {37},
	issn = {0272-1716},
	doi = {10.1109/MCG.2017.6},
	abstract = {Sampling is becoming an essential tool for scalable interactive visual analysis. After outlining prior work by the database community on sampling for visualization of aggregation queries, this article considers how these results might be improved and extended to a broader setting. The goal is to better understand how users interact with sampling to enable wider adoption of sampling for scalable visual analytics.},
	number = {1},
	journal = {IEEE Computer Graphics and Applications},
	author = {Kwon, B. C. and Verma, J. and Haas, P. J. and Demiralp, Ç},

	year = {2017},
	keywords = {data analysis, data visualisation, Data visualization, Visual databases, sampling, online aggregation, interactive systems, aggregation queries, sampling methods, cscheid-aqp, computer graphics, interactive visual analytics, leibatt-aqp, scalable interactive visual analysis, scalable visualization, Temperature sensors, Uncertainty, Visual analytics, visualization, cscheid-maybe, leibatt-survey, leibatt-interaction-aggregate, leibatt-interaction-filter, leibatt-interaction-select, leibatt-interaction-arrange, leibatt-interaction-derive, leibatt-interaction-navigate},
	pages = {100--108}
}

@inproceedings{dursun_revisiting_2017,
	address = {New York, NY, USA},
	series = {{SIGMOD} '17},
	title = {Revisiting {Reuse} in {Main} {Memory} {Database} {Systems}},
	isbn = {978-1-4503-4197-4},
	url = {http://doi.acm.org/10.1145/3035918.3035957},
	doi = {10.1145/3035918.3035957},
	abstract = {Loading...},
	urldate = {2019-05-31},
	booktitle = {Proceedings of the 2017 {ACM} {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Dursun, Kayhan and Binnig, Carsten and Cetintemel, Ugur and Kraska, TIm},
	year = {2017},
	note = {event-place: Chicago, Illinois, USA},
	keywords = {query optimization, cscheid-materialized-views, cscheid-mqo, hash tables, main-memory database systems, reuse, leibatt-mqo, leibatt-materialized-views, leibatt-interaction-aggregate, leibatt-interaction-filter, leibatt-interaction-select, leibatt-interaction-derive, cscheid-interaction-aggregate, cscheid-interaction-filter, cscheid-interaction-select, cscheid-interaction-derive},
	pages = {1275--1289}
}

@article{tao_kyrix:_2019,
	title = {Kyrix: {Interactive} {Visual} {Data} {Exploration} at {Scale}},
	shorttitle = {Kyrix},
	url = {http://arxiv.org/abs/1905.04638},
	abstract = {Scalable interactive visual data exploration is crucial in many domains due to increasingly large datasets generated at rapid rates. Details-on-demand provides a useful interaction paradigm for exploring large datasets, where users start at an overview, find regions of interest, zoom in to see detailed views, zoom out and then repeat. This paradigm is the primary user interaction mode of widely-used systems such as Google Maps, Aperture Tiles and ForeCache. These earlier systems, however, are highly customized with hardcoded visual representations and optimizations. A more general framework is needed to facilitate the development of visual data exploration systems at scale. In this paper, we present Kyrix, an end-to-end system for developing scalable details-on-demand data exploration applications. Kyrix provides developers with a declarative model for easy specification of general visualizations. Behind the scenes, Kyrix utilizes a suite of performance optimization techniques to achieve a response time within 500ms for various user interactions. We also report results from a performance study which shows that a novel dynamic fetching scheme adopted by Kyrix outperforms tile-based fetching used in earlier systems.},
	urldate = {2019-08-16},
	journal = {arXiv:1905.04638 [cs]},
	author = {Tao, Wenbo and Liu, Xiaoyu and Demiralp, Çağatay and Chang, Remco and Stonebraker, Michael},

	year = {2019},
	note = {arXiv: 1905.04638},
	keywords = {cscheid-materialized-views, Computer Science - Databases, Computer Science - Human-Computer Interaction, leibatt-materialized-views, leibatt-spatial-index, leibatt-interaction-filter, leibatt-interaction-select, leibatt-interaction-navigate, cscheid-spatial-index, cscheid-interaction-navigate, cscheid-interaction-filter, leibatt-index, cscheid-interaction-select, cscheid-index}
}

@article{tao_kyrix:_2019-1,
	title = {Kyrix: {Interactive} {Pan}/{Zoom} {Visualizations} at {Scale}},
	volume = {38},
	copyright = {© 2019 The Author(s) Computer Graphics Forum © 2019 The Eurographics Association and John Wiley \& Sons Ltd. Published by John Wiley \& Sons Ltd.},
	issn = {1467-8659},
	shorttitle = {Kyrix},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.13708},
	doi = {10.1111/cgf.13708},
	abstract = {Pan and zoom are basic yet powerful interaction techniques for exploring large datasets. However, existing zoomable UI toolkits such as Pad++ and ZVTM do not provide the backend database support and data-driven primitives that are necessary for creating large-scale visualizations. This limitation in existing general-purpose toolkits has led to many purpose-built solutions (e.g. Google Maps and ForeCache) that address the issue of scalability but cannot be easily extended to support visualizations beyond their intended data types and usage scenarios. In this paper, we introduce Kyrix to ease the process of creating general and large-scale web-based pan/zoom visualizations. Kyrix is an integrated system that provides the developer with a concise and expressive declarative language along with a backend support for performance optimization of large-scale data. To evaluate the scalability of Kyrix, we conducted a set of benchmarked experiments and show that Kyrix can support high interactivity (with an average latency of 100 ms or below) on pan/zoom visualizations of 100 million data points. We further demonstrate the accessibility of Kyrix through an observational study with 8 developers. Results indicate that developers can quickly learn Kyrix's underlying declarative model to create scalable pan/zoom visualizations. Finally, we provide a gallery of visualizations and show that Kyrix is expressive and flexible in that it can support the developer in creating a wide range of customized visualizations across different application domains and data types.},
	language = {en},
	number = {3},
	urldate = {2019-08-16},
	journal = {Computer Graphics Forum},
	author = {Tao, Wenbo and Liu, Xiaoyu and Wang, Yedi and Battle, Leilani and Demiralp, Çağatay and Chang, Remco and Stonebraker, Michael},
	year = {2019},
	keywords = {cscheid-materialized-views, leibatt-materialized-views, leibatt-spatial-index, leibatt-interaction-filter, leibatt-interaction-select, leibatt-interaction-navigate, cscheid-spatial-index, cscheid-interaction-aggregate, cscheid-interaction-navigate, leibatt-index, cscheid-index},
	pages = {529--540}
}

@inproceedings{battle_making_2016,
	address = {New York, NY, USA},
	series = {{CHI} '16},
	title = {Making {Sense} of {Temporal} {Queries} with {Interactive} {Visualization}},
	isbn = {978-1-4503-3362-7},
	url = {http://doi.acm.org/10.1145/2858036.2858408},
	doi = {10.1145/2858036.2858408},
	abstract = {As real-time monitoring and analysis become increasingly important, researchers and developers turn to data stream management systems (DSMS's) for fast, efficient ways to pose temporal queries over their datasets. However, these systems are inherently complex, and even database experts find it difficult to understand the behavior of DSMS queries. To help analysts better understand these temporal queries, we developed StreamTrace, an interactive visualization tool that breaks down how a temporal query processes a given dataset, step-by-step. The design of StreamTrace is based on input from expert DSMS users; we evaluated the system with a lab study of programmers who were new to streaming queries. Results from the study demonstrate that StreamTrace can help users to verify that queries behave as expected and to isolate the regions of a query that may be causing unexpected results.},
	urldate = {2019-08-16},
	booktitle = {Proceedings of the 2016 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Battle, Leilani and Fisher, Danyel and DeLine, Robert and Barnett, Mike and Chandramouli, Badrish and Goldstein, Jonathan},
	year = {2016},
	note = {event-place: San Jose, California, USA},
	keywords = {data visualization, cscheid-materialized-views, data analysts, streaming data, leibatt-materialized-views, leibatt-provenance, leibatt-interaction-filter, leibatt-interaction-select, cscheid-interaction-filter, cscheid-interaction-select, cscheid-provenance},
	pages = {5433--5443},
	file = {ACM Full Text PDF:/Users/cscheid/Zotero/storage/6UTURDFY/Battle et al. - 2016 - Making Sense of Temporal Queries with Interactive .pdf:application/pdf}
}

@article{jugel_faster_2014,
	title = {Faster {Visual} {Analytics} {Through} {Pixel}-perfect {Aggregation}},
	volume = {7},
	issn = {2150-8097},
	url = {http://dx.doi.org/10.14778/2733004.2733066},
	doi = {10.14778/2733004.2733066},
	abstract = {State-of-the-art visual data analysis tools ignore bandwidth limitations. They fetch millions of records of high-volume time series data from an underlying RDBMS to eventually draw only a few thousand pixels on the screen. In this work, we demonstrate a pixel-aware big data visualization system that dynamically adapts the number of data points transmitted and thus the data rate, while preserving pixel-perfect visualizations. We show how to carefully select the data points to fetch for each pixel of a visualization, using a visualization-driven data aggregation that models the visualization process. Defining all required data reduction operators at the query level, our system trades off a few milliseconds of query execution time for dozens of seconds of data transfer time. The results are significantly reduced response times and a near real-time visualization of millions of data points. Using our pixel-aware system, the audience will be able to enjoy the speed and ease of big data visualizations and learn about the scientific background of our system through an interactive evaluation component, allowing the visitor to measure, visualize, and compare competing visualization-related data reduction techniques.},
	number = {13},
	urldate = {2019-08-16},
	journal = {Proc. VLDB Endow.},
	author = {Jugel, Uwe and Jerzak, Zbigniew and Hackenbroich, Gregor and Markl, Volker},

	year = {2014},
	keywords = {cscheid-aqp, leibatt-aqp, leibatt-aggregation, leibatt-interaction-aggregate, leibatt-interaction-filter, cscheid-interaction-aggregate, cscheid-interaction-filter, cscheid-interaction-encode},
	pages = {1705--1708}
}

@article{jugel_m4:_2014,
	title = {M4: {A} {Visualization}-oriented {Time} {Series} {Data} {Aggregation}},
	volume = {7},
	issn = {2150-8097},
	shorttitle = {M4},
	url = {http://dx.doi.org/10.14778/2732951.2732953},
	doi = {10.14778/2732951.2732953},
	abstract = {Visual analysis of high-volume time series data is ubiquitous in many industries, including finance, banking, and discrete manufacturing. Contemporary, RDBMS-based systems for visualization of high-volume time series data have difficulty to cope with the hard latency requirements and high ingestion rates of interactive visualizations. Existing solutions for lowering the volume of time series data disregard the semantics of visualizations and result in visualization errors. In this work, we introduce M4, an aggregation-based time series dimensionality reduction technique that provides error-free visualizations at high data reduction rates. Focusing on line charts, as the predominant form of time series visualization, we explain in detail the drawbacks of existing data reduction techniques and how our approach outperforms state of the art, by respecting the process of line rasterization. We describe how to incorporate aggregation-based dimensionality reduction at the query level in a visualization-driven query rewriting system. Our approach is generic and applicable to any visualization system that uses an RDBMS as data source. Using real world data sets from high tech manufacturing, stock markets, and sports analytics domains we demonstrate that our visualization-oriented data aggregation can reduce data volumes by up to two orders of magnitude, while preserving perfect visualizations.},
	number = {10},
	urldate = {2019-08-16},
	journal = {Proc. VLDB Endow.},
	author = {Jugel, Uwe and Jerzak, Zbigniew and Hackenbroich, Gregor and Markl, Volker},

	year = {2014},
	keywords = {dimensionality reduction, relational databases, cscheid-aqp, leibatt-aqp, line rasterization, query rewriting, leibatt-aggregation, leibatt-interaction-aggregate, leibatt-interaction-filter, cscheid-query-rewrite, cscheid-interaction-aggregate, cscheid-interaction-encode},
	pages = {797--808}
}

@inproceedings{zukowski_cooperative_2007,
	series = {{VLDB} '07},
	title = {Cooperative {Scans}: {Dynamic} {Bandwidth} {Sharing} in a {DBMS}},
	isbn = {978-1-59593-649-3},
	shorttitle = {Cooperative {Scans}},
	url = {http://dl.acm.org/citation.cfm?id=1325851.1325934},
	abstract = {This paper analyzes the performance of concurrent (index) scan operations in both record (NSM/PAX) and column (DSM) disk storage models and shows that existing scheduling policies do not fully exploit data-sharing opportunities and therefore result in poor disk bandwidth utilization. We propose the Cooperative Scans framework that enhances performance in such scenarios by improving data-sharing between concurrent scans. It performs dynamic scheduling of queries and their data requests, taking into account the current system situation. We first present results on top of an NSM/PAX storage layout, showing that it achieves significant performance improvements over traditional policies in terms of both the number of I/Os and overall execution time, as well as latency of individual queries. We provide benchmarks with varying system parameters, data sizes and query loads to confirm the improvement occurs in a wide range of scenarios. Then we extend our proposal to a more complicated DSM scenario, discussing numerous problems related to the two-dimensional nature of disk scheduling in column stores.},
	urldate = {2019-08-23},
	booktitle = {Proceedings of the 33rd {International} {Conference} on {Very} {Large} {Data} {Bases}},
	publisher = {VLDB Endowment},
	author = {Zukowski, Marcin and Héman, Sándor and Nes, Niels and Boncz, Peter},
	year = {2007},
	note = {event-place: Vienna, Austria},
	keywords = {cscheid-mqo, leibatt-mqo, leibatt-interaction-filter, leibatt-interaction-select, cscheid-interaction-filter, cscheid-interaction-select, cscheid-interaction-encode},
	pages = {723--734}
}

@article{qiao_main-memory_2008,
	title = {Main-memory {Scan} {Sharing} for {Multi}-core {CPUs}},
	volume = {1},
	issn = {2150-8097},
	url = {http://dx.doi.org/10.14778/1453856.1453924},
	doi = {10.14778/1453856.1453924},
	abstract = {Computer architectures are increasingly based on multi-core CPUs and large memories. Memory bandwidth, which has riot kept pace with the increasing number of cores, has become the primary processing bottleneck, replacing disk I/O as the limiting factor. To address this challenge, we provide novel algorithms for increasing the throughput of Business Intelligence (BI) queries, as well as for ensuring fairness and avoiding starvation among a concurrent set of such queries. To maximize throughput, we propose a novel FullSharing scheme that allows all concurrent queries, when performing base-table I/O, to share the cache belonging to a given core. We then generalize this approach to a BatchSharing scheme that avoids thrashing on "agg-tables" ---hash tables that are used for aggregation processing---caused by execution of too many queries on a core. This scheme partitions queries into batches such that the working-set of agg-table entries for each batch can fit into a cache; an efficient sampling technique is used to estimate selectivities and working-set sizes for purposes of query partitioning. Finally, we use lottery-scheduling techniques to ensure fairness and impose a hard upper bound on staging time to avoid starvation. On our 8-core testbed, we were able to completely remove the memory I/O bottleneck, increasing throughput by a factor of 2 to 2.5, while also maintaining fairness and avoiding starvation.},
	number = {1},
	urldate = {2019-08-23},
	journal = {Proc. VLDB Endow.},
	author = {Qiao, Lin and Raman, Vijayshankar and Reiss, Frederick and Haas, Peter J. and Lohman, Guy M.},

	year = {2008},
	keywords = {cscheid-mqo, leibatt-mqo, leibatt-interaction-filter, leibatt-interaction-select, leibatt-data-parallel, cscheid-interaction-filter, cscheid-interaction-select, cscheid-data-parallel},
	pages = {610--621}
}

@article{giannikis_shared_2014,
	title = {Shared {Workload} {Optimization}},
	volume = {7},
	issn = {2150-8097},
	url = {http://dx.doi.org/10.14778/2732279.2732280},
	doi = {10.14778/2732279.2732280},
	abstract = {As a result of increases in both the query load and the data managed, as well as changes in hardware architecture (multicore), the last years have seen a shift from query-at-a-time approaches towards shared work (SW) systems where queries are executed in groups. Such groups share operators like scans and joins, leading to systems that process hundreds to thousands of queries in one go. SW systems range from storage engines that use in-memory co-operative scans to more complex query processing engines that share joins over analytical and star schema queries. In all cases, they rely on either single query optimizers, predicate sharing, or on manually generated plans. In this paper we explore the problem of shared workload optimization (SWO) for SW systems. The challenge in doing so is that the optimization has to be done for the entire workload and that results in a class of stochastic knapsack with uncertain weights optimization, which can only be addressed with heuristics to achieve a reasonable runtime. In this paper we focus on hash joins and shared scans and present a first algorithm capable of optimizing the execution of entire workloads by deriving a global executing plan for all the queries in the system. We evaluate the optimizer over the TPC-W and the TPC-H benchmarks. The results prove the feasibility of this approach and demonstrate the performance gains that can be obtained from SW systems.},
	number = {6},
	urldate = {2019-08-23},
	journal = {Proc. VLDB Endow.},
	author = {Giannikis, Georgios and Makreshanski, Darko and Alonso, Gustavo and Kossmann, Donald},

	year = {2014},
	keywords = {cscheid-mqo, leibatt-mqo, leibatt-interaction-filter, leibatt-interaction-select, cscheid-interaction-filter, cscheid-interaction-select},
	pages = {429--440}
}

@inproceedings{chan_hierarchical_1999,
	title = {Hierarchical cubes for range-sum queries},
	booktitle = {Proceedings of the 25th {VLDB} {Conference}},
	author = {Chan, Chee Yong and Ioannidis, Yannis E},
	year = {1999},
	keywords = {cscheid-materialized-views, cscheid-interaction-aggregate},
	pages = {675--686}
}

@inproceedings{chun_dynamic_2001,
	title = {Dynamic update cube for range-sum queries},
	booktitle = {{VLDB}},
	author = {Chun, Seok-Ju and Chung, Chin-Wan and Lee, Ju-Hong and Lee, Seok-Lyong},
	year = {2001},
	keywords = {cscheid-materialized-views, cscheid-materialized-views-aggregate, cscheid-interaction-aggregate},
	pages = {521--530}
}

@inproceedings{ho_range_1997,
	address = {New York, NY, USA},
	series = {{SIGMOD} '97},
	title = {Range {Queries} in {OLAP} {Data} {Cubes}},
	isbn = {0-89791-911-4},
	url = {http://doi.acm.org/10.1145/253260.253274},
	doi = {10.1145/253260.253274},
	booktitle = {Proceedings of the 1997 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Ho, Ching-Tien and Agrawal, Rakesh and Megiddo, Nimrod and Srikant, Ramakrishnan},
	year = {1997},
	note = {event-place: Tucson, Arizona, USA},
	keywords = {cscheid-materialized-views, cscheid-interaction-aggregate, cscheid-interaction-filter},
	pages = {73--88}
}

@article{procopio_selective_2019,
	title = {Selective {Wander} {Join}: {Fast} {Progressive} {Visualizations} for {Data} {Joins}},
	volume = {6},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	shorttitle = {Selective {Wander} {Join}},
	url = {https://www.mdpi.com/2227-9709/6/1/14},
	doi = {10.3390/informatics6010014},
	abstract = {Progressive visualization offers a great deal of promise for big data visualization; however, current progressive visualization systems do not allow for continuous interaction. What if users want to see more confident results on a subset of the visualization? This can happen when users are in exploratory analysis mode but want to ask some directed questions of the data as well. In a progressive visualization system, the online aggregation algorithm determines the database sampling rate and resulting convergence rate, not the user. In this paper, we extend a recent method in online aggregation, called Wander Join, that is optimized for queries that join tables, one of the most computationally expensive operations. This extension leverages importance sampling to enable user-driven sampling when data joins are in the query. We applied user interaction techniques that allow the user to view and adjust the convergence rate, providing more transparency and control over the online aggregation process. By leveraging importance sampling, our extension of Wander Join also allows for stratified sampling of groups when there is data distribution skew. We also improve the convergence rate of filtering queries, but with additional overhead costs not needed in the original Wander Join algorithm.},
	language = {en},
	number = {1},
	urldate = {2019-12-11},
	journal = {Informatics},
	author = {Procopio, Marianne and Scheidegger, Carlos and Wu, Eugene and Chang, Remco},

	year = {2019},
	keywords = {online aggregation, leibatt-progressive, cscheid-aqp, leibatt-aqp, cscheid-maybe, leibatt-aggregation, cscheid-progressive, leibatt-interaction-aggregate, leibatt-interaction-filter, leibatt-interaction-select, information visualization, interaction, progressive visualization, cscheid-interaction-aggregate, cscheid-interaction-filter, cscheid-interaction-select},
	pages = {14}
}

@inproceedings{chang_recipescape:_2018,
	address = {New York, NY, USA},
	series = {{CHI} '18},
	title = {{RecipeScape}: {An} {Interactive} {Tool} for {Analyzing} {Cooking} {Instructions} at {Scale}},
	isbn = {978-1-4503-5620-6},
	shorttitle = {{RecipeScape}},
	url = {http://doi.acm.org/10.1145/3173574.3174025},
	doi = {10.1145/3173574.3174025},
	abstract = {For cooking professionals and culinary students, understanding cooking instructions is an essential yet demanding task. Common tasks include categorizing different approaches to cooking a dish and identifying usage patterns of particular ingredients or cooking methods, all of which require extensive browsing and comparison of multiple recipes. However, no existing system provides support for such in-depth and at-scale analysis. We present RecipeScape, an interactive system for browsing and analyzing the hundreds of recipes of a single dish available online. We also introduce a computational pipeline that extracts cooking processes from recipe text and calculates a procedural similarity between them. To evaluate how RecipeScape supports culinary analysis at scale, we conducted a user study with cooking professionals and culinary students with 500 recipes for two different dishes. Results show that RecipeScape clusters recipes into distinct approaches, and captures notable usage patterns of ingredients and cooking actions.},
	urldate = {2019-12-17},
	booktitle = {Proceedings of the 2018 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Chang, Minsuk and Guillain, Leonore V. and Jung, Hyeungshik and Hare, Vivian M. and Kim, Juho and Agrawala, Maneesh},
	year = {2018},
	note = {event-place: Montreal QC, Canada},
	keywords = {cscheid-maybe, analysis at scale, cooking recipes, culinary analysis, interactive data mining, naturally crowdsourced data},
	pages = {451:1--451:12}
}

@inproceedings{xu_ecglens:_2018,
	address = {New York, NY, USA},
	series = {{CHI} '18},
	title = {{ECGLens}: {Interactive} {Visual} {Exploration} of {Large} {Scale} {ECG} {Data} for {Arrhythmia} {Detection}},
	isbn = {978-1-4503-5620-6},
	shorttitle = {{ECGLens}},
	url = {http://doi.acm.org/10.1145/3173574.3174237},
	doi = {10.1145/3173574.3174237},
	abstract = {The Electrocardiogram (ECG) is commonly used to detect arrhythmias. Traditionally, a single ECG observation is used for diagnosis, making it difficult to detect irregular arrhythmias. Recent technology developments, however, have made it cost-effective to collect large amounts of raw ECG data over time. This promises to improve diagnosis accuracy, but the large data volume presents new challenges for cardiologists. This paper introduces ECGLens, an interactive system for arrhythmia detection and analysis using large-scale ECG data. Our system integrates an automatic heartbeat classification algorithm based on convolutional neural network, an outlier detection algorithm, and a set of rich interaction techniques. We also introduce A-glyph, a novel glyph designed to improve the readability and comparison of ECG signals. We report results from a comprehensive user study showing that A-glyph improves the efficiency in arrhythmia detection, and demonstrate the effectiveness of ECGLens in arrhythmia detection through two expert interviews.},
	urldate = {2019-12-17},
	booktitle = {Proceedings of the 2018 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Xu, Ke and Guo, Shunan and Cao, Nan and Gotz, David and Xu, Aiwen and Qu, Huamin and Yao, Zhenjie and Chen, Yixin},
	year = {2018},
	note = {event-place: Montreal QC, Canada},
	keywords = {visualization, cscheid-maybe, artifact or system, health - clinical, interaction design, visual design},
	pages = {663:1--663:12}
}

@inproceedings{glassman_visualizing_2018,
	address = {New York, NY, USA},
	series = {{CHI} '18},
	title = {Visualizing {API} {Usage} {Examples} at {Scale}},
	isbn = {978-1-4503-5620-6},
	url = {http://doi.acm.org/10.1145/3173574.3174154},
	doi = {10.1145/3173574.3174154},
	abstract = {Using existing APIs properly is a key challenge in programming, given that libraries and APIs are increasing in number and complexity. Programmers often search for online code examples in Q\&A forums and read tutorials and blog posts to learn how to use a given API. However, there are often a massive number of related code examples and it is difficult for a user to understand the commonalities and variances among them, while being able to drill down to concrete details. We introduce an interactive visualization for exploring a large collection of code examples mined from open-source repositories at scale. This visualization summarizes hundreds of code examples in one synthetic code skeleton with statistical distributions for canonicalized statements and structures enclosing an API call. We implemented this interactive visualization for a set of Java APIs and found that, in a lab study, it helped users (1) answer significantly more API usage questions correctly and comprehensively and (2) explore how other programmers have used an unfamiliar API.},
	urldate = {2019-12-17},
	booktitle = {Proceedings of the 2018 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Glassman, Elena L. and Zhang, Tianyi and Hartmann, Björn and Kim, Miryung},
	year = {2018},
	note = {event-place: Montreal QC, Canada},
	keywords = {interactive visualization, cscheid-maybe, api, code examples, programming support},
	pages = {580:1--580:12}
}

@article{pezzotti_multiscale_2018,
	title = {Multiscale {Visualization} and {Exploration} of {Large} {Bipartite} {Graphs}},
	volume = {37},
	copyright = {© 2018 The Author(s) Computer Graphics Forum © 2018 The Eurographics Association and John Wiley \& Sons Ltd. Published by John Wiley \& Sons Ltd.},
	issn = {1467-8659},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.13441},
	doi = {10.1111/cgf.13441},
	abstract = {A bipartite graph is a powerful abstraction for modeling relationships between two collections. Visualizations of bipartite graphs allow users to understand the mutual relationships between the elements in the two collections, e.g., by identifying clusters of similarly connected elements. However, commonly-used visual representations do not scale for the analysis of large bipartite graphs containing tens of millions of vertices, often resorting to an a-priori clustering of the sets. To address this issue, we present the Who's-Active-On-What-Visualization (WAOW-Vis) that allows for multiscale exploration of a bipartite social-network without imposing an a-priori clustering. To this end, we propose to treat a bipartite graph as a high-dimensional space and we create the WAOW-Vis adapting the multiscale dimensionality-reduction technique HSNE. The application of HSNE for bipartite graph requires several modifications that form the contributions of this work. Given the nature of the problem, a set-based similarity is proposed. For efficient and scalable computations, we use compressed bitmaps to represent sets and we present a novel space partitioning tree to efficiently compute similarities; the Sets Intersection Tree. Finally, we validate WAOW-Vis on several datasets connecting Twitter-users and -streams in different domains: news, computer science and politics. We show how WAOW-Vis is particularly effective in identifying hierarchies of communities among social-media users.},
	language = {en},
	number = {3},
	urldate = {2019-12-17},
	journal = {Computer Graphics Forum},
	author = {Pezzotti, Nicola and Fekete, Jean-Daniel and Höllt, Thomas and Lelieveldt, Boudewijn P. F. and Eisemann, Elmar and Vilanova, Anna},
	year = {2018},
	keywords = {cscheid-maybe, leibatt-compression, leibatt-maybe, leibatt-interaction-filter, leibatt-interaction-navigate, Categories and Subject Descriptors (according to ACM CCS), I.3.3 Computer Graphics: Picture/Image Generation—Line and curve generation, cscheid-interaction-navigate, cscheid-interaction-filter},
	pages = {549--560}
}

@article{li_concavecubes:_2018,
	title = {{ConcaveCubes}: {Supporting} {Cluster}-based {Geographical} {Visualization} in {Large} {Data} {Scale}},
	volume = {37},
	copyright = {© 2018 The Author(s) Computer Graphics Forum © 2018 The Eurographics Association and John Wiley \& Sons Ltd. Published by John Wiley \& Sons Ltd.},
	issn = {1467-8659},
	shorttitle = {{ConcaveCubes}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.13414},
	doi = {10.1111/cgf.13414},
	abstract = {In this paper we study the problem of supporting effective and scalable visualization for the rapidly increasing volumes of urban data. From an extensive literature study, we find that the existing solutions suffer from at least one of the drawbacks below: (i) loss of interesting structures/outliers due to sampling; (ii) supporting heatmaps only, which provides limited information; and (iii) no notion of real-world geography semantics (e.g., country, state, city) is captured in the visualization result as well as the underlying index. Therefore, we propose ConcaveCubes, a cluster-based data cube to support interactive visualization of large-scale multidimensional urban data. Specifically, we devise an appropriate visualization abstraction and visualization design based on clusters. We propose a novel concave hull construction method to support boundary based cluster map visualization, where real-world geographical semantics are preserved without any information loss. Instead of calculating the clusters on demand, ConcaveCubes (re)utilizes existing calculation and visualization results to efficiently support different kinds of user interactions. We conduct extensive experiments using real-world datasets and show the efficiency and effectiveness of ConcaveCubes by comparing with the state-of-the-art cube-based solutions.},
	language = {en},
	number = {3},
	urldate = {2019-12-17},
	journal = {Computer Graphics Forum},
	author = {Li, Mingzhao and Choudhury, Farhana and Bao, Zhifeng and Samet, Hanan and Sellis, Timos},
	year = {2018},
	keywords = {cscheid-materialized-views, leibatt-materialized-views, leibatt-aggregation, leibatt-interaction-aggregate, leibatt-interaction-filter, leibatt-interaction-select, leibatt-interaction-navigate, cscheid-spatial-index, cscheid-reuse, cscheid-interaction-aggregate, cscheid-interaction-filter},
	pages = {217--228}
}

@article{chegini_interactive_2018,
	title = {Interactive {Visual} {Exploration} of {Local} {Patterns} in {Large} {Scatterplot} {Spaces}},
	volume = {37},
	copyright = {© 2018 The Author(s) Computer Graphics Forum © 2018 The Eurographics Association and John Wiley \& Sons Ltd. Published by John Wiley \& Sons Ltd.},
	issn = {1467-8659},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.13404},
	doi = {10.1111/cgf.13404},
	abstract = {Analysts often use visualisation techniques like a scatterplot matrix (SPLOM) to explore multivariate datasets. The scatterplots of a SPLOM can help to identify and compare two-dimensional global patterns. However, local patterns which might only exist within subsets of records are typically much harder to identify and may go unnoticed among larger sets of plots in a SPLOM. This paper explores the notion of local patterns and presents a novel approach to visually select, search for, and compare local patterns in a multivariate dataset. Model-based and shape-based pattern descriptors are used to automatically compare local regions in scatterplots to assist in the discovery of similar local patterns. Mechanisms are provided to assess the level of similarity between local patterns and to rank similar patterns effectively. Moreover, a relevance feedback module is used to suggest potentially relevant local patterns to the user. The approach has been implemented in an interactive tool and demonstrated with two real-world datasets and use cases. It supports the discovery of potentially useful information such as clusters, functional dependencies between variables, and statistical relationships in subsets of data records and dimensions.},
	language = {en},
	number = {3},
	urldate = {2019-12-17},
	journal = {Computer Graphics Forum},
	author = {Chegini, Mohammad and Shao, Lin and Gregor, Robert and Lehmann, Dirk J. and Andrews, Keith and Schreck, Tobias},
	year = {2018},
	keywords = {cscheid-materialized-views, Categories and Subject Descriptors (according to ACM CCS), Human-centered computing: Visualization—Visualization systems and tools, cscheid-interaction-filter, cscheid-interaction-select},
	pages = {99--109}
}

@article{fan_fast_2018,
	title = {Fast and {Accurate} {CNN}-based {Brushing} in {Scatterplots}},
	volume = {37},
	copyright = {© 2018 The Author(s) Computer Graphics Forum © 2018 The Eurographics Association and John Wiley \& Sons Ltd. Published by John Wiley \& Sons Ltd.},
	issn = {1467-8659},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.13405},
	doi = {10.1111/cgf.13405},
	abstract = {Brushing plays a central role in most modern visual analytics solutions and effective and efficient techniques for data selection are key to establishing a successful human-computer dialogue. With this paper, we address the need for brushing techniques that are both fast, enabling a fluid interaction in visual data exploration and analysis, and also accurate, i.e., enabling the user to effectively select specific data subsets, even when their geometric delimination is non-trivial. We present a new solution for a near-perfect sketch-based brushing technique, where we exploit a convolutional neural network (CNN) for estimating the intended data selection from a fast and simple click-and-drag interaction and from the data distribution in the visualization. Our key contributions include a drastically reduced error rate—now below 3\%, i.e., less than half of the so far best accuracy—and an extension to a larger variety of selected data subsets, going beyond previous limitations due to linear estimation models.},
	language = {en},
	number = {3},
	urldate = {2019-12-17},
	journal = {Computer Graphics Forum},
	author = {Fan, Chaoran and Hauser, Helwig},
	year = {2018},
	keywords = {cscheid-user-modeling, leibatt-user-modeling, leibatt-user-query-prediction, leibatt-interaction-filter, leibatt-interaction-select, cscheid-interaction-filter, cscheid-interaction-select},
	pages = {111--120}
}

@article{zhou_key_2018,
	title = {Key {Time} {Steps} {Selection} for {Large}-{Scale} {Time}-{Varying} {Volume} {Datasets} {Using} an {Information}-{Theoretic} {Storyboard}},
	volume = {37},
	copyright = {© 2018 The Author(s) Computer Graphics Forum © 2018 The Eurographics Association and John Wiley \& Sons Ltd. Published by John Wiley \& Sons Ltd.},
	issn = {1467-8659},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.13399},
	doi = {10.1111/cgf.13399},
	abstract = {Key time steps selection is essential for effective and efficient scientific visualization of large-scale time-varying datasets. We present a novel approach that can decide the number of most representative time steps while selecting them to minimize the difference in the amount of information from the original data. We use linear interpolation to reconstruct the data of intermediate time steps between selected time steps. We propose an evaluation of selected time steps by computing the difference in the amount of information (called information difference) using variation of information (VI) from information theory, which compares the interpolated time steps against the original data. In the one-time preprocessing phase, a dynamic programming is applied to extract the subset of time steps that minimize the information difference. In the run-time phase, a novel chart is used to present the dynamic programming results, which serves as a storyboard of the data to guide the user to select the best time steps very efficiently. We extend our preprocessing approach to a novel out-of-core approximate algorithm to achieve optimal I/O cost, which also greatly reduces the in-core computing time and exhibits a nice trade-off between computing speed and accuracy. As shown in the experiments, our approximate method outperforms the previous globally optimal DTW approach [TLS12] on out-of-core data by significantly improving the running time while keeping similar qualities, and is our major contribution.},
	language = {en},
	number = {3},
	urldate = {2019-12-17},
	journal = {Computer Graphics Forum},
	author = {Zhou, Bo and Chiang, Yi-Jen},
	year = {2018},
	keywords = {leibatt-aqp, cscheid-maybe, leibatt-interaction-filter, Information Theory, Key Time Steps Selection, Scalar Field Data, Time-Varying Volume Data, cscheid-interaction-navigate},
	pages = {37--49}
}

@article{miranda_time_2018,
	title = {Time {Lattice}: {A} {Data} {Structure} for the {Interactive} {Visual} {Analysis} of {Large} {Time} {Series}},
	volume = {37},
	copyright = {© 2018 The Author(s) Computer Graphics Forum © 2018 The Eurographics Association and John Wiley \& Sons Ltd. Published by John Wiley \& Sons Ltd.},
	issn = {1467-8659},
	shorttitle = {Time {Lattice}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.13398},
	doi = {10.1111/cgf.13398},
	abstract = {Advances in technology coupled with the availability of low-cost sensors have resulted in the continuous generation of large time series from several sources. In order to visually explore and compare these time series at different scales, analysts need to execute online analytical processing (OLAP) queries that include constraints and group-by's at multiple temporal hierarchies. Effective visual analysis requires these queries to be interactive. However, while existing OLAP cube-based structures can support interactive query rates, the exponential memory requirement to materialize the data cube is often unsuitable for large data sets. Moreover, none of the recent space-efficient cube data structures allow for updates. Thus, the cube must be re-computed whenever there is new data, making them impractical in a streaming scenario. We propose Time Lattice, a memory-efficient data structure that makes use of the implicit temporal hierarchy to enable interactive OLAP queries over large time series. Time Lattice is a subset of a fully materialized cube and is designed to handle fast updates and streaming data. We perform an experimental evaluation which shows that the space efficiency of the data structure does not hamper its performance when compared to the state of the art. In collaboration with signal processing and acoustics research scientists, we use the Time Lattice data structure to design the Noise Profiler, a web-based visualization framework that supports the analysis of noise from cities. We demonstrate the utility of Noise Profiler through a set of case studies.},
	language = {en},
	number = {3},
	urldate = {2019-12-17},
	journal = {Computer Graphics Forum},
	author = {Miranda, Fabio and Lage, Marcos and Doraiswamy, Harish and Mydlarz, Charlie and Salamon, Justin and Lockerman, Yitzchak and Freire, Juliana and Silva, Claudio T.},
	year = {2018},
	keywords = {cscheid-materialized-views, leibatt-materialized-views, leibatt-aggregation, leibatt-interaction-aggregate, leibatt-interaction-filter, leibatt-interaction-select, leibatt-interaction-navigate, cscheid-interaction-aggregate, cscheid-interaction-navigate, cscheid-interaction-filter},
	pages = {23--35}
}

@article{xie_semantic-based_2019,
	title = {A {Semantic}-{Based} {Method} for {Visualizing} {Large} {Image} {Collections}},
	volume = {25},
	issn = {2160-9306},
	doi = {10.1109/TVCG.2018.2835485},
	abstract = {Interactive visualization of large image collections is important and useful in many applications, such as personal album management and user profiling on images. However, most prior studies focus on using low-level visual features of images, such as texture and color histogram, to create visualizations without considering the more important semantic information embedded in images. This paper proposes a novel visual analytic system to analyze images in a semantic-aware manner. The system mainly comprises two components: a semantic information extractor and a visual layout generator. The semantic information extractor employs an image captioning technique based on convolutional neural network (CNN) to produce descriptive captions for images, which can be transformed into semantic keywords. The layout generator employs a novel co-embedding model to project images and the associated semantic keywords to the same 2D space. Inspired by the galaxy metaphor, we further turn the projected 2D space to a galaxy visualization of images, in which semantic keywords and images are visually encoded as stars and planets. Our system naturally supports multi-scale visualization and navigation, in which users can immediately see a semantic overview of an image collection and drill down for detailed inspection of a certain group of images. Users can iteratively refine the visual layout by integrating their domain knowledge into the co-embedding process. Two task-based evaluations are conducted to demonstrate the effectiveness of our system.},
	number = {7},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Xie, Xiao and Cai, Xiwen and Zhou, Junpei and Cao, Nan and Wu, Yingcai},

	year = {2019},
	keywords = {data analysis, data visualisation, Data visualization, interactive visualization, Visualization, Data mining, cscheid-maybe, CNN, co-embedding process, convolutional neural nets, convolutional neural network, feature extraction, galaxy visualization, Image analysis, image captioning, image captioning technique, image classification, image collection, image colour analysis, image representation, image retrieval, Image visualization, Layout, low-level visual features, personal album management, semantic information, semantic information extractor, semantic keywords, semantic layout, semantic networks, semantic overview, semantic-aware manner, semantic-based method, Semantics, Task analysis, user profiling, visual analytic system, visual layout generator},
	pages = {2362--2377}
}

@article{duran_visualization_2019,
	title = {Visualization of {Large} {Molecular} {Trajectories}},
	volume = {25},
	issn = {2160-9306},
	doi = {10.1109/TVCG.2018.2864851},
	abstract = {The analysis of protein-ligand interactions is a time-intensive task. Researchers have to analyze multiple physico-chemical properties of the protein at once and combine them to derive conclusions about the protein-ligand interplay. Typically, several charts are inspected, and 3D animations can be played side-by-side to obtain a deeper understanding of the data. With the advances in simulation techniques, larger and larger datasets are available, with up to hundreds of thousands of steps. Unfortunately, such large trajectories are very difficult to investigate with traditional approaches. Therefore, the need for special tools that facilitate inspection of these large trajectories becomes substantial. In this paper, we present a novel system for visual exploration of very large trajectories in an interactive and user-friendly way. Several visualization motifs are automatically derived from the data to give the user the information about interactions between protein and ligand. Our system offers specialized widgets to ease and accelerate data inspection and navigation to interesting parts of the simulation. The system is suitable also for simulations where multiple ligands are involved. We have tested the usefulness of our tool on a set of datasets obtained from protein engineers, and we describe the expert feedback.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Duran, David and Hermosilla, Pedro and Ropinski, Timo and Kozlíková, Barbora and Vinacua, Álvar and Vázquez, Pere-Pau},

	year = {2019},
	keywords = {Data models, visual exploration, data visualisation, Visualization, data mining, Three-dimensional displays, Computational modeling, cscheid-maybe, biology computing, computer animation, data inspection, environmental science computing, Inspection, interactive user-friendly way, learning (artificial intelligence), long trajectories, molecular biophysics, molecular trajectories, Molecular visualization, multiple physico-chemical properties, protein engineers, protein-ligand interactions, protein-ligand interplay, proteins, Proteins, simulation inspection, time-intensive task, Trajectory, visualization motifs},
	pages = {987--996}
}

@article{luciani_details-first_2019,
	title = {Details-{First}, {Show} {Context}, {Overview} {Last}: {Supporting} {Exploration} of {Viscous} {Fingers} in {Large}-{Scale} {Ensemble} {Simulations}},
	volume = {25},
	issn = {2160-9306},
	shorttitle = {Details-{First}, {Show} {Context}, {Overview} {Last}},
	doi = {10.1109/TVCG.2018.2864849},
	abstract = {Visualization research often seeks designs that first establish an overview of the data, in accordance to the information seeking mantra: “Overview first, zoom and filter, then details on demand”. However, in computational fluid dynamics (CFD), as well as in other domains, there are many situations where such a spatial overview is not relevant or practical for users, for example when the experts already have a good mental overview of the data, or when an analysis of a large overall structure may not be related to the specific, information-driven tasks of users. Using scientific workflow theory and, as a vehicle, the problem of viscous finger evolution, we advocate an alternative model that allows domain experts to explore features of interest first, then explore the context around those features, and finally move to a potentially unfamiliar summarization overview. In a model instantiation, we show how a computational back-end can identify and track over time low-level, small features, then be used to filter the context of those features while controlling the complexity of the visualization, and finally to summarize and compare simulations. We demonstrate the effectiveness of this approach with an online web-based exploration of a total volume of data approaching half a billion seven-dimensional data points, and report supportive feedback provided by domain experts with respect to both the instantiation and the theoretical model.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Luciani, Timothy and Burks, Andrew and Sugiyama, Cassiano and Komperda, Jonathan and Marai, G. Elisabeta},

	year = {2019},
	keywords = {data visualisation, Data visualization, Visualization, Internet, Computational modeling, Spatiotemporal phenomena, cscheid-maybe, CFD, computational back-end, computational fluid dynamics, Context modeling, details-first model, discourse paper, domain experts, Feature extraction, information seeking mantra, information-driven tasks, large-scale ensemble simulations, model instantiation, online Web-based exploration, scientific workflow theory, seven-dimensional data points, theoretical model, theory, Tracking, viscous finger evolution, visualization design, visualization research},
	pages = {1225--1235}
}

@article{hazarika_codda:_2019,
	title = {{CoDDA}: {A} {Flexible} {Copula}-based {Distribution} {Driven} {Analysis} {Framework} for {Large}-{Scale} {Multivariate} {Data}},
	volume = {25},
	issn = {2160-9306},
	shorttitle = {{CoDDA}},
	doi = {10.1109/TVCG.2018.2864801},
	abstract = {CoDDA (Copula-based Distribution Driven Analysis) is a flexible framework for large-scale multivariate datasets. A common strategy to deal with large-scale scientific simulation data is to partition the simulation domain and create statistical data summaries. Instead of storing the high-resolution raw data from the simulation, storing the compact statistical data summaries results in reduced storage overhead and alleviated I/O bottleneck. Such summaries, often represented in the form of statistical probability distributions, can serve various post-hoc analysis and visualization tasks. However, for multivariate simulation data using standard multivariate distributions for creating data summaries is not feasible. They are either storage inefficient or are computationally expensive to be estimated in simulation time (in situ) for large number of variables. In this work, using copula functions, we propose a flexible multivariate distribution-based data modeling and analysis framework that offers significant data reduction and can be used in an in situ environment. The framework also facilitates in storing the associated spatial information along with the multivariate distributions in an efficient representation. Using the proposed multivariate data summaries, we perform various multivariate post-hoc analyses like query-driven visualization and sampling-based visualization. We evaluate our proposed method on multiple real-world multivariate scientific datasets. To demonstrate the efficacy of our framework in an in situ environment, we apply it on a large-scale flow simulation.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Hazarika, Subhashis and Dutta, Soumya and Shen, Han-Wei and Chen, Jen-Ping},

	year = {2019},
	keywords = {data analysis, Data models, data visualisation, Data visualization, Histograms, Analytical models, Computational modeling, data reduction, leibatt-aqp, cscheid-maybe, leibatt-materialized-views, leibatt-maybe, Task analysis, CoDDA, compact statistical data summaries results, Copula, copula functions, Distribution-based, flexible copula-based distribution driven analysis framework, flexible framework, flexible multivariate distribution-based data modeling, flow simulation, high-resolution raw data, In situ processing, large-scale flow simulation, large-scale multivariate data, large-scale multivariate datasets, large-scale scientific simulation data, Multivariate, multivariate data summaries, multivariate post-hoc analyses, multivariate simulation data, probability, Probability distribution, Query-driven, query-driven visualization, real-world multivariate scientific datasets, reduced storage overhead, significant data reduction, simulation domain, standard multivariate distributions, statistical analysis, statistical distributions, statistical probability distributions},
	pages = {1214--1224}
}

@article{el-assady_visual_2019,
	title = {Visual {Analytics} for {Topic} {Model} {Optimization} based on {User}-{Steerable} {Speculative} {Execution}},
	volume = {25},
	issn = {2160-9306},
	doi = {10.1109/TVCG.2018.2864769},
	abstract = {To effectively assess the potential consequences of human interventions in model-driven analytics systems, we establish the concept of speculative execution as a visual analytics paradigm for creating user-steerable preview mechanisms. This paper presents an explainable, mixed-initiative topic modeling framework that integrates speculative execution into the algorithmic decision-making process. Our approach visualizes the model-space of our novel incremental hierarchical topic modeling algorithm, unveiling its inner-workings. We support the active incorporation of the user's domain knowledge in every step through explicit model manipulation interactions. In addition, users can initialize the model with expected topic seeds, the backbone priors. For a more targeted optimization, the modeling process automatically triggers a speculative execution of various optimization strategies, and requests feedback whenever the measured model quality deteriorates. Users compare the proposed optimizations to the current model state and preview their effect on the next model iterations, before applying one of them. This supervised human-in-the-Ioop process targets maximum improvement for minimum feedback and has proven to be effective in three independent studies that confirm topic model quality improvements.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {El-Assady, Mennatallah and Sperrle, Fabian and Deussen, Oliver and Keim, Daniel and Collins, Christopher},

	year = {2019},
	keywords = {data analysis, data visualisation, Optimization, Clustering algorithms, Analytical models, Computational modeling, Visual analytics, cscheid-maybe, Task analysis, algorithmic decision-making process, current model state, decision making, expected topic seeds, Explainable Machine Learning, explicit model manipulation interactions, human interventions, human-in-the-Ioop process, incremental hierarchical topic modeling algorithm, minimum feedback, mixed-initiative topic modeling framework, Mixed-Initiative Visual Analytics, model iterations, model-driven analytics systems, model-space, optimisation, optimization strategies, potential consequences, requests feedback, Speculative Execution, topic model optimization, topic model quality improvements, user-steerable preview mechanisms, user-steerable speculative execution, User-Steerable Topic Modeling, users domain knowledge, visual analytics paradigm},
	pages = {374--384}
}

@article{chan_vibr:_2019,
	title = {{ViBr}: {Visualizing} {Bipartite} {Relations} at {Scale} with the {Minimum} {Description} {Length} {Principle}},
	volume = {25},
	issn = {2160-9306},
	shorttitle = {{ViBr}},
	doi = {10.1109/TVCG.2018.2864826},
	abstract = {Bipartite graphs model the key relations in many large scale real-world data: customers purchasing items, legislators voting for bills, people's affiliation with different social groups, faults occurring in vehicles, etc. However, it is challenging to visualize large scale bipartite graphs with tens of thousands or even more nodes or edges. In this paper, we propose a novel visual summarization technique for bipartite graphs based on the minimum description length (MDL) principle. The method simultaneously groups the two different set of nodes and constructs aggregated bipartite relations with balanced granularity and precision. It addresses the key trade-off that often occurs for visualizing large scale and noisy data: acquiring a clear and uncluttered overview while maximizing the information content in it. We formulate the visual summarization task as a co-clustering problem and propose an efficient algorithm based on locality sensitive hashing (LSH) that can easily scale to large graphs under reasonable interactive time constraints that previous related methods cannot satisfy. The method leads to the opportunity of introducing a visual analytics framework with multiple levels-of-detail to facilitate interactive data exploration. In the framework, we also introduce a compact visual design inspired by adjacency list representation of graphs as the building block for a small multiples display to compare the bipartite relations for different subsets of data. We showcase the applicability and effectiveness of our approach by applying it on synthetic data with ground truth and performing case studies on real-world datasets from two application domains including roll-call vote record analysis and vehicle fault pattern analysis. Interviews with experts in the political science community and the automotive industry further highlight the benefits of our approach.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Chan, Gromit Yeuk-Yin and Xu, Panpan and Dai, Zeng and Ren, Liu},

	year = {2019},
	keywords = {data analysis, Data models, data visualisation, Data visualization, Visualization, Clustering algorithms, pattern clustering, cscheid-maybe, visual design, Information Theory, automotive industry, balanced granularity, Bipartite graph, Bipartite Graph, bipartite graphs model, bipartite relation visualization, co-clustering problem, Complexity theory, customer purchasing items, graph theory, interactive data exploration, interactive time constraints, legislators voting, locality sensitive hashing, LSH, MDL, Minimum Description Length, minimum description length principle, Noise measurement, people affiliation, political science community, roll-call vote record analysis, social groups, synthetic data, vehicle fault pattern analysis, ViBr, visual analytics framework, Visual Summarization, visual summarization technique},
	pages = {321--330}
}

@article{liu_tpflow:_2019,
	title = {{TPFlow}: {Progressive} {Partition} and {Multidimensional} {Pattern} {Extraction} for {Large}-{Scale} {Spatio}-{Temporal} {Data} {Analysis}},
	volume = {25},
	issn = {2160-9306},
	shorttitle = {{TPFlow}},
	doi = {10.1109/TVCG.2018.2865018},
	abstract = {Consider a multi-dimensional spatio-temporal (ST) dataset where each entry is a numerical measure defined by the corresponding temporal, spatial and other domain-specific dimensions. A typical approach to explore such data utilizes interactive visualizations with multiple coordinated views. Each view displays the aggregated measures along one or two dimensions. By brushing on the views, analysts can obtain detailed information. However, this approach often cannot provide sufficient guidance for analysts to identify patterns hidden within subsets of data. Without a priori hypotheses, analysts need to manually select and iterate through different slices to search for patterns, which can be a tedious and lengthy process. In this work, we model multidimensional ST data as tensors and propose a novel piecewise rank-one tensor decomposition algorithm which supports automatically slicing the data into homogeneous partitions and extracting the latent patterns in each partition for comparison and visual summarization. The algorithm optimizes a quantitative measure about how faithfully the extracted patterns visually represent the original data. Based on the algorithm we further propose a visual analytics framework that supports a top-down, progressive partitioning workflow for level-of-detail multidimensional ST data exploration. We demonstrate the general applicability and effectiveness of our technique on three datasets from different application domains: regional sales trend analysis, customer traffic analysis in department stores, and taxi trip analysis with origin-destination (OD) data. We further interview domain experts to verify the usability of the prototype.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Liu, Dongyu and Xu, Panpan and Ren, Liu},

	year = {2019},
	keywords = {data analysis, Data models, Data analysis, data visualisation, Data visualization, Visualization, Partitioning algorithms, data mining, Data mining, interactive visualizations, interactive exploration, cscheid-aqp, cscheid-maybe, leibatt-materialized-views, leibatt-aggregation, leibatt-interaction-select, leibatt-interaction-navigate, visual analytics framework, automatic pattern discoveries, customer traffic analysis, domain-specific dimensions, extracted patterns, homogeneous partitions, interview domain experts, large-scale spatio-temporal data analysis, latent patterns, level-of-detail multidimensional ST data exploration, model multidimensional ST data, multidimensional pattern extraction, multidimensional spatio-temporal, multiple coordinated views, numerical measure, origin-destination data, progressive partition, progressive partitioning workflow, quantitative measure, regional sales trend analysis, Spatio-temporal data, taxi trip analysis, Tensile stress, tensor decomposition, tensors, visual summarization, cscheid-interaction-aggregate},
	pages = {1--11}
}

@article{chen_recursive_2020,
	title = {A {Recursive} {Subdivision} {Technique} for {Sampling} {Multi}-class {Scatterplots}},
	volume = {26},
	issn = {2160-9306},
	doi = {10.1109/TVCG.2019.2934541},
	abstract = {We present a non-uniform recursive sampling technique for multi-class scatterplots, with the specific goal of faithfully presenting relative data and class densities, while preserving major outliers in the plots. Our technique is based on a customized binary kd-tree, in which leaf nodes are created by recursively subdividing the underlying multi-class density map. By backtracking, we merge leaf nodes until they encompass points of all classes for our subsequently applied outlier-aware multi-class sampling strategy. A quantitative evaluation shows that our approach can better preserve outliers and at the same time relative densities in multi-class scatterplots compared to the previous approaches, several case studies demonstrate the effectiveness of our approach in exploring complex and real world data.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Chen, Xin and Ge, Tong and Zhang, Jian and Chen, Baoquan and Fu, Chi-Wing and Deussen, Oliver and Wang, Yunhai},

	year = {2020},
	keywords = {Data visualization, Visualization, Measurement, cscheid-materialized-views, Estimation, cscheid-aqp, leibatt-aqp, leibatt-materialized-views, leibatt-spatial-index, leibatt-interaction-aggregate, leibatt-interaction-select, Clutter, Image color analysis, kd-tree, multi-class sampling, outlier, relative density, Sampling methods, Scatterplot, cscheid-spatial-index, cscheid-interaction-aggregate, cscheid-interaction-select, cscheid-index},
	pages = {729--738}
}

@article{rapp_void-and-cluster_2020,
	title = {Void-and-{Cluster} {Sampling} of {Large} {Scattered} {Data} and {Trajectories}},
	volume = {26},
	issn = {2160-9306},
	doi = {10.1109/TVCG.2019.2934335},
	abstract = {We propose a data reduction technique for scattered data based on statistical sampling. Our void-and-cluster sampling technique finds a representative subset that is optimally distributed in the spatial domain with respect to the blue noise property. In addition, it can adapt to a given density function, which we use to sample regions of high complexity in the multivariate value domain more densely. Moreover, our sampling technique implicitly defines an ordering on the samples that enables progressive data loading and a continuous level-of-detail representation. We extend our technique to sample time-dependent trajectories, for example pathlines in a time interval, using an efficient and iterative approach. Furthermore, we introduce a local and continuous error measure to quantify how well a set of samples represents the original dataset. We apply this error measure during sampling to guide the number of samples that are taken. Finally, we use this error measure and other quantities to evaluate the quality, performance, and scalability of our algorithm.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Rapp, Tobias and Peters, Christoph and Dachsbacher, Carsten},

	year = {2020},
	keywords = {Data models, Data visualization, sampling, Computational modeling, cscheid-aqp, leibatt-aqp, cscheid-encode, Trajectory, blue noise, Data reduction, Entropy, entropy-based sampling, Loading, Measurement uncertainty, pathlines, scattered data},
	pages = {780--789}
}

@article{liu_smartcube:_2020,
	title = {{SmartCube}: {An} {Adaptive} {Data} {Management} {Architecture} for the {Real}-{Time} {Visualization} of {Spatiotemporal} {Datasets}},
	volume = {26},
	issn = {2160-9306},
	shorttitle = {{SmartCube}},
	doi = {10.1109/TVCG.2019.2934434},
	abstract = {Interactive visualization and exploration of large spatiotemporal data sets is difficult without carefully-designed data pre-processing and management tools. We propose a novel architecture for spatiotemporal data management. The architecture can dynamically update itself based on user queries. Datasets is stored in a tree-like structure to support memory sharing among cuboids in a logical structure of data cubes. An update mechanism is designed to create or remove cuboids on it, according to the analysis of the user queries, with the consideration of memory size limitation. Data structure is dynamically optimized according to different user queries. During a query process, user queries are recorded to predict the performance increment of the new cuboid. The creation or deletion of a cuboid is determined by performance increment. Experiment results show that our prototype system deliveries good performance towards user queries on different spatiotemporal datasets, which costing small memory size with comparable performance compared with other state-of-the-art algorithms.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Liu, Can and Wu, Cong and Shao, Hanning and Yuan, Xiaoru},

	year = {2020},
	keywords = {Data visualization, Data structures, Real-time systems, Memory management, Spatiotemporal phenomena, cscheid-materialized-views, cscheid-user-modeling, leibatt-materialized-views, leibatt-aggregation, leibatt-user-modeling, leibatt-interaction-aggregate, leibatt-interaction-filter, leibatt-interaction-select, leibatt-interaction-navigate, Task analysis, Data aggregation, data management, spatial-temporal data, cscheid-interaction-aggregate, cscheid-interaction-navigate, cscheid-interaction-filter, cscheid-interaction-select},
	pages = {790--799}
}

@article{raji_scientific_2018,
	title = {Scientific {Visualization} as a {Microservice}},
	issn = {2160-9306},
	doi = {10.1109/TVCG.2018.2879672},
	abstract = {In this paper, we propose using a decoupled architecture to create a microservice that can deliver scientific visualization remotely with efficiency, scalability, and superior availability, affordability and accessibility. Through our effort, we have created an open source platform, Tapestry, which can be deployed on Amazon AWS as a production use microservice. The applications we use to demonstrate the efficacy of the Tapestry microservice in this work are: (1) embedding interactive visualizations into lightweight web pages, (2) creating scientific visualization movies that are fully controllable by the viewers, (3) serving as a rendering engine for high-end displays such as power-walls, and (4) embedding data-intensive visualizations into augmented reality devices efficiently. In addition, we show results of an extensive performance study, and suggest how applications can make optimal use of microservices such as Tapestry.},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Raji, Mohammad and Hota, Alok and Hobson, Tanner and Huang, Jian},
	year = {2018},
	keywords = {Data visualization, Visualization, cscheid-maybe, leibatt-maybe, and Distributed Visualization, Browsers, Cloud Computing, Computer architecture, Rendering (computer graphics), Scientific Visualization, Servers, Visualization Systems, Web Applications, Web pages},
	pages = {1--1}
}

@article{mei_rsatree:_2020,
	title = {{RSATree}: {Distribution}-{Aware} {Data} {Representation} of {Large}-{Scale} {Tabular} {Datasets} for {Flexible} {Visual} {Query}},
	volume = {26},
	issn = {2160-9306},
	shorttitle = {{RSATree}},
	doi = {10.1109/TVCG.2019.2934800},
	abstract = {Analysts commonly investigate the data distributions derived from statistical aggregations of data that are represented by charts, such as histograms and binned scatterplots, to visualize and analyze a large-scale dataset. Aggregate queries are implicitly executed through such a process. Datasets are constantly extremely large; thus, the response time should be accelerated by calculating predefined data cubes. However, the queries are limited to the predefined binning schema of preprocessed data cubes. Such limitation hinders analysts' flexible adjustment of visual specifications to investigate the implicit patterns in the data effectively. Particularly, RSATree enables arbitrary queries and flexible binning strategies by leveraging three schemes, namely, an R-tree-based space partitioning scheme to catch the data distribution, a locality-sensitive hashing technique to achieve locality-preserving random access to data items, and a summed area table scheme to support interactive query of aggregated values with a linear computational complexity. This study presents and implements a web-based visual query system that supports visual specification, query, and exploration of large-scale tabular data with user-adjustable granularities. We demonstrate the efficiency and utility of our approach by performing various experiments on real-world datasets and analyzing time and space complexity.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Mei, Honghui and Chen, Wei and Wei, Yating and Hu, Yuanzhe and Zhou, Shuyue and Lin, Bingru and Zhao, Ying and Xia, Jiazhi},

	year = {2020},
	keywords = {Data visualization, Visual databases, Visualization, Histograms, Time factors, Aggregates, cscheid-materialized-views, leibatt-materialized-views, leibatt-aggregation, leibatt-spatial-index, leibatt-interaction-aggregate, leibatt-interaction-filter, leibatt-interaction-select, leibatt-interaction-navigate, Aggregate query, hashing, large-scale data visualization, R-tree, Social networking (online), summed area table, visual query, cscheid-spatial-index, cscheid-interaction-aggregate, cscheid-interaction-filter, cscheid-interaction-select, cscheid-index},
	pages = {1161--1171}
}

@article{de_lara_pahins_real-time_2019,
	title = {Real-{Time} {Exploration} of {Large} {Spatiotemporal} {Datasets} based on {Order} {Statistics}},
	issn = {2160-9306},
	url = {https://ieeexplore.ieee.org/document/8706590},
	doi = {10.1109/TVCG.2019.2914446},
	abstract = {In recent years sophisticated data structures based on datacubes have been proposed to perform interactive visual exploration of large datasets. While powerful, these approaches overlook the important fact that aggregations used to produce datacubes do not represent the actual distribution of the data being analyzed. As a result, these methods might produce biased results as well as hide important features in the data. In this paper, we introduce the Quantile Datacube Structure (QDS) that bridges this gap by supporting interactive visual exploration based on order statistics. To achieve this, QDS makes use of an efficient non-parametric distribution approximation scheme called p-digest and employs a novel datacube indexing scheme that reduces the memory usage of previous datacube methods. This enables interactive slicing and dicing while accurately approximating the distribution of quantitative variables of interest. We present two case studies that illustrate the ability of QDS to not only build order statistics based visualizations interactively but also to perform event detection on very large datasets. Finally, we present extensive experimental results that validate the effectiveness of QDS regarding memory usage and accuracy in the approximation of order statistics for real-world datasets.},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {de Lara Pahins, Cicero Augusto and Ferreira, Nivan and Comba, Joao},
	year = {2019},
	keywords = {Data visualization, Visualization, Data structures, Real-time systems, Spatiotemporal phenomena, cscheid-materialized-views, cscheid-aqp, leibatt-aqp, leibatt-materialized-views, leibatt-aggregation, cscheid-navigate, leibatt-interaction-aggregate, leibatt-interaction-filter, leibatt-interaction-arrange, leibatt-interaction-navigate, Airports, Data structures for visualization, Delays, event detection, order statistics, quantile sketch, visual analytics, cscheid-interaction-aggregate, cscheid-interaction-filter, cscheid-interaction-encode},
	pages = {1--1}
}

@article{li_p5:_2020,
	title = {P5: {Portable} {Progressive} {Parallel} {Processing} {Pipelines} for {Interactive} {Data} {Analysis} and {Visualization}},
	volume = {26},
	issn = {2160-9306},
	shorttitle = {P5},
	doi = {10.1109/TVCG.2019.2934537},
	abstract = {We present P5, a web-based visualization toolkit that combines declarative visualization grammar and GPU computing for progressive data analysis and visualization. To interactively analyze and explore big data, progressive analytics and visualization methods have recently emerged. Progressive visualizations of incrementally refining results have the advantages of allowing users to steer the analysis process and make early decisions. P5 leverages declarative grammar for specifying visualization designs and exploits GPU computing to accelerate progressive data processing and rendering. The declarative specifications can be modified during progressive processing to create different visualizations for analyzing the intermediate results. To enable user interactions for progressive data analysis, P5 utilizes the GPU to automatically aggregate and index data based on declarative interaction specifications to facilitate effective interactive visualization. We demonstrate the effectiveness and usefulness of P5 through a variety of example applications and several performance benchmark tests.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Li, Jianping Kelvin and Ma, Kwan-Liu},

	year = {2020},
	keywords = {data exploration, Data analysis, Data visualization, Big Data, cscheid-aqp, leibatt-aqp, leibatt-aggregation, leibatt-interaction-aggregate, leibatt-interaction-filter, leibatt-interaction-select, leibatt-interaction-derive, GPU computing, Grammar, Graphics processing units, Information visualization, Libraries, Parallel processing, progressive analytics, visualization software, leibatt-data-parallel, cscheid-interaction-aggregate, cscheid-interaction-filter, cscheid-interaction-select, cscheid-data-parallel, cscheid-interaction-derive},
	pages = {1151--1160}
}

@article{bock_openspace:_2020,
	title = {{OpenSpace}: {A} {System} for {Astrographics}},
	volume = {26},
	issn = {2160-9306},
	shorttitle = {{OpenSpace}},
	doi = {10.1109/TVCG.2019.2934259},
	abstract = {Human knowledge about the cosmos is rapidly increasing as instruments and simulations are generating new data supporting the formation of theory and understanding of the vastness and complexity of the universe. OpenSpace is a software system that takes on the mission of providing an integrated view of all these sources of data and supports interactive exploration of the known universe from the millimeter scale showing instruments on spacecrafts to billions of light years when visualizing the early universe. The ambition is to support research in astronomy and space exploration, science communication at museums and in planetariums as well as bringing exploratory astrographics to the class room. There is a multitude of challenges that need to be met in reaching this goal such as the data variety, multiple spatio-temporal scales, collaboration capabilities, etc. Furthermore, the system has to be flexible and modular to enable rapid prototyping and inclusion of new research results or space mission data and thereby shorten the time from discovery to dissemination. To support the different use cases the system has to be hardware agnostic and support a range of platforms and interaction paradigms. In this paper we describe how OpenSpace meets these challenges in an open source effort that is paving the path for the next generation of interactive astrographics.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Bock, Alexander and Axelsson, Emil and Costa, Jonathas and Payne, Gene and Acinapura, Micah and Trakinski, Vivian and Emmart, Carter and Silva, Cláudio and Hansen, Charles and Ynnerman, Anders},

	year = {2020},
	keywords = {Data models, Data visualization, cscheid-maybe, leibatt-maybe, Tools, Rendering (computer graphics), Astrographics, astronomy, Astronomy, astrophysics, Space missions, Space vehicles, system},
	pages = {633--642}
}

@article{hu_data_2020,
	title = {Data {Sampling} in {Multi}-view and {Multi}-class {Scatterplots} via {Set} {Cover} {Optimization}},
	volume = {26},
	issn = {2160-9306},
	doi = {10.1109/TVCG.2019.2934799},
	abstract = {We present a method for data sampling in scatterplots by jointly optimizing point selection for different views or classes. Our method uses space-filling curves (Z-order curves) that partition a point set into subsets that, when covered each by one sample, provide a sampling or coreset with good approximation guarantees in relation to the original point set. For scatterplot matrices with multiple views, different views provide different space-filling curves, leading to different partitions of the given point set. For multi-class scatterplots, the focus on either per-class distribution or global distribution provides two different partitions of the given point set that need to be considered in the selection of the coreset. For both cases, we convert the coreset selection problem into an Exact Cover Problem (ECP), and demonstrate with quantitative and qualitative evaluations that an approximate solution that solves the ECP efficiently is able to provide high-quality samplings.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Hu, Ruizhen and Sha, Tingkai and Van Kaick, Oliver and Deussen, Oliver and Huang, Hui},

	year = {2020},
	keywords = {Visualization, Optimization, Measurement, Kernel, cscheid-aqp, leibatt-aqp, cscheid-aggregate, cscheid-filter, cscheid-precomputed-samples, cscheid-select, Image color analysis, Sampling methods, Scatterplot, Exact Cover Problem, Sampling, SPLOM, Two dimensional displays},
	pages = {739--748}
}

@article{krueger_birds-eye_2019,
	title = {Bird's-{Eye} - {Large}-{Scale} {Visual} {Analytics} of {City} {Dynamics} using {Social} {Location} {Data}},
	volume = {38},
	copyright = {© 2019 The Author(s) Computer Graphics Forum © 2019 The Eurographics Association and John Wiley \& Sons Ltd. Published by John Wiley \& Sons Ltd.},
	issn = {1467-8659},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.13713},
	doi = {10.1111/cgf.13713},
	abstract = {The analysis of behavioral city dynamics, such as temporal patterns of visited places and citizens' mobility routines, is an essential task for urban and transportation planning. Social media applications such as Foursquare and Twitter provide access to large-scale and up-to-date dynamic movement data that not only help to understand the social life and pulse of a city but also to maintain and improve urban infrastructure. However, the fast growth rate of this data poses challenges for conventional methods to provide up-to-date, flexible analysis. Therefore, planning authorities barely consider it. We present a system and design study to leverage social media data that assist urban and transportation planners to achieve better monitoring and analysis of city dynamics such as visited places and mobility patterns in large metropolitan areas. We conducted a goal-and-task analysis with urban planning experts. To address these goals, we designed a system with a scalable data monitoring back-end and an interactive visual analytics interface. The monitoring component uses intelligent pre-aggregation to allow dynamic queries in near real-time. The visual analytics interface leverages unsupervised learning to reveal clusters, routines, and unusual behavior in massive data, allowing to understand patterns in time and space. We evaluated our approach based on a qualitative user study with urban planning experts which demonstrates that intuitive integration of advanced analytical tools with visual interfaces is pivotal in making behavioral city dynamics accessible to practitioners. Our interviews also revealed areas for future research.},
	language = {en},
	number = {3},
	urldate = {2019-12-19},
	journal = {Computer Graphics Forum},
	author = {Krueger, Robert and Han, Qi and Ivanov, Nikolay and Mahtal, Sanae and Thom, Dennis and Pfister, Hanspeter and Ertl, Thomas},
	year = {2019},
	keywords = {cscheid-materialized-views, leibatt-materialized-views, leibatt-aggregation, leibatt-interaction-aggregate, leibatt-interaction-filter, leibatt-interaction-select, leibatt-interaction-navigate, • Human-centered computing → Geographic visualization, • Information Search and Retrieval → Information Filtering, CCS Concepts, cscheid-db-application, cscheid-interaction-aggregate, cscheid-interaction-navigate, cscheid-interaction-filter, cscheid-interaction-select},
	pages = {595--607}
}

@article{dextrasromagnino_segmentifier:_2019,
	title = {Segmentifier: {Interactive} {Refinement} of {Clickstream} {Data}},
	volume = {38},
	copyright = {© 2019 The Author(s) Computer Graphics Forum © 2019 The Eurographics Association and John Wiley \& Sons Ltd. Published by John Wiley \& Sons Ltd.},
	issn = {1467-8659},
	shorttitle = {Segmentifier},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.13715},
	doi = {10.1111/cgf.13715},
	abstract = {Clickstream data has the potential to provide insights into e-commerce consumer behavior, but previous techniques fall short of handling the scale and complexity of real-world datasets because they require relatively clean and small input. We present Segmentifier, a novel visual analytics interface that supports an iterative process of refining collections of action sequences into meaningful segments. We present task and data abstractions for clickstream data analysis, leading to a high-level model built around an iterative view-refine-record loop with outcomes of conclude with an answer, export segment for further analysis in downstream tools, or abandon the question for a more fruitful analysis path. Segmentifier supports fast and fluid refinement of segments through tightly coupled visual encoding and interaction with a rich set of views that show evocative derived attributes for segments, sequences, and actions in addition to underlying raw sequences. These views support fast and fluid refinement of segments through filtering and partitioning attribute ranges. Interactive visual queries on custom action sequences are aggregated according to a three-level hierarchy. Segmentifier features a detailed glyph-based visual history of the automatically recorded analysis process showing the provenance of each segment as an analysis path of attribute constraints. We demonstrate the effectiveness of our approach through a usage scenario with real-world data and a case study documenting the insights gained by a corporate e-commerce analyst.},
	language = {en},
	number = {3},
	urldate = {2019-12-19},
	journal = {Computer Graphics Forum},
	author = {Dextras‐Romagnino, K. and Munzner, T.},
	year = {2019},
	keywords = {cscheid-maybe, cscheid-encode},
	pages = {623--634}
}

@article{besancon_hybrid_2019,
	title = {Hybrid {Touch}/{Tangible} {Spatial} {3D} {Data} {Selection}},
	volume = {38},
	url = {https://hal.inria.fr/hal-02079308},
	doi = {10.1111/cgf.13710},
	abstract = {We discuss spatial selection techniques for three-dimensional datasets. Such 3D spatial selection is fundamental to exploratory data analysis. While 2D selection is efficient for datasets with explicit shapes and structures, it is less efficient for data without such properties. We first propose a new taxonomy of 3D selection techniques, focusing on the amount of control the user has to define the selection volume. We then describe the 3D spatial selection technique Tangible Brush, which gives manual control over the final selection volume. It combines 2D {\textbackslash}new\{touch\} with 6-DOF 3D tangible input to allow users to perform 3D selections in volumetric data. We use {\textbackslash}new\{touch\} input to draw a 2D lasso, extruding it to a 3D selection volume based on the motion of a tangible, spatially-aware tablet. We describe our approach and present its quantitative and qualitative comparison to state-of-the-art structure-dependent selection. Our results show that, in addition to being dataset-independent, Tangible Brush is more accurate than existing dataset-dependent techniques, thus providing a trade-off between precision and effort.},
	language = {en},
	number = {3},
	urldate = {2019-12-19},
	journal = {Computer Graphics Forum},
	author = {Besançon, Lonni and Sereno, Mickael and Yu, Lingyun and Ammi, Mehdi and Isenberg, Tobias},

	year = {2019},
	keywords = {cscheid-maybe, cscheid-interaction-filter, cscheid-interaction-select},
	pages = {553--567}
}

@misc{noauthor_designing_nodate,
	title = {Designing {Animated} {Transitions} to {Convey} {Aggregate} {Operations} - {Kim} - 2019 - {Computer} {Graphics} {Forum} - {Wiley} {Online} {Library}},
	url = {https://onlinelibrary.wiley.com/doi/full/10.1111/cgf.13709},
	urldate = {2019-12-19},
	keywords = {cscheid-maybe, cscheid-aggregate, cscheid-encode}
}

@article{pezzotti_gpgpu_2020,
	title = {{GPGPU} {Linear} {Complexity} t-{SNE} {Optimization}},
	volume = {26},
	issn = {2160-9306},
	doi = {10.1109/TVCG.2019.2934307},
	abstract = {In recent years the t-distributed Stochastic Neighbor Embedding (t-SNE) algorithm has become one of the most used and insightful techniques for exploratory data analysis of high-dimensional data. It reveals clusters of high-dimensional data points at different scales while only requiring minimal tuning of its parameters. However, the computational complexity of the algorithm limits its application to relatively small datasets. To address this problem, several evolutions of t-SNE have been developed in recent years, mainly focusing on the scalability of the similarity computations between data points. However, these contributions are insufficient to achieve interactive rates when visualizing the evolution of the t-SNE embedding for large datasets. In this work, we present a novel approach to the minimization of the t-SNE objective function that heavily relies on graphics hardware and has linear computational complexity. Our technique decreases the computational cost of running t-SNE on datasets by orders of magnitude and retains or improves on the accuracy of past approximated techniques. We propose to approximate the repulsive forces between data points by splatting kernel textures for each data point. This approximation allows us to reformulate the t-SNE minimization problem as a series of tensor operations that can be efficiently executed on the graphics card. An efficient implementation of our technique is integrated and available for use in the widely used Google TensorFlow.js, and an open-source C++ library.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Pezzotti, Nicola and Thijssen, Julian and Mordvintsev, Alexander and Höllt, Thomas and Van Lew, Baldur and Lelieveldt, Boudewijn P.F. and Eisemann, Elmar and Vilanova, Anna},

	year = {2020},
	keywords = {Data visualization, Optimization, Computational modeling, cscheid-maybe, Complexity theory, Approximate Computation, Approximation algorithms, Dimensionality Reduction, GPGPU, High Dimensional Data, Linear programming, Minimization, Progressive Visual Analytics},
	pages = {1172--1181}
}

@inproceedings{zhang_topogroups:_2017,
	title = {{TopoGroups}: {Context}-{Preserving} {Visual} {Illustration} of {Multi}-{Scale} {Spatial} {Aggregates}},
	isbn = {978-1-4503-4655-9},
	shorttitle = {{TopoGroups}},
	url = {http://dl.acm.org/citation.cfm?id=3025453.3025801},
	doi = {10.1145/3025453.3025801},
	urldate = {2019-12-20},
	publisher = {ACM},
	author = {Zhang, Jiawei and Malik, Abish and Ahlbrand, Benjamin and Elmqvist, Niklas and Maciejewski, Ross and Ebert, David S.},

	year = {2017},
	keywords = {cscheid-maybe, leibatt-aggregation, leibatt-maybe, leibatt-interaction-aggregate, leibatt-interaction-filter, leibatt-interaction-select, leibatt-interaction-navigate},
	pages = {2940--2951}
}

@misc{noauthor_topogroups_nodate,
	title = {{TopoGroups}},
	url = {https://dl.acm.org/citation.cfm?id=3025801},
	urldate = {2019-12-20}
}

@article{wilkinson_visualizing_2018,
	title = {Visualizing {Big} {Data} {Outliers} {Through} {Distributed} {Aggregation}},
	volume = {24},
	issn = {2160-9306},
	doi = {10.1109/TVCG.2017.2744685},
	abstract = {Visualizing outliers in massive datasets requires statistical pre-processing in order to reduce the scale of the problem to a size amenable to rendering systems like D3, Plotly or analytic systems like R or SAS. This paper presents a new algorithm, called hdoutliers, for detecting multidimensional outliers. It is unique for a) dealing with a mixture of categorical and continuous variables, b) dealing with big-p (many columns of data), c) dealing with big-\$n\$ (many rows of data), d) dealing with outliers that mask other outliers, and e) dealing consistently with unidimensional and multidimensional datasets. Unlike ad hoc methods found in many machine learning papers, hdoutliers is based on a distributional model that allows outliers to be tagged with a probability. This critical feature reduces the likelihood of false discoveries.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Wilkinson, Leland},

	year = {2018},
	keywords = {Sociology, leibatt-aqp, cscheid-maybe, Robustness, Anomalies, Anomaly detection, Covariance matrices, Gaussian distribution, Outliers, Standards},
	pages = {256--266}
}

@article{nguyen_dspcp:_2018,
	title = {{DSPCP}: {A} {Data} {Scalable} {Approach} for {Identifying} {Relationships} in {Parallel} {Coordinates}},
	volume = {24},
	issn = {2160-9306},
	shorttitle = {{DSPCP}},
	doi = {10.1109/TVCG.2017.2661309},
	abstract = {Parallel coordinates plots (PCPs) are a well-studied technique for exploring multi-attribute datasets. In many situations, users find them a flexible method to analyze and interact with data. Unfortunately, using PCPs becomes challenging as the number of data items grows large or multiple trends within the data mix in the visualization. The resulting overdraw can obscure important features. A number of modifications to PCPs have been proposed, including using color, opacity, smooth curves, frequency, density, and animation to mitigate this problem. However, these modified PCPs tend to have their own limitations in the kinds of relationships they emphasize. We propose a new data scalable design for representing and exploring data relationships in PCPs. The approach exploits the point/line duality property of PCPs and a local linear assumption of data to extract and represent relationship summarizations. This approach simultaneously shows relationships in the data and the consistency of those relationships. Our approach supports various visualization tasks, including mixed linear and nonlinear pattern identification, noise detection, and outlier detection, all in large data. We demonstrate these tasks on multiple synthetic and real-world datasets.},
	number = {3},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Nguyen, Hoa and Rosen, Paul},

	year = {2018},
	keywords = {data analysis, data visualisation, Data visualization, Visualization, Histograms, data structures, Encoding, computational geometry, leibatt-aqp, visualization, cscheid-maybe, leibatt-aggregation, leibatt-maybe, Correlation, data items, data scalable approach, data scalable design, geometry-based approaches, identifying relationships, large data visualization, Market research, mixed linear pattern identification, multiattribute datasets, nonlinear pattern identification, parallel coordinates plot, parallel coordinates plots, relationship summarizations, representing exploring data relationships, Shape, cscheid-interaction-encode},
	pages = {1301--1315}
}

@misc{noauthor_extensions_2008,
	type = {text},
	title = {Extensions of {Parallel} {Coordinates} for {Interactive} {Exploration} of {Large} {Multi}-{Timepoint} {Data} {Sets}},
	url = {https://www.computer.org/csdl/journal/tg/2008/06/ttg2008061436/13rRUxjQyp8},
	abstract = {Parallel coordinate plots (PCPs) are commonly used in information visualization to provide insight into multi-variate data. These plots help to spot correlations between variables. PCPs have been successfully  applied to unstructured datasets up to a few millions of points. In this paper, we present techniques to enhance the usability of PCPs for  the exploration of large, multi-timepoint volumetric data sets, containing  tens of millions of points per timestep.   The main difficulties that arise when applying PCPs to large numbers of data points are visual clutter and slow performance, making interactive  exploration infeasible. Moreover, the spatial context of the volumetric  data is usually lost.  We describe techniques for preprocessing using data quantization and compression, and for fast GPU-based rendering of PCPs using joint density distributions for each pair of consecutive variables, resulting in a smooth,  continuous visualization. Also, fast brushing techniques are proposed for interactive data selection in multiple linked views, including a 3D spatial volume view.  These techniques have been successfully applied to three large data sets: Hurricane Isabel (Vis'04 contest), the ionization front instability data set (Vis'08 design contest), and data from a large-eddy simulation of cumulus clouds. With these data, we show how PCPs can be extended to successfully visualize and interactively explore multi-timepoint volumetric datasets with an order of magnitude more data points.},
	language = {en},
	urldate = {2020-01-10},

	year = {2008},
	doi = {10.1109/TVCG.2008.131},
	keywords = {cscheid-materialized-views, leibatt-materialized-views, leibatt-aggregation, leibatt-compression, leibatt-interaction-aggregate, leibatt-interaction-filter, leibatt-interaction-select, cscheid-interaction-aggregate, cscheid-interaction-filter, cscheid-data-parallel, cscheid-compression}
}

@inproceedings{stolte_polaris_2000,
	title = {Polaris: a system for query, analysis and visualization of multi-dimensional relational databases},
	shorttitle = {Polaris},
	url = {https://ieeexplore.ieee.org/document/885086},
	doi = {10.1109/INFVIS.2000.885086},
	abstract = {In the last several years, large multi-dimensional databases have become common in a variety of applications such as data warehousing and scientific computing. Analysis and exploration tasks place significant demands on the interfaces to these databases. Because of the size of the data sets, dense graphical representations are more effective for exploration than spreadsheets and charts. Furthermore, because of the exploratory nature of the analysis, it must be possible for the analysts to change visualizations rapidly as they pursue a cycle involving first hypothesis and then experimentation. The authors present Polaris, an interface for exploring large multi-dimensional databases that extends the well-known Pivot Table interface. The novel features of Polaris include an interface for constructing visual specifications of table based graphical displays and the ability to generate a precise set of relational queries from the visual specifications. The visual specifications can be rapidly and incrementally developed, giving the analyst visual feedback as they construct complex queries and visualizations.},
	booktitle = {{IEEE} {Symposium} on {Information} {Visualization} 2000. {INFOVIS} 2000. {Proceedings}},
	author = {Stolte, C. and Hanrahan, P.},

	year = {2000},
	note = {ISSN: 1522-404X},
	keywords = {query processing, Relational databases, data visualisation, Visualization, relational databases, very large databases, interactive systems, Polarization, cscheid-materialized-views, leibatt-aggregation, leibatt-interaction-aggregate, leibatt-interaction-filter, leibatt-interaction-select, leibatt-interaction-arrange, leibatt-interaction-encode, complex queries, data sets, dense graphical representations, exploration tasks, formal specification, large multi-dimensional databases, multi-dimensional relational database visualization, Pivot Table interface, Polaris, relational queries, table based graphical displays, user interface, user interfaces, visual feedback, visual specifications, cscheid-db-application, leibatt-db-application, leibatt-interaction-change, cscheid-interaction-aggregate, cscheid-interaction-filter, cscheid-interaction-select, cscheid-interaction-encode, cscheid-interaction-change},
	pages = {5--14}
}

@article{weiss_supercubes_2009,
	title = {Supercubes: {A} {High}-{Level} {Primitive} for {Diamond} {Hierarchies}},
	volume = {15},
	issn = {2160-9306},
	shorttitle = {Supercubes},
	doi = {10.1109/TVCG.2009.186},
	abstract = {Volumetric datasets are often modeled using a multiresolution approach based on a nested decomposition of the domain into a polyhedral mesh. Nested tetrahedral meshes generated through the longest edge bisection rule are commonly used to decompose regular volumetric datasets since they produce highly adaptive crack-free representations. Efficient representations for such models have been achieved by clustering the set of tetrahedra sharing a common longest edge into a structure called a diamond. The alignment and orientation of the longest edge can be used to implicitly determine the geometry of a diamond and its relations to the other diamonds within the hierarchy. We introduce the supercube as a high-level primitive within such meshes that encompasses all unique types of diamonds. A supercube is a coherent set of edges corresponding to three consecutive levels of subdivision. Diamonds are uniquely characterized by the longest edge of the tetrahedra forming them and are clustered in supercubes through the association of the longest edge of a diamond with a unique edge in a supercube. Supercubes are thus a compact and highly efficient means of associating information with a subset of the vertices, edges and tetrahedra of the meshes generated through longest edge bisection. We demonstrate the effectiveness of the supercube representation when encoding multiresolution diamond hierarchies built on a subset of the points of a regular grid. We also show how supercubes can be used to efficiently extract meshes from diamond hierarchies and to reduce the storage requirements of such variable-resolution meshes.},
	number = {6},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Weiss, Kenneth and De Floriani, Leila},

	year = {2009},
	keywords = {Encoding, Computer science, computational geometry, cscheid-maybe, leibatt-maybe, Geometry, Isosurfaces, diamond hierarchies, diamonds, edge bisection rule, hierarchy of diamonds, highly adaptive crack-free representations, Longest edge bisection, mesh generation, multiresolution approach, multiresolution models, nested decomposition, nested tetrahedral meshes, polyhedral mesh, rendering (computer graphics), selective refinement., Spatial resolution, supercubes, tetrahedra sharing, volumetric datasets},
	pages = {1603--1610}
}

@article{piringer_multi-threading_2009,
	title = {A {Multi}-{Threading} {Architecture} to {Support} {Interactive} {Visual} {Exploration}},
	volume = {15},
	issn = {2160-9306},
	doi = {10.1109/TVCG.2009.110},
	abstract = {During continuous user interaction, it is hard to provide rich visual feedback at interactive rates for datasets containing millions of entries. The contribution of this paper is a generic architecture that ensures responsiveness of the application even when dealing with large data and that is applicable to most types of information visualizations. Our architecture builds on the separation of the main application thread and the visualization thread, which can be cancelled early due to user interaction. In combination with a layer mechanism, our architecture facilitates generating previews incrementally to provide rich visual feedback quickly. To help avoiding common pitfalls of multi-threading, we discuss synchronization and communication in detail. We explicitly denote design choices to control trade-offs. A quantitative evaluation based on the system VI S P L ORE shows fast visual feedback during continuous interaction even for millions of entries. We describe instantiations of our architecture in additional tools.},
	number = {6},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Piringer, Harald and Tominski, Christian and Muigg, Philipp and Berger, Wolfgang},

	year = {2009},
	keywords = {data visualisation, Data visualization, Feedback, Concurrent computing, cscheid-materialized-views, leibatt-progressive, cscheid-aqp, cscheid-mqo, leibatt-materialized-views, cscheid-progressive, leibatt-interaction-aggregate, leibatt-interaction-filter, leibatt-interaction-select, leibatt-interaction-navigate, user interfaces, visual feedback, Communication system control, continuous interaction, continuous user interaction, Delay, Frequency synchronization, Information visualization architecture, information visualizations, Interactive systems, interactive visual exploration, layer, Manipulator dynamics, multi-threading, multi-threading architecture, Navigation, preview, software architecture, VISPLORE, cscheid-reuse, leibatt-data-parallel, cscheid-interaction-aggregate, cscheid-interaction-navigate, cscheid-interaction-filter},
	pages = {1113--1120}
}

@inproceedings{bethel_accelerating_2006,
	title = {Accelerating {Network} {Traffic} {Analytics} {Using} {Query}-{Driven} {Visualization}},
	doi = {10.1109/VAST.2006.261437},
	abstract = {Realizing operational analytics solutions where large and complex data must be analyzed in a time-critical fashion entails integrating many different types of technology. This paper focuses on an interdisciplinary combination of scientific data management and visualization/analysis technologies targeted at reducing the time required for data filtering, querying, hypothesis testing and knowledge discovery in the domain of network connection data analysis. We show that use of compressed bitmap indexing can quickly answer queries in an interactive visual data analysis application, and compare its performance with two alternatives for serial and parallel filtering/querying on 2.5 billion records' worth of network connection data collected over a period of 42 weeks. Our approach to visual network connection data exploration centers on two primary factors: interactive ad-hoc and multiresolution query formulation and execution over n dimensions and visual display of the n-dimensional histogram results. This combination is applied in a case study to detect a distributed network scan and to then identify the set of remote hosts participating in the attack. Our approach is sufficiently general to be applied to a diverse set of data understanding problems as well as used in conjunction with a diverse set of analysis and visualization tools},
	booktitle = {2006 {IEEE} {Symposium} {On} {Visual} {Analytics} {Science} {And} {Technology}},
	author = {Bethel, E. Wes and Campbell, Scott and Dart, Eli and Stockinger, Kurt and Wu, Kesheng},

	year = {2006},
	note = {ISSN: null},
	keywords = {Data analysis, data visualisation, Data visualization, Time factors, data mining, Filtering, leibatt-interaction-filter, query-driven visualization, visual analytics, Acceleration, compressed bitmap indexing, data filtering, hypothesis testing, Indexing, interactive ad-hoc formulation, interactive visual data analysis, knowledge discovery, Knowledge management, multiresolution query formulation, network connection data analysis, network security, network traffic analytics, query formulation, scientific data management, Technology management, telecommunication traffic, Telecommunication traffic, Testing, cscheid-db-application, leibatt-db-application, cscheid-interaction-filter},
	pages = {115--122}
}

@article{cui_measuring_2006,
	title = {Measuring {Data} {Abstraction} {Quality} in {Multiresolution} {Visualizations}},
	volume = {12},
	issn = {2160-9306},
	doi = {10.1109/TVCG.2006.161},
	abstract = {Data abstraction techniques are widely used in multiresolution visualization systems to reduce visual clutter and facilitate analysis from overview to detail. However, analysts are usually unaware of how well the abstracted data represent the original dataset, which can impact the reliability of results gleaned from the abstractions. In this paper, we define two data abstraction quality measures for computing the degree to which the abstraction conveys the original dataset: the histogram difference measure and the nearest neighbor measure. They have been integrated within XmdvTool, a public-domain multiresolution visualization system for multivariate data analysis that supports sampling as well as clustering to simplify data. Several interactive operations are provided, including adjusting the data abstraction level, changing selected regions, and setting the acceptable data abstraction quality level. Conducting these operations, analysts can select an optimal data abstraction level. Also, analysts can compare different abstraction methods using the measures to see how well relative data density and outliers are maintained, and then select an abstraction method that meets the requirement of their analytic tasks},
	number = {5},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Cui, Qingguang and Ward, Matthew and Rundensteiner, Elke and Yang, Jing},

	year = {2006},
	keywords = {data analysis, Data analysis, data visualisation, Data visualization, Histograms, very large databases, data structures, pattern clustering, sampling methods, leibatt-aqp, cscheid-maybe, leibatt-maybe, Sampling methods, Sampling, Delay, Bioinformatics, Clustering, Coordinate measuring machines, data abstraction quality measures, data clustering, Density measurement, Displays, histogram difference measure, Metrics, Multiresolution Visualization Authors 1:, multivariate data analysis, nearest neighbor measure, Nearest neighbor searches, public-domain multiresolution visualization system, XmdvTool},
	pages = {709--716}
}

@book{chi_case_2000,
	title = {Case {Study}: {Resource} {Steering} in a {Visualization} {System}},
	isbn = {978-3-211-83515-9},
	shorttitle = {Case {Study}},
	url = {https://diglib.eg.org:443/xmlui/handle/10.2312/VisSym.VisSym00.269-278},
	abstract = {Visual computational steering environments extend traditional visualization environments by enabling the user to interactively steer the computations applied to the data. In this paper, we develop a new type of computational steering.  Resource steering  extends current visual steering techniques by providing machine resource estimation and control to the user. With resource steering, the user controls the execution of the computation on a parallel or distributed computer based on experimentally or theoretically derived estimates of the parallel performance of the computation. We demonstrate this extended steering model by applying it to an information visualization system that analyzes genetic sequence similarity reports. We show how our extended steering model enhances the user s ability to control visualization computations.},
	language = {en},
	urldate = {2020-01-08},
	publisher = {The Eurographics Association},
	author = {Chi, Ed H. and Riedl, John T.},
	year = {2000},
	doi = {http://dx.doi.org/10.2312/VisSym/VisSym00/269-278},
	keywords = {leibatt-progressive, cscheid-maybe, leibatt-maybe, cscheid-humaninloop}
}

@book{stegmaier_generic_2002,
	title = {A {Generic} {Solution} for {Hardware}-{Accelerated} {Remote} {Visualization}},
	isbn = {978-1-58113-536-7},
	url = {https://diglib.eg.org:443/xmlui/handle/10.2312/VisSym.VisSym02.087-094},
	abstract = {This paper presents a generic solution for hardware-accelerated remote visualization that works transparently for all OpenGL-based applications and OpenGL-based scene graphs. Universality is achieved by taking advantage of dynamic linking, efficient data transfer by means of VNC. The proposed solution does not require any modifications of existing applications and allows for remote visualization with different hardware architectures involved in the visualization process. The library s performance is evaluated using standard OpenGL example programs and by volume rendering substantial data sets.},
	language = {en},
	urldate = {2020-01-08},
	publisher = {The Eurographics Association},
	author = {Stegmaier, Simon and Magallón, Marcelo and Ertl, Thomas},
	year = {2002},
	doi = {http://dx.doi.org/10.2312/VisSym/VisSym02/087-094},
	keywords = {cscheid-maybe, leibatt-maybe}
}

@book{chisnall_knowledge-based_2006,
	title = {Knowledge-{Based} {Out}-of-{Core} {Algorithms} for {Data} {Management} in {Visualization}},
	isbn = {978-3-905673-31-9},
	url = {https://diglib.eg.org:443/xmlui/handle/10.2312/VisSym.EuroVis06.107-114},
	abstract = {Data management is the very first issue in handling very large datasets. Many existing out-of-core algorithms used in visualization are closely coupled with application-specific logic. This paper presents two knowledgebased out-of-core prefetching algorithms that do not use hard-coded rendering-related logic. They acquire the knowledge of the access history and patterns dynamically, and adapt their prefetching strategies accordingly. We have compared the algorithms with a demand-based algorithm, as well as a more domain-specific out-of-core algorithm. We carried out our evaluation in conjunction with an example application where rendering multiple point sets in a volume scene graph put a great strain on the rendering algorithm in terms of memory management. Our results have shown that the knowledge-based approach offers a better cache-hit to disk-access trade-off. This work demonstrates that it is possible to build an out-of-core prefetching algorithm without depending on rendering-related application-specific logic. The knowledge based approach has the advantage of being generic, efficient, flexible and self-adaptive.},
	language = {en},
	urldate = {2020-01-08},
	publisher = {The Eurographics Association},
	author = {Chisnall, David and Chen, Min and Hansen, Charles},
	year = {2006},
	doi = {http://dx.doi.org/10.2312/VisSym/EuroVis06/107-114},
	keywords = {cscheid-user-modeling, cscheid-maybe, leibatt-user-modeling, leibatt-maybe, cscheid-navigate, leibatt-interaction-navigate}
}

@book{hao_multi-resolution_2007,
	title = {Multi-{Resolution} {Techniques} for {Visual} {Exploration} of {Large} {Time}-{Series} {Data}},
	isbn = {978-3-905673-45-6},
	url = {https://diglib.eg.org:443/xmlui/handle/10.2312/VisSym.EuroVis07.027-034},
	abstract = {Time series are a data type of utmost importance in many domains such as business management and service monitoring. We address the problem of visualizing large time-related data sets which are difficult to visualize effectively with standard techniques given the limitations of current display devices. We propose a framework for intelligent time- and data-dependent visual aggregation of data along multiple resolution levels. This idea leads to effective visualization support for long time-series data providing both focus and context. The basic idea of the technique is that either data-dependent or application-dependent, display space is allocated in proportion to the degree of interest of data subintervals, thereby (a) guiding the user in perceiving important information, and (b) freeing required display space to visualize all the data. The automatic part of the framework can accommodate any time series analysis algorithm yielding a numeric degree of interest scale. We apply our techniques on real-world data sets, compare it with the standard visualization approach, and conclude the usefulness and scalability of the approach.},
	language = {en},
	urldate = {2020-01-08},
	publisher = {The Eurographics Association},
	author = {Hao, Ming and Dayal, Umeshwar and Keim, Daniel and Schreck, Tobias},
	year = {2007},
	doi = {http://dx.doi.org/10.2312/VisSym/EuroVis07/027-034},
	keywords = {cscheid-maybe, leibatt-aggregation, leibatt-interaction-aggregate, cscheid-interaction-aggregate}
}

@article{lex_visbricks_2011,
	title = {{VisBricks}: {Multiform} {Visualization} of {Large}, {Inhomogeneous} {Data}},
	volume = {17},
	issn = {2160-9306},
	shorttitle = {{VisBricks}},
	url = {https://ieeexplore.ieee.org/document/6064995},
	doi = {10.1109/TVCG.2011.250},
	abstract = {Large volumes of real-world data often exhibit inhomogeneities: vertically in the form of correlated or independent dimensions and horizontally in the form of clustered or scattered data items. In essence, these inhomogeneities form the patterns in the data that researchers are trying to find and understand. Sophisticated statistical methods are available to reveal these patterns, however, the visualization of their outcomes is mostly still performed in a one-view-fits-all manner, In contrast, our novel visualization approach, VisBricks, acknowledges the inhomogeneity of the data and the need for different visualizations that suit the individual characteristics of the different data subsets. The overall visualization of the entire data set is patched together from smaller visualizations, there is one VisBrick for each cluster in each group of interdependent dimensions. Whereas the total impression of all VisBricks together gives a comprehensive high-level overview of the different groups of data, each VisBrick independently shows the details of the group of data it represents, State-of-the-art brushing and visual linking between all VisBricks furthermore allows the comparison of the groupings and the distribution of data items among them. In this paper, we introduce the VisBricks visualization concept, discuss its design rationale and implementation, and demonstrate its usefulness by applying it to a use case from the field of biomedicine.},
	number = {12},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Lex, Alexander and Schulz, Hans-Jorg and Streit, Marc and Partl, Christian and Schmalstieg, Dieter},

	year = {2011},
	keywords = {data visualisation, Data visualization, data visualization, Algorithms, Computer Graphics, User-Computer Interface, cscheid-maybe, leibatt-maybe, Semantics, multiple coordinated views, biomedicine field, Cluster Analysis, Computer Simulation, Data Interpretation, Statistical, Humans, inhomogeneous data, Inhomogeneous data, medical computing, multiform visualization., Nonhomogeneous media, statistical method, VisBricks visualization concept},
	pages = {2291--2300}
}

@inproceedings{zhang_visual_2012,
	title = {Visual analytics for the big data era — {A} comparative review of state-of-the-art commercial systems},
	url = {https://ieeexplore.ieee.org/document/6400554},
	doi = {10.1109/VAST.2012.6400554},
	abstract = {Visual analytics (VA) system development started in academic research institutions where novel visualization techniques and open source toolkits were developed. Simultaneously, small software companies, sometimes spin-offs from academic research institutions, built solutions for specific application domains. In recent years we observed the following trend: some small VA companies grew exponentially; at the same time some big software vendors such as IBM and SAP started to acquire successful VA companies and integrated the acquired VA components into their existing frameworks. Generally the application domains of VA systems have broadened substantially. This phenomenon is driven by the generation of more and more data of high volume and complexity, which leads to an increasing demand for VA solutions from many application domains. In this paper we survey a selection of state-of-the-art commercial VA frameworks, complementary to an existing survey on open source VA tools. From the survey results we identify several improvement opportunities as future research directions.},
	booktitle = {2012 {IEEE} {Conference} on {Visual} {Analytics} {Science} and {Technology} ({VAST})},
	author = {Zhang, Leishi and Stoffel, Andreas and Behrisch, Michael and Mittelstadt, Sebastian and Schreck, Tobias and Pompl, René and Weber, Stefan and Last, Holger and Keim, Daniel},

	year = {2012},
	note = {ISSN: null},
	keywords = {Data models, data visualisation, Data visualization, Analytical models, cscheid-survey, Visual analytics, leibatt-survey, Bismuth, Data handling, H.4 [Information Systems]: INFORMATION SYSTEMS APPLICATIONS, IBM, K.1 [Computing Milieux], open source toolkit, public domain software, SAP, Software, software company, software vendor, THE COMPUTER INDUSTRY — Markets, visual analytics system development, visualization technique, leibatt-db-application},
	pages = {173--182}
}

@article{yuan_scalable_2010,
	title = {Scalable {Multi}-variate {Analytics} of {Seismic} and {Satellite}-based {Observational} {Data}},
	volume = {16},
	issn = {2160-9306},
	doi = {10.1109/TVCG.2010.192},
	abstract = {Over the past few years, large human populations around the world have been affected by an increase in significant seismic activities. For both conducting basic scientific research and for setting critical government policies, it is crucial to be able to explore and understand seismic and geographical information obtained through all scientific instruments. In this work, we present a visual analytics system that enables explorative visualization of seismic data together with satellite-based observational data, and introduce a suite of visual analytical tools. Seismic and satellite data are integrated temporally and spatially. Users can select temporal ;and spatial ranges to zoom in on specific seismic events, as well as to inspect changes both during and after the events. Tools for designing high dimensional transfer functions have been developed to enable efficient and intuitive comprehension of the multi-modal data. Spread-sheet style comparisons are used for data drill-down as well as presentation. Comparisons between distinct seismic events are also provided for characterizing event-wise differences. Our system has been designed for scalability in terms of data size, complexity (i.e. number of modalities), and varying form factors of display environments.},
	number = {6},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Yuan, Xiaoru and Xiao, He and Guo, Hanqi and Guo, Peihong and Kendall, Wesley and Huang, Jian and Zhang, Yongxian},

	year = {2010},
	keywords = {Catalogs, data visualisation, Data visualization, leibatt-aqp, cscheid-maybe, leibatt-interaction-filter, leibatt-interaction-navigate, Image color analysis, Earth Science Visualization, Earthquakes, geographic information systems, geographical information, Multivariate Visualization, research and development, satellite based observational data, Satellites, scalable multivariate analytics, Scalable Visualization, scientific research, seismic based observational data, Seismic Data, seismology, Three dimensional displays, Transfer functions, visual analytics system},
	pages = {1413--1420}
}

@article{ferreira_visual_2013,
	title = {Visual {Exploration} of {Big} {Spatio}-{Temporal} {Urban} {Data}: {A} {Study} of {New} {York} {City} {Taxi} {Trips}},
	volume = {19},
	issn = {2160-9306},
	shorttitle = {Visual {Exploration} of {Big} {Spatio}-{Temporal} {Urban} {Data}},
	url = {https://ieeexplore.ieee.org/document/6634127},
	doi = {10.1109/TVCG.2013.226},
	abstract = {As increasing volumes of urban data are captured and become available, new opportunities arise for data-driven analysis that can lead to improvements in the lives of citizens through evidence-based decision making and policies. In this paper, we focus on a particularly important urban data set: taxi trips. Taxis are valuable sensors and information associated with taxi trips can provide unprecedented insight into many different aspects of city life, from economic activity and human behavior to mobility patterns. But analyzing these data presents many challenges. The data are complex, containing geographical and temporal components in addition to multiple variables associated with each trip. Consequently, it is hard to specify exploratory queries and to perform comparative analyses (e.g., compare different regions over time). This problem is compounded due to the size of the data-there are on average 500,000 taxi trips each day in NYC. We propose a new model that allows users to visually query taxi trips. Besides standard analytics queries, the model supports origin-destination queries that enable the study of mobility across the city. We show that this model is able to express a wide range of spatio-temporal queries, and it is also flexible in that not only can queries be composed but also different aggregations and visual representations can be applied, allowing users to explore and compare results. We have built a scalable system that implements this model which supports interactive response times; makes use of an adaptive level-of-detail rendering strategy to generate clutter-free visualization for large results; and shows hidden details to the users in a summary through the use of overlay heat maps. We present a series of case studies motivated by traffic engineers and economists that show how our model and system enable domain experts to perform tasks that were previously unattainable for them.},
	number = {12},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Ferreira, Nivan and Poco, Jorge and Vo, Huy T. and Freire, Juliana and Silva, Cláudio T.},

	year = {2013},
	keywords = {Data models, query processing, visual exploration, data visualisation, Data visualization, Time factors, Analytical models, Algorithms, Computer Graphics, Reproducibility of Results, Sensitivity and Specificity, Spatio-Temporal Analysis, User-Computer Interface, Visual analytics, leibatt-spatial-index, leibatt-interaction-aggregate, leibatt-interaction-filter, leibatt-interaction-select, leibatt-interaction-navigate, decision making, cscheid-spatial-index, rendering (computer graphics), adaptive level-of-detail rendering strategy, big spatio-temporal urban data, Cities and towns, clutter-free visualization, data-driven analysis, economic activity, evidence-based decision making, evidence-based decision policies, Geographic Information Systems, geographical components, human behavior, Mathematical model, mobility patterns, Models, Statistical, Motor Vehicles, New York City, New York City taxi trips, NYC taxis, origin-destination queries, Spatio-temporal queries, spatiotemporal phenomena, spatiotemporal queries, standard analytics, taxi trip visually query, temporal components, traffic engineers, traffic information systems, urban data, urban data set, visual representations, cscheid-interaction-aggregate, cscheid-interaction-navigate, cscheid-interaction-filter, leibatt-index, cscheid-interaction-select, cscheid-index},
	pages = {2149--2158}
}

@book{bach_review_2014,
	title = {A {Review} of {Temporal} {Data} {Visualizations} {Based} on {Space}-{Time} {Cube} {Operations}},
	isbn = {978-3-03868-028-4},
	url = {https://diglib.eg.org:443/xmlui/handle/10.2312/eurovisstar.20141171.023-041},
	abstract = {We review a range of temporal data visualization techniques through a new lens, by describing them as series of operations performed on a conceptual space-time cube. These operations include extracting subparts of a space-time cube, flattening it across space or time, or transforming the cube's geometry or content. We introduce a taxonomy of elementary space-time cube operations, and explain how they can be combined to turn a three-dimensional space-time cube into an easily-readable two-dimensional visualization. Our model captures most visualizations showing two or more data dimensions in addition to time, such as geotemporal visualizations, dynamic networks, time-evolving scatterplots, or videos. We finally review interactive systems that support a range of operations. By introducing this conceptual framework we hope to facilitate the description, criticism and comparison of existing temporal data visualizations, as well as encourage the exploration of new techniques and systems.},
	language = {en},
	urldate = {2020-01-06},
	publisher = {The Eurographics Association},
	author = {Bach, B. and Dragicevic, P. and Archambault, D. and Hurter, C. and Carpendale, S.},
	year = {2014},
	doi = {http://dx.doi.org/10.2312/eurovisstar.20141171},
	keywords = {cscheid-survey, leibatt-aggregation, leibatt-survey, leibatt-interaction-aggregate, leibatt-interaction-filter, leibatt-interaction-navigate, leibatt-filter}
}

@article{moritz_perfopticon_2015,
	title = {Perfopticon: {Visual} {Query} {Analysis} for {Distributed} {Databases}},
	url = {https://onlinelibrary.wiley.com/doi/full/10.1111/cgf.12619},
	abstract = {Distributed database performance is often unpredictable due to issues such as system complexity, network congestion, or imbalanced data distribution. These issues are difficult for users to assess ...},
	language = {EN},
	urldate = {2020-01-06},
	journal = {Computer Graphics Forum},
	author = {Moritz, Dominik and Halperin, Daniel and Howe, Bill and Heer, Jeffrey},

	year = {2015},
	keywords = {cscheid-maybe, leibatt-maybe}
}

@misc{noauthor_item_nodate,
	title = {Item sampling for information architecture {\textbar} {Proceedings} of the {SIGCHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	url = {https://dl.acm.org/doi/abs/10.1145/1978942.1979264},
	language = {EN},
	urldate = {2020-01-06},
	keywords = {cscheid-aqp, cscheid-maybe, leibatt-maybe, cscheid-evaluation}
}

@misc{noauthor_polyzoom_nodate,
	title = {Polyzoom {\textbar} {Proceedings} of the {SIGCHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	url = {https://dl.acm.org/doi/abs/10.1145/2207676.2207716},
	language = {EN},
	urldate = {2020-01-06},
	keywords = {cscheid-maybe, leibatt-maybe}
}

@misc{noauthor_cascade_nodate,
	title = {Cascade {\textbar} {Proceedings} of the {SIGCHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	url = {https://dl.acm.org/doi/abs/10.1145/2470654.2466265},
	language = {EN},
	urldate = {2020-01-06},
	keywords = {cscheid-maybe, leibatt-maybe}
}

@misc{noauthor_sample-oriented_nodate,
	title = {Sample-oriented task-driven visualizations {\textbar} {Proceedings} of the {SIGCHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	url = {https://dl.acm.org/doi/abs/10.1145/2556288.2557131},
	language = {EN},
	urldate = {2020-01-06},
	keywords = {cscheid-maybe, leibatt-maybe}
}

@article{wood_interactive_2007,
	title = {Interactive {Visual} {Exploration} of a {Large} {Spatio}-temporal {Dataset}: {Reflections} on a {Geovisualization} {Mashup}.},
	volume = {13},
	issn = {2160-9306},
	shorttitle = {Interactive {Visual} {Exploration} of a {Large} {Spatio}-temporal {Dataset}},
	doi = {10.1109/TVCG.2007.70570},
	abstract = {Exploratory visual analysis is useful for the preliminary investigation of large structured, multifaceted spatio-temporal datasets. This process requires the selection and aggregation of records by time, space and attribute, the ability to transform data and the flexibility to apply appropriate visual encodings and interactions. We propose an approach inspired by geographical 'mashups' in which freely-available functionality and data are loosely but flexibly combined using de facto exchange standards. Our case study combines MySQL, PHP and the LandSerf GIS to allow Google Earth to be used for visual synthesis and interaction with encodings described in KML. This approach is applied to the exploration of a log of 1.42 million requests made of a mobile directory service. Novel combinations of interaction and visual encoding are developed including spatial 'tag clouds', 'tag maps', 'data dials' and multi-scale density surfaces. Four aspects of the approach are informally evaluated: the visual encodings employed, their success in the visual exploration of the dataset, the specific tools used and the 'mashup' approach. Preliminary findings will be beneficial to others considering using mashups for visualization. The specific techniques developed may be more widely applied to offer insights into the structure of multifarious spatio-temporal data of the type explored here.},
	number = {6},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Wood, Jo and Dykes, Jason and Slingsby, Aidan and Clarke, Keith},

	year = {2007},
	keywords = {data visualisation, Data visualization, Data mining, Encoding, visual encodings, Filters, leibatt-aggregation, leibatt-interaction-aggregate, leibatt-interaction-filter, leibatt-interaction-select, leibatt-interaction-derive, leibatt-interaction-navigate, Spatial resolution, interactive visual exploration, geographic information systems, Geographic Information Systems, spatiotemporal phenomena, Application software, applications of infovis., data dials, de facto exchange standards, Earth, geographic visualization, geographical mashups, geovisualization mashup, Google Earth, KML, LandSerf GIS, Large dataset visualization, Mashups, mobile directory service, multi-scale density surfaces, multiresolution visualization, MySQL, PHP, Reflection, spatial tag clouds, spatio-temporal dataset, SQL, tag maps, text and document visualization, cscheid-db-application, leibatt-db-application, cscheid-interaction-navigate, cscheid-interaction-filter, cscheid-interaction-select, cscheid-interaction-encode, cscheid-interaction-derive},
	pages = {1176--1183}
}

@inproceedings{tesone_balancing_2007,
	title = {Balancing {Interactive} {Data} {Management} of {Massive} {Data} with {Situational} {Awareness} through {Smart} {Aggregation}},
	doi = {10.1109/VAST.2007.4388998},
	abstract = {Designing a visualization system capable of processing, managing, and presenting massive data sets while maximizing the user's situational awareness (SA) is a challenging, but important, research question in visual analytics. Traditional data management and interactive retrieval approaches have often focused on solving the data overload problem at the expense of the user's SA. This paper discusses various data management strategies and the strengths and limitations of each approach in providing the user with SA. A new data management strategy, coined Smart Aggregation, is presented as a powerful approach to overcome the challenges of both massive data sets and maintaining SA. By combining automatic data aggregation with user-defined controls on what, how, and when data should be aggregated, we present a visualization system that can handle massive amounts of data while affording the user with the best possible SA. This approach ensures that a system is always usable in terms of both system resources and human perceptual resources. We have implemented our Smart Aggregation approach in a visual analytics system called VIAssist (Visual Assistant for Information Assurance Analysis) to facilitate exploration, discovery, and SA in the domain of Information Assurance.},
	booktitle = {2007 {IEEE} {Symposium} on {Visual} {Analytics} {Science} and {Technology}},
	author = {Tesone, Daniel R. and Goodall, John R.},

	year = {2007},
	note = {ISSN: null},
	keywords = {data visualisation, Data visualization, interactive systems, Databases, Information analysis, Visual analytics, cscheid-maybe, leibatt-aggregation, leibatt-interaction-aggregate, leibatt-interaction-filter, leibatt-interaction-navigate, information visualization, visual analytics, Technology management, Displays, visual analytics system, Automation, Data management, data retrieval, Energy management, Human resource management, Information retrieval, interactive data management, massive data sets, situational awareness, smart aggregation, smart data aggregation, user situational awareness, visualization system},
	pages = {67--74}
}

@inproceedings{childs_contract_2005,
	title = {A contract based system for large data visualization},
	doi = {10.1109/VISUAL.2005.1532795},
	abstract = {VisIt is a richly featured visualization tool that is used to visualize some of the largest simulations ever run. The scale of these simulations requires that optimizations are incorporated into every operation VisIt performs. But the set of applicable optimizations that VisIt can perform is dependent on the types of operations being done. Complicating the issue, VisIt has a plugin capability that allows new, unforeseen components to be added, making it even harder to determine which optimizations can be applied. We introduce the concept of a contract to the standard data flow network design. This contract enables each component of the data flow network to modify the set of optimizations used. In addition, the contract allows for new components to be accommodated gracefully within VisIt's data flow network system.},
	booktitle = {{VIS} 05. {IEEE} {Visualization}, 2005.},
	author = {Childs, H. and Brugger, E. and Bonnell, K. and Meredith, J. and Miller, M. and Whitlock, B. and Max, N.},

	year = {2005},
	note = {ISSN: null},
	keywords = {Data analysis, data visualisation, Data visualization, very large databases, Concurrent computing, Graphics, Geometry, Rendering (computer graphics), contract based system, Contracts, data flow computing, featured visualization tool, Hardware, Laboratories, Pipelines, VisIt data flow network design, cscheid-db-application, cscheid-interaction-filter, cscheid-interaction},
	pages = {191--198}
}

@inproceedings{stockinger_query-driven_2005,
	title = {Query-driven visualization of large data sets},
	url = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1532792&isnumber=32702},
	doi = {10.1109/VISUAL.2005.1532792},
	abstract = {We present a practical and general-purpose approach to large and
complex visual data analysis where visualization processing, rendering and subsequent human interpretation is constrained to the
subset of data deemed interesting by the user. In many scientific data analysis applications, “interesting” data can be defined
by compound Boolean range queries of the form (temperature {\textgreater}
1000) AND (70 {\textless} pressure {\textless} 90). As data sizes grow larger, a
central challenge is to answer such queries as efficiently as possible.
Prior work in the visualization community has focused on answering range queries for scalar fields within the context of accelerating
the search phase of isosurface algorithms. In contrast, our work describes an approach that leverages state-of-the-art indexing technology from the scientific data management community called “bitmap
indexing.” Our implementation, which we call “DEX” (short for
dextrous data explorer), uses bitmap indexing to efficiently answer
multivariate, multidimensional data queries to provide input to a
visualization pipeline. We present an analysis overview and benchmark results that show bitmap indexing offers significant storage
and performance improvements when compared to previous approaches for accelerating the search phase of isosurface algorithms.
More importantly, since bitmap indexing supports complex multidimensional, multivariate range queries, it is more generally applicable to scientific data visualization and analysis problems. In addition to benchmark performance and analysis, we apply DEX to a
typical scientific visualization problem encountered in combustion
simulation data analysis.},
	booktitle = {{VIS} 05. {IEEE} {Visualization}, 2005.},
	publisher = {IEEE},
	author = {Stockinger, K. and Shalf, J. and Wu, K. and Bethel, E. W. and Miller, M. and Whitlock, B. and Max, N.},
	year = {2005},
	keywords = {cscheid-maybe, leibatt-maybe},
	pages = {167--174}
}

@inproceedings{derthick_constant_2003,
	title = {Constant density displays using diversity sampling},
	doi = {10.1109/INFVIS.2003.1249019},
	abstract = {The Informedia Digital Video Library user interface summarizes query results with a collage of representative keyframes. We present a user study in which keyframe occlusion caused difficulties. To use the screen space most efficiently to display images, both occlusion and wasted whitespace should be minimized. Thus optimal choices will tend toward constant density displays. However, previous constant density algorithms are based on global density, which leads to occlusion and empty space if the density is not uniform. We introduce an algorithm that considers the layout of individual objects and avoids occlusion altogether. Efficiency concerns are important for dynamic summaries of the Informedia Digital Video Library, which has hundreds of thousands of shots. Posting multiple queries that take into account parameters of the visualization as well as the original query reduces the amount of work required. This greedy algorithm is then compared to an optimal one. The approach is also applicable to visualizations containing complex graphical objects other than images, such as text, icons, or trees.},
	booktitle = {{IEEE} {Symposium} on {Information} {Visualization} 2003 ({IEEE} {Cat}. {No}.{03TH8714})},
	author = {Derthick, M. and Christel, M.G. and Hauptmann, A.G. and Wactlar, H.D.},

	year = {2003},
	note = {ISSN: null},
	keywords = {data visualisation, Visualization, graphical user interfaces, Computer science, leibatt-aqp, cscheid-maybe, leibatt-interaction-filter, leibatt-interaction-navigate, information visualization, Sampling methods, user interface, Information retrieval, computer displays, Computer displays, constant density algorithms, constant density displays, diversity sampling, global density, graphical objects, greedy algorithm, Greedy algorithms, hidden feature removal, human factors, image display, Image retrieval, Informedia Digital Video Library, keyframe occlusion, query results, representative keyframes, Software libraries, Tree graphs, User interfaces},
	pages = {137--144}
}

@inproceedings{conklin_multiple_2002,
	title = {Multiple foci drill-down through tuple and attribute aggregation polyarchies in tabular data},
	doi = {10.1109/INFVIS.2002.1173158},
	abstract = {Information analysis often involves decomposing data into sub-groups to allow for comparison and identification of relationships. Breakdown Visualization provides a mechanism to support this analysis through user guided drill-down of polyarchical metadata. This metadata describes multiple hierarchical structures for organizing tuple aggregations and table attributes. This structure is seen in financial data, organizational structures, sport statistics, and other domains. A spreadsheet format enables comparison of visualizations at any level of the hierarchy. Breakdown Visualization allows users to drill-down a single hierarchy then pivot into another hierarchy within the same view. It utilizes a fix and move technique that allows users to select multiple foci for drill-down.},
	booktitle = {{IEEE} {Symposium} on {Information} {Visualization}, 2002. {INFOVIS} 2002.},
	author = {Conklin, N. and Prabhakar, S. and North, C.},

	year = {2002},
	note = {ISSN: 1522-404X},
	keywords = {Data analysis, data visualisation, Data visualization, data visualization, Performance analysis, Statistics, Computer science, Information analysis, cscheid-maybe, leibatt-maybe, attribute aggregation polyarchies, Electric breakdown, Geography, multiple foci drill-down, multiple hierarchical structures, Organizing, Pattern analysis, polyarchical metadata, sport statistics, spreadsheet format, table attributes, tabular data, tuple aggregation polyarchies},
	pages = {131--134}
}

@article{moritz_perfopticon_2015-1,
	title = {Perfopticon: {Visual} {Query} {Analysis} for {Distributed} {Databases}},
	volume = {34},
	copyright = {© 2015 The Author(s) Computer Graphics Forum © 2015 The Eurographics Association and John Wiley \& Sons Ltd. Published by John Wiley \& Sons Ltd.},
	issn = {1467-8659},
	shorttitle = {Perfopticon},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.12619},
	doi = {10.1111/cgf.12619},
	abstract = {Distributed database performance is often unpredictable due to issues such as system complexity, network congestion, or imbalanced data distribution. These issues are difficult for users to assess in part due to the opaque mapping between declaratively specified queries and actual physical execution plans. Database developers currently must expend significant time and effort scanning log files to isolate and debug the root causes of performance issues. In response, we present Perfopticon, an interactive query profiling tool that enables rapid insight into common problems such as performance bottlenecks and data skew. Perfopticon combines interactive visualizations of (1) query plans, (2) overall query execution, (3) data flow among servers, and (4) execution traces. These views coordinate multiple levels of abstraction to enable detection, isolation, and understanding of performance issues. We evaluate our design choices through engagements with system developers, scientists, and students. We demonstrate that Perfopticon enables performance debugging for real-world tasks.},
	language = {en},
	number = {3},
	urldate = {2020-01-15},
	journal = {Computer Graphics Forum},
	author = {Moritz, Dominik and Halperin, Daniel and Howe, Bill and Heer, Jeffrey},
	year = {2015},
	keywords = {cscheid-maybe, leibatt-maybe},
	pages = {71--80}
}

@inproceedings{moritz_falcon_nodate,
	address = {New York, NY, USA},
	series = {{CHI} '19},
	title = {Falcon: {Balancing} {Interactive} {Latency} and {Resolution} {Sensitivity} for {Scalable} {Linked} {Visualizations}},
	url = {https://dl.acm.org/doi/abs/10.1145/3290605.3300924},
	doi = {https://doi.org/10.1145/3290605.3300924},
	language = {EN},
	urldate = {2020-01-15},
	booktitle = {Proceedings of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Moritz, Dominik and Howe, Bill and Heer, Jeffrey},
	keywords = {cscheid-materialized-views, cscheid-aqp, leibatt-materialized-views, leibatt-aggregation, leibatt-interaction-aggregate, leibatt-interaction-filter, leibatt-interaction-select, leibatt-db-application, cscheid-interaction-aggregate, cscheid-interaction-filter, cscheid-interaction-select},
	pages = {1--11}
}

@article{vartak_seedb_2015,
	title = {{SeeDB}: efficient data-driven visualization recommendations to support visual analytics},
	volume = {8},
	issn = {2150-8097},
	shorttitle = {{SeeDB}},
	url = {https://doi.org/10.14778/2831360.2831371},
	doi = {10.14778/2831360.2831371},
	abstract = {Data analysts often build visualizations as the first step in their analytical workflow. However, when working with high-dimensional datasets, identifying visualizations that show relevant or desired trends in data can be laborious. We propose SeeDB, a visualization recommendation engine to facilitate fast visual analysis: given a subset of data to be studied, SeeDB intelligently explores the space of visualizations, evaluates promising visualizations for trends, and recommends those it deems most "useful" or "interesting". The two major obstacles in recommending interesting visualizations are (a) scale: evaluating a large number of candidate visualizations while responding within interactive time scales, and (b) utility: identifying an appropriate metric for assessing interestingness of visualizations. For the former, SeeDB introduces pruning optimizations to quickly identify high-utility visualizations and sharing optimizations to maximize sharing of computation across visualizations. For the latter, as a first step, we adopt a deviation-based metric for visualization utility, while indicating how we may be able to generalize it to other factors influencing utility. We implement SeeDB as a middleware layer that can run on top of any DBMS. Our experiments show that our framework can identify interesting visualizations with high accuracy. Our optimizations lead to multiple orders of magnitude speedup on relational row and column stores and provide recommendations at interactive time scales. Finally, we demonstrate via a user study the effectiveness of our deviation-based utility metric and the value of recommendations in supporting visual analytics.},
	number = {13},
	urldate = {2020-01-17},
	journal = {Proceedings of the VLDB Endowment},
	author = {Vartak, Manasi and Rahman, Sajjadur and Madden, Samuel and Parameswaran, Aditya and Polyzotis, Neoklis},

	year = {2015},
	keywords = {leibatt-progressive, cscheid-mqo, leibatt-mqo, leibatt-aggregation, cscheid-progressive, leibatt-interaction-aggregate, leibatt-interaction-filter, leibatt-data-parallel, cscheid-interaction-filter, cscheid-interactive-aggregate},
	pages = {2182--2193}
}
